#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, sys, argparse, gzip, csv, calendar, tempfile, time, psutil, threading
from datetime import datetime, timedelta
from typing import List, Optional
import pymysql
import smtplib
from email.message import EmailMessage
from contextlib import contextmanager
import queue
import threading

# å…¨åŸŸè³‡æºç›£æ§è®Šæ•¸
_resource_monitor = None
_query_semaphore = None

class ResourceMonitor:
    """è³‡æºç›£æ§é¡åˆ¥ - ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨é‡ä¸¦æä¾›å‘Šè­¦"""
    def __init__(self, max_memory_mb=1024):
        self.max_memory_mb = max_memory_mb
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self.process = psutil.Process()
        self.monitoring = False
        
    def get_memory_usage_mb(self):
        """å–å¾—ç›®å‰è¨˜æ†¶é«”ä½¿ç”¨é‡ (MB)"""
        try:
            memory_info = self.process.memory_info()
            return memory_info.rss / 1024 / 1024
        except:
            return 0
            
    def check_memory_limit(self):
        """æª¢æŸ¥è¨˜æ†¶é«”æ˜¯å¦è¶…éé™åˆ¶"""
        current_mb = self.get_memory_usage_mb()
        if current_mb > self.max_memory_mb:
            print(f"âš ï¸  è¨˜æ†¶é«”ä½¿ç”¨é‡è­¦å‘Š: {current_mb:.1f}MB / {self.max_memory_mb}MB")
            return False
        return True
        
    def force_gc_if_needed(self):
        """å¿…è¦æ™‚å¼·åˆ¶åƒåœ¾å›æ”¶"""
        import gc
        if not self.check_memory_limit():
            print("ğŸ—‘ï¸  åŸ·è¡Œåƒåœ¾å›æ”¶ä»¥é‡‹æ”¾è¨˜æ†¶é«”...")
            gc.collect()
            time.sleep(0.5)  # çµ¦ç³»çµ±æ™‚é–“é‡‹æ”¾è¨˜æ†¶é«”
            return True
        return False

class MySQL57ConnectionPool:
    """
    é‡å° MySQL 5.7.27 å„ªåŒ–çš„é€£æ¥æ± å¯¦ç¾
    æ”¯æ´é€£æ¥å¥åº·æª¢æŸ¥ã€é‡è©¦æ©Ÿåˆ¶å’Œè³‡æºç›£æ§
    """
    def __init__(self, host, port, user, password, database, charset='utf8mb4', 
                 min_connections=2, max_connections=10, max_idle_time=300, **kwargs):
        self.host = host
        self.port = port
        self.user = user
        self.password = password
        self.database = database
        self.charset = charset
        self.min_connections = min_connections
        self.max_connections = max_connections
        self.max_idle_time = max_idle_time  # æœ€å¤§é—œç½®æ™‚é–“ï¼ˆç§’ï¼‰
        self.kwargs = kwargs
        
        # é€£æ¥æ± å’Œç®¡ç†çµæ§‹
        self._pool = queue.Queue(maxsize=max_connections)
        self._active_connections = set()  # æ´»èºé€£æ¥é›†åˆ
        self._connection_timestamps = {}  # é€£æ¥æ™‚é–“æˆ³
        self._lock = threading.Lock()
        self._created_connections = 0
        self._stats = {
            'created': 0,
            'reused': 0,
            'failed': 0,
            'closed': 0
        }
        
        # åˆå§‹åŒ–æœ€å°é€£æ¥æ•¸
        self._initialize_pool()
        
        # å•Ÿå‹•æ¸…ç†ç·šç¨‹
        self._cleanup_thread = threading.Thread(target=self._cleanup_idle_connections, daemon=True)
        self._cleanup_thread.start()
        
    def _create_connection(self):
        """å‰µå»º MySQL 5.7.27 å„ªåŒ–çš„é€£æ¥"""
        try:
            conn = pymysql.connect(
                host=self.host,
                port=self.port,
                user=self.user,
                password=self.password,
                database=self.database,
                charset=self.charset,
                autocommit=True,
                local_infile=True,
                connect_timeout=30,
                read_timeout=600,
                write_timeout=600,
                # MySQL 5.7.27 å„ªåŒ–åƒæ•¸
                sql_mode='STRICT_TRANS_TABLES,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO',
                init_command="""
                    SET SESSION tmp_table_size = 67108864;
                    SET SESSION max_heap_table_size = 67108864;
                    SET SESSION sort_buffer_size = 2097152;
                    SET SESSION join_buffer_size = 2097152;
                    SET SESSION read_buffer_size = 131072;
                    SET SESSION read_rnd_buffer_size = 262144;
                    SET SESSION big_tables = 1;
                    SET SESSION innodb_lock_wait_timeout = 50;
                    SET SESSION net_read_timeout = 600;
                    SET SESSION net_write_timeout = 600;
                    SET SESSION query_cache_type = OFF;
                """,
                **self.kwargs
            )
            self._stats['created'] += 1
            return conn
        except Exception as e:
            self._stats['failed'] += 1
            raise e
            
    def _initialize_pool(self):
        """åˆå§‹åŒ–é€£æ¥æ± è‡³æœ€å°é€£æ¥æ•¸"""
        for _ in range(self.min_connections):
            try:
                conn = self._create_connection()
                self._pool.put_nowait(conn)
                self._connection_timestamps[id(conn)] = time.time()
                self._created_connections += 1
            except Exception as e:
                print(f"âš ï¸  åˆå§‹åŒ–é€£æ¥æ± å¤±æ•—: {e}")
                break
                
    def _is_connection_valid(self, conn):
        """æª¢æŸ¥é€£æ¥æ˜¯å¦æœ‰æ•ˆ"""
        try:
            if not conn or not conn.open:
                return False
            # ä½¿ç”¨è¼•é‡ç´šæª¢æŸ¥
            conn.ping(reconnect=False)
            return True
        except:
            return False
            
    def _cleanup_idle_connections(self):
        """æ¸…ç†é—œç½®é€£æ¥çš„å¾Œå°ç·šç¨‹"""
        while True:
            try:
                time.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
                current_time = time.time()
                
                with self._lock:
                    # æª¢æŸ¥æ± ä¸­çš„é€£æ¥
                    temp_connections = []
                    
                    # å–å‡ºæ‰€æœ‰é€£æ¥é€²è¡Œæª¢æŸ¥
                    while not self._pool.empty():
                        try:
                            conn = self._pool.get_nowait()
                            conn_id = id(conn)
                            
                            # æª¢æŸ¥é€£æ¥æ˜¯å¦éæœŸæˆ–ç„¡æ•ˆ
                            if (conn_id in self._connection_timestamps and 
                                current_time - self._connection_timestamps[conn_id] > self.max_idle_time) or \
                               not self._is_connection_valid(conn):
                                # é€£æ¥éæœŸæˆ–ç„¡æ•ˆï¼Œé—œé–‰å®ƒ
                                try:
                                    conn.close()
                                    self._stats['closed'] += 1
                                except:
                                    pass
                                if conn_id in self._connection_timestamps:
                                    del self._connection_timestamps[conn_id]
                                self._created_connections -= 1
                            else:
                                # é€£æ¥ä»ç„¶æœ‰æ•ˆï¼Œä¿ç•™å®ƒ
                                temp_connections.append(conn)
                        except queue.Empty:
                            break
                    
                    # å°‡æœ‰æ•ˆé€£æ¥æ”¾å›æ± ä¸­
                    for conn in temp_connections:
                        try:
                            self._pool.put_nowait(conn)
                        except queue.Full:
                            # æ± å·²æ»¿ï¼Œé—œé–‰å¤šé¤˜é€£æ¥
                            try:
                                conn.close()
                                self._stats['closed'] += 1
                            except:
                                pass
                    
                    # ç¢ºä¿æœ€å°é€£æ¥æ•¸
                    current_pool_size = self._pool.qsize()
                    if current_pool_size < self.min_connections:
                        for _ in range(self.min_connections - current_pool_size):
                            try:
                                conn = self._create_connection()
                                self._pool.put_nowait(conn)
                                self._connection_timestamps[id(conn)] = time.time()
                                self._created_connections += 1
                            except:
                                break
                                
            except Exception as e:
                print(f"âš ï¸  é€£æ¥æ± æ¸…ç†ç·šç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
                
    def get_connection(self, timeout=30):
        """å¾é€£æ¥æ± ç²å–é€£æ¥ï¼Œæ”¯æ´é‡è©¦æ©Ÿåˆ¶"""
        retry_count = 3
        
        for attempt in range(retry_count):
            try:
                # å˜—è©¦å¾æ± ä¸­ç²å–ç¾æœ‰é€£æ¥
                try:
                    conn = self._pool.get(timeout=min(timeout, 10))
                    
                    # æª¢æŸ¥é€£æ¥æ˜¯å¦æœ‰æ•ˆ
                    if self._is_connection_valid(conn):
                        with self._lock:
                            self._active_connections.add(id(conn))
                            self._connection_timestamps[id(conn)] = time.time()
                            self._stats['reused'] += 1
                        return conn
                    else:
                        # é€£æ¥ç„¡æ•ˆï¼Œå˜—è©¦å‰µå»ºæ–°é€£æ¥
                        try:
                            conn.close()
                        except:
                            pass
                        
                except queue.Empty:
                    pass
                
                # å‰µå»ºæ–°é€£æ¥
                with self._lock:
                    if self._created_connections < self.max_connections:
                        conn = self._create_connection()
                        self._created_connections += 1
                        self._active_connections.add(id(conn))
                        self._connection_timestamps[id(conn)] = time.time()
                        return conn
                    
                # å¦‚æœåˆ°é”æœ€å¤§é€£æ¥æ•¸ï¼Œç­‰å¾…ä¸€æœƒå…’å†è©¦
                if attempt < retry_count - 1:
                    time.sleep(1)
                    
            except Exception as e:
                if attempt < retry_count - 1:
                    print(f"âš ï¸  ç²å–é€£æ¥å¤±æ•—ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è©¦: {e}")
                    time.sleep(1)
                else:
                    raise e
        
        raise Exception("ç„¡æ³•ç²å–è³‡æ–™åº«é€£æ¥ï¼Œé€£æ¥æ± å·²æ»¿")
    
    def release_connection(self, conn):
        """é‡‹æ”¾é€£æ¥å›æ± ä¸­"""
        if not conn:
            return
            
        conn_id = id(conn)
        
        with self._lock:
            if conn_id in self._active_connections:
                self._active_connections.remove(conn_id)
        
        if self._is_connection_valid(conn):
            try:
                # é‡è¨­é€£æ¥ç‹€æ…‹
                with conn.cursor() as cur:
                    cur.execute("ROLLBACK")
                    
                self._connection_timestamps[conn_id] = time.time()
                self._pool.put_nowait(conn)
            except queue.Full:
                # æ± å·²æ»¿ï¼Œé—œé–‰é€£æ¥
                try:
                    conn.close()
                    self._stats['closed'] += 1
                except:
                    pass
                with self._lock:
                    self._created_connections -= 1
                    if conn_id in self._connection_timestamps:
                        del self._connection_timestamps[conn_id]
            except Exception as e:
                # é€£æ¥å‡ºéŒ¯ï¼Œé—œé–‰å®ƒ
                try:
                    conn.close()
                    self._stats['closed'] += 1
                except:
                    pass
                with self._lock:
                    self._created_connections -= 1
                    if conn_id in self._connection_timestamps:
                        del self._connection_timestamps[conn_id]
        else:
            # é€£æ¥ç„¡æ•ˆï¼Œé—œé–‰å®ƒ
            try:
                conn.close()
                self._stats['closed'] += 1
            except:
                pass
            with self._lock:
                self._created_connections -= 1
                if conn_id in self._connection_timestamps:
                    del self._connection_timestamps[conn_id]
                    
    def get_stats(self):
        """ç²å–é€£æ¥æ± çµ±è¨ˆè³‡è¨Š"""
        with self._lock:
            return {
                'created_connections': self._created_connections,
                'active_connections': len(self._active_connections),
                'pool_size': self._pool.qsize(),
                'max_connections': self.max_connections,
                'min_connections': self.min_connections,
                'stats': self._stats.copy()
            }
            
    def close_all(self):
        """é—œé–‰æ‰€æœ‰é€£æ¥"""
        with self._lock:
            # é—œé–‰æ± ä¸­çš„é€£æ¥
            while not self._pool.empty():
                try:
                    conn = self._pool.get_nowait()
                    conn.close()
                    self._stats['closed'] += 1
                except:
                    pass
                    
            # æ¸…ç†ç‹€æ…‹
            self._created_connections = 0
            self._active_connections.clear()
            self._connection_timestamps.clear()


# åŠ å…¥ tqdm æ”¯æ´
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("âš ï¸  å»ºè­°å®‰è£ tqdm ä»¥ç²å¾—æ›´å¥½çš„é€²åº¦é¡¯ç¤ºï¼špip install tqdm")

# åŠ å…¥ dotenv æ”¯æ´
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("âŒ è«‹å…ˆå®‰è£ python-dotenvï¼špip install python-dotenv")
    sys.exit(1)

try:
    from reportlab.lib.pagesizes import A4
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib import colors
    REPORTLAB_AVAILABLE = True
except ImportError:
    REPORTLAB_AVAILABLE = False

class Config:
    def __init__(self):
        self.mysql_host = os.getenv('MYSQL_HOST', 'localhost')
        self.mysql_port = int(os.getenv('MYSQL_PORT', '3306'))
        self.mysql_user = os.getenv('MYSQL_USER', 'root')
        self.mysql_password = os.getenv('MYSQL_PASSWORD', '')
        self.mysql_db = os.getenv('MYSQL_DB', 'auditdb')
        # æ–°å¢è³‡æ–™åº«é€£ç·šæ± å’Œæ•ˆèƒ½ç›¸é—œè¨­å®š
        self.db_pool_size = int(os.getenv('DB_POOL_SIZE', '5'))
        self.db_max_overflow = int(os.getenv('DB_MAX_OVERFLOW', '10'))
        self.db_pool_timeout = int(os.getenv('DB_POOL_TIMEOUT', '30'))
        self.db_query_timeout = int(os.getenv('DB_QUERY_TIMEOUT', '300'))
        self.max_fetch_size = int(os.getenv('MAX_FETCH_SIZE', '100000'))
        self.batch_fetch_size = int(os.getenv('BATCH_FETCH_SIZE', '10000'))
        self.query_retry_count = int(os.getenv('QUERY_RETRY_COUNT', '3'))
        self.retry_delay = float(os.getenv('RETRY_DELAY', '1.0'))
        # è³‡æºé™åˆ¶å’Œç¯€æµæ§åˆ¶
        self.max_memory_usage_mb = int(os.getenv('MAX_MEMORY_USAGE_MB', '1024'))
        self.query_throttle_delay = float(os.getenv('QUERY_THROTTLE_DELAY', '0.1'))
        self.enable_resource_monitoring = os.getenv('ENABLE_RESOURCE_MONITORING', 'true').lower() == 'true'
        self.max_concurrent_queries = int(os.getenv('MAX_CONCURRENT_QUERIES', '3'))
        self.log_base_path = os.getenv('LOG_BASE_PATH', '/var/log/mysql/audit')
        self.log_file_prefix = os.getenv('LOG_FILE_PREFIX', 'server_audit.log')
        self.output_dir = os.getenv('OUTPUT_DIR', '/tmp/mysql_reports')
        self.failed_login_threshold = int(os.getenv('FAILED_LOGIN_THRESHOLD', '5'))
        self.allowed_ips = [ip.strip() for ip in os.getenv('ALLOWED_IPS', '').split(',') if ip.strip()]
        self.after_hours_users = [u.strip() for u in os.getenv('AFTER_HOURS_USERS', '').split(',') if u.strip()]
        self.work_hour_start = int(os.getenv('WORK_HOUR_START', '9'))
        self.work_hour_end = int(os.getenv('WORK_HOUR_END', '18'))
        self.privileged_users = [u.strip() for u in os.getenv('PRIVILEGED_USERS', '').split(',') if u.strip()]
        self.report_title = os.getenv('REPORT_TITLE', 'MySQL Audit Log Security Analysis Report')
        self.company_name = os.getenv('COMPANY_NAME', 'Your Company')
        self.generate_pdf = os.getenv('GENERATE_PDF', 'true').lower() == 'true'
        self.generate_csv = os.getenv('GENERATE_CSV', 'true').lower() == 'true'
        self.privileged_keywords = [k.strip() for k in os.getenv(
            'PRIVILEGED_KEYWORDS',
            'CREATE USER,DROP USER,GRANT,REVOKE,CREATE DATABASE,DROP DATABASE,CREATE TABLE,DROP TABLE,ALTER USER,SET PASSWORD'
        ).split(',') if k.strip()]
        self.send_email = os.getenv('SEND_EMAIL', 'false').lower() == 'true'
        self.smtp_server = os.getenv('SMTP_SERVER', '')
        self.smtp_port = int(os.getenv('SMTP_PORT', '25')) 
        self.mail_from = os.getenv('MAIL_FROM', '')
        self.mail_to = [m.strip() for m in os.getenv('MAIL_TO', '').split(',') if m.strip()]
        # æ–°å¢ LOAD DATA INFILE ç›¸é—œè¨­å®š
        self.use_load_data_infile = os.getenv('USE_LOAD_DATA_INFILE', 'true').lower() == 'true'
        self.temp_dir = os.getenv('TEMP_DIR', '/tmp')

    def get_log_file_path(self, date_str: str = None) -> str:
        return os.path.join(self.log_base_path, self.log_file_prefix if not date_str else f"{self.log_file_prefix}-{date_str}")

    def as_dict(self):
        return {
            "MYSQL_HOST": self.mysql_host,
            "MYSQL_PORT": self.mysql_port,
            "MYSQL_USER": self.mysql_user,
            "MYSQL_PASSWORD": self.mysql_password,
            "MYSQL_DB": self.mysql_db,
            "LOG_BASE_PATH": self.log_base_path,
            "LOG_FILE_PREFIX": self.log_file_prefix,
            "OUTPUT_DIR": self.output_dir,
            "FAILED_LOGIN_THRESHOLD": self.failed_login_threshold,
            "ALLOWED_IPS": self.allowed_ips,
            "AFTER_HOURS_USERS": self.after_hours_users,
            "WORK_HOUR_START": self.work_hour_start,
            "WORK_HOUR_END": self.work_hour_end,
            "PRIVILEGED_USERS": self.privileged_users,
            "REPORT_TITLE": self.report_title,
            "COMPANY_NAME": self.company_name,
            "GENERATE_PDF": self.generate_pdf,
            "GENERATE_CSV": self.generate_csv,
            "PRIVILEGED_KEYWORDS": self.privileged_keywords,
            "SEND_EMAIL": self.send_email,
            "SMTP_SERVER": self.smtp_server,
            "SMTP_PORT": self.smtp_port,
            "MAIL_FROM": self.mail_from,
            "MAIL_TO": self.mail_to,
            "USE_LOAD_DATA_INFILE": self.use_load_data_infile,
            "TEMP_DIR": self.temp_dir,
        }

def init_resource_monitoring(config: Config):
    """åˆå§‹åŒ–è³‡æºç›£æ§"""
    global _resource_monitor, _query_semaphore
    
    if config.enable_resource_monitoring:
        _resource_monitor = ResourceMonitor(config.max_memory_usage_mb)
        print(f"âœ… è³‡æºç›£æ§åˆå§‹åŒ–å®Œæˆ (è¨˜æ†¶é«”é™åˆ¶: {config.max_memory_usage_mb}MB)")
    
    _query_semaphore = threading.Semaphore(config.max_concurrent_queries)
    print(f"âœ… æŸ¥è©¢ä¸¦è¡Œæ§åˆ¶åˆå§‹åŒ–å®Œæˆ (æœ€å¤§ä¸¦è¡Œ: {config.max_concurrent_queries})")

# å…¨åŸŸé€£ç·šæ± è®Šæ•¸
_connection_pool = None

def init_connection_pool(config: Config):
    """
    åˆå§‹åŒ– MySQL 5.7.27 å„ªåŒ–çš„è³‡æ–™åº«é€£ç·šæ± 
    """
    global _connection_pool
    if _connection_pool is None:
        _connection_pool = MySQL57ConnectionPool(
            host=config.mysql_host,
            port=config.mysql_port,
            user=config.mysql_user,
            password=config.mysql_password,
            database=config.mysql_db,
            charset='utf8mb4',
            min_connections=max(1, config.db_pool_size // 2),  # æœ€å°é€£æ¥æ•¸
            max_connections=config.db_pool_size + config.db_max_overflow,
            max_idle_time=300,  # 5åˆ†é˜é—œç½®æ™‚é–“
        )
        print(f"âœ… MySQL 5.7.27 é€£ç·šæ± åˆå§‹åŒ–å®Œæˆ")
        print(f"   æœ€å°é€£æ¥: {_connection_pool.min_connections}")
        print(f"   æœ€å¤§é€£æ¥: {_connection_pool.max_connections}")
        print(f"   é—œç½®æ™‚é–“: {_connection_pool.max_idle_time}ç§’")

@contextmanager
def get_db_conn(config: Config):
    """
    å–å¾—è³‡æ–™åº«é€£ç·š (ä½¿ç”¨ MySQL 5.7.27 å„ªåŒ–é€£ç·šæ± å’Œè‡ªå‹•é‡‹æ”¾)
    """
    if _connection_pool is None:
        init_connection_pool(config)
    
    conn = None
    start_time = time.time()
    
    try:
        conn = _connection_pool.get_connection(timeout=config.db_pool_timeout)
        
        # è¨˜éŒ„é€£æ¥ç²å–æ™‚é–“
        get_time = time.time() - start_time
        if get_time > 5:  # å¦‚æœç²å–é€£æ¥è¶…é5ç§’ï¼Œçµ¦å‡ºè­¦å‘Š
            print(f"âš ï¸  é€£æ¥ç²å–è€—æ™‚: {get_time:.2f}ç§’")
            
        yield conn
        
    except Exception as e:
        if conn:
            try:
                conn.rollback()
            except:
                pass  # å¿½ç•¥ rollback éŒ¯èª¤
        raise e
    finally:
        if conn:
            _connection_pool.release_connection(conn)

def get_legacy_db_conn(config: Config):
    """
    å–å¾—å‚³çµ±è³‡æ–™åº«é€£ç·š (ç”¨æ–¼ä¸æ”¯æ´ context manager çš„èˆŠç¨‹å¼ç¢¼)
    é‡å° MySQL 5.7.27 å„ªåŒ–
    """
    connection = pymysql.connect(
        host=config.mysql_host,
        port=config.mysql_port,
        user=config.mysql_user,
        password=config.mysql_password,
        database=config.mysql_db,
        charset='utf8mb4',
        autocommit=True,
        local_infile=True,
        connect_timeout=30,
        read_timeout=config.db_query_timeout,
        write_timeout=config.db_query_timeout,
        # MySQL 5.7.27 å„ªåŒ–åƒæ•¸
        sql_mode='STRICT_TRANS_TABLES,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO',
        init_command="""
            SET SESSION tmp_table_size = 67108864;
            SET SESSION max_heap_table_size = 67108864;
            SET SESSION sort_buffer_size = 2097152;
            SET SESSION join_buffer_size = 2097152;
            SET SESSION read_buffer_size = 131072;
            SET SESSION read_rnd_buffer_size = 262144;
            SET SESSION big_tables = 1;
            SET SESSION innodb_lock_wait_timeout = 50;
            SET SESSION net_read_timeout = 600;
            SET SESSION net_write_timeout = 600;
        """
    )
    return connection

def get_connection_pool_stats():
    """ç²å–é€£æ¥æ± çµ±è¨ˆè³‡è¨Š"""
    global _connection_pool
    if _connection_pool and hasattr(_connection_pool, 'get_stats'):
        return _connection_pool.get_stats()
    return None

def close_connection_pool():
    """é—œé–‰é€£æ¥æ± """
    global _connection_pool
    if _connection_pool and hasattr(_connection_pool, 'close_all'):
        _connection_pool.close_all()
        _connection_pool = None
        print("âœ… é€£æ¥æ± å·²é—œé–‰")

def get_simple_db_conn(config: Config):
    """
    å–å¾—ç°¡å–®çš„è³‡æ–™åº«é€£ç·šç”¨æ–¼åˆ†æ (ä¸ä½¿ç”¨é€£æ¥æ± )
    é‡å° MySQL 5.7.27 å„ªåŒ–
    """
    connection = pymysql.connect(
        host=config.mysql_host,
        port=config.mysql_port,
        user=config.mysql_user,
        password=config.mysql_password,
        database=config.mysql_db,
        charset='utf8mb4',
        autocommit=True,
        connect_timeout=30,
        read_timeout=600,  # åˆ†ææŸ¥è©¢å¯èƒ½éœ€è¦æ›´é•·æ™‚é–“
        write_timeout=600,
        cursorclass=pymysql.cursors.DictCursor,  # ä½¿ç”¨å­—å…¸æ¸¸æ¨™ä¾¿æ–¼çµæœè™•ç†
        # MySQL 5.7.27 åˆ†ææŸ¥è©¢å„ªåŒ–åƒæ•¸
        sql_mode='STRICT_TRANS_TABLES,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO',
        init_command="""
            SET SESSION tmp_table_size = 134217728;
            SET SESSION max_heap_table_size = 134217728;
            SET SESSION sort_buffer_size = 4194304;
            SET SESSION join_buffer_size = 4194304;
            SET SESSION read_buffer_size = 262144;
            SET SESSION read_rnd_buffer_size = 524288;
            SET SESSION big_tables = 1;
            SET SESSION optimizer_search_depth = 62;
            SET SESSION query_cache_type = OFF;
            SET SESSION innodb_lock_wait_timeout = 120;
            SET SESSION net_read_timeout = 1200;
            SET SESSION net_write_timeout = 1200;
        """
    )
    return connection

def execute_simple_query(conn, query, params=None, max_rows=10000):
    """
    åŸ·è¡Œç°¡å–®æŸ¥è©¢ç”¨æ–¼åˆ†æ (ä¸ä½¿ç”¨è¤‡é›œçš„é‡è©¦å’Œè³‡æºç›£æ§)
    """
    try:
        with conn.cursor() as cur:
            cur.execute(query, params)
            # é™åˆ¶çµæœæ•¸é‡ä»¥é˜²æ­¢è¨˜æ†¶é«”å•é¡Œ
            if "SELECT COUNT" in query.upper() or "LIMIT" in query.upper():
                return cur.fetchall()
            else:
                return cur.fetchmany(max_rows)
    except Exception as e:
        print(f"âŒ æŸ¥è©¢åŸ·è¡Œå¤±æ•—: {e}")
        return []

def execute_analysis_query(conn, query, params=None, use_simple=True):
    """
    åŸ·è¡Œåˆ†ææŸ¥è©¢ï¼Œå¯é¸æ“‡ç°¡å–®æ¨¡å¼æˆ–è¤‡é›œæ¨¡å¼
    """
    if use_simple:
        return execute_simple_query(conn, query, params)
    else:
        return execute_query_with_retry(conn, query, params, None, 'fetchall')

def execute_query_with_retry(conn, query, params=None, config=None, fetch_mode='fetchall'):
    """
    åŸ·è¡ŒæŸ¥è©¢ä¸¦åŠ å…¥é‡è©¦æ©Ÿåˆ¶ã€è¨˜æ†¶é«”ç®¡ç†å’Œè³‡æºç›£æ§
    
    Args:
        conn: è³‡æ–™åº«é€£ç·š
        query: SQLæŸ¥è©¢èªå¥
        params: æŸ¥è©¢åƒæ•¸
        config: è¨­å®šç‰©ä»¶
        fetch_mode: 'fetchall', 'fetchone', 'fetchmany', 'iterator'
    
    Returns:
        æŸ¥è©¢çµæœ
    """
    global _resource_monitor, _query_semaphore
    
    retry_count = config.query_retry_count if config else 3
    retry_delay = config.retry_delay if config else 1.0
    max_fetch_size = config.max_fetch_size if config else 100000
    batch_size = config.batch_fetch_size if config else 10000
    throttle_delay = config.query_throttle_delay if config else 0.1
    
    # è³‡æºç›£æ§å’Œä¸¦è¡Œæ§åˆ¶
    if _resource_monitor:
        _resource_monitor.force_gc_if_needed()
    
    # ä½¿ç”¨ä¿¡è™Ÿé‡æ§åˆ¶ä¸¦è¡ŒæŸ¥è©¢æ•¸é‡
    if _query_semaphore:
        _query_semaphore.acquire()
    
    try:
        for attempt in range(retry_count):
            try:
                # æŸ¥è©¢ç¯€æµ
                if throttle_delay > 0:
                    time.sleep(throttle_delay)
                
                with conn.cursor() as cur:
                            # MySQL 5.7.27 æŸ¥è©¢å„ªåŒ–åƒæ•¸è¨­å®š
                    if config:
                        try:
                            # è¨­å®š MySQL 5.7.27 ç‰¹å®šçš„å„ªåŒ–åƒæ•¸
                            optimization_sql = f"""
                                SET SESSION innodb_lock_wait_timeout = {min(config.db_query_timeout, 120)};
                                SET SESSION max_execution_time = {config.db_query_timeout * 1000};
                                SET SESSION optimizer_search_depth = 62;
                                SET SESSION eq_range_index_dive_limit = 200;
                            """
                            for sql in optimization_sql.strip().split(';'):
                                if sql.strip():
                                    cur.execute(sql.strip())
                        except Exception as opt_error:
                            # å¿½ç•¥å„ªåŒ–åƒæ•¸è¨­ç½®å¤±æ•—ï¼Œä½†è¨˜éŒ„è­¦å‘Š
                            print(f"âš ï¸  MySQL 5.7.27 å„ªåŒ–åƒæ•¸è¨­ç½®å¤±æ•—: {opt_error}")
                    
                    cur.execute(query, params)
                    
                    if fetch_mode == 'fetchone':
                        result = cur.fetchone()
                    elif fetch_mode == 'fetchmany':
                        result = cur.fetchmany(batch_size)
                    elif fetch_mode == 'iterator':
                        result = cur  # å›å‚³æ¸¸æ¨™è¿­ä»£å™¨ï¼Œç¯€çœè¨˜æ†¶é«”
                    elif fetch_mode == 'fetchall_safe':
                        # å®‰å…¨çš„ fetchallï¼Œé™åˆ¶çµæœæ•¸é‡ä¸¦ç›£æ§è¨˜æ†¶é«”
                        results = []
                        count = 0
                        while True:
                            # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨é‡
                            if _resource_monitor and not _resource_monitor.check_memory_limit():
                                print(f"âš ï¸  è¨˜æ†¶é«”ä¸è¶³ï¼ŒæŸ¥è©¢çµæœæˆªæ–·æ–¼ {count:,} ç­†")
                                break
                            
                            batch = cur.fetchmany(batch_size)
                            if not batch:
                                break
                            results.extend(batch)
                            count += len(batch)
                            
                            if count >= max_fetch_size:
                                print(f"âš ï¸  æŸ¥è©¢çµæœè¶…éé™åˆ¶ ({max_fetch_size:,} ç­†)ï¼Œå·²æˆªæ–·")
                                break
                        result = results
                    else:  # fetchall (default)
                        result = cur.fetchall()
                    
                    return result
                        
            except (pymysql.Error, Exception) as e:
                if attempt < retry_count - 1:
                    print(f"âš ï¸  æŸ¥è©¢å¤±æ•—ï¼Œç¬¬ {attempt + 1}/{retry_count} æ¬¡é‡è©¦... éŒ¯èª¤: {str(e)}")
                    time.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•¸é€€é¿
                else:
                    print(f"âŒ æŸ¥è©¢æœ€çµ‚å¤±æ•—: {str(e)}")
                    raise e
    finally:
        # é‡‹æ”¾ä¿¡è™Ÿé‡
        if _query_semaphore:
            _query_semaphore.release()
        
        # å¦‚æœæ˜¯é•·æ™‚é–“æŸ¥è©¢ï¼Œè¼¸å‡ºçµ±è¨ˆè³‡è¨Š
        if _connection_pool and hasattr(_connection_pool, 'get_stats'):
            stats = _connection_pool.get_stats()
            if stats['active_connections'] > stats['max_connections'] * 0.8:
                print(f"âš ï¸  é€£æ¥æ± ä½¿ç”¨ç‡éé«˜: {stats['active_connections']}/{stats['max_connections']}")

def get_file_line_count(file_path):
    """å¿«é€Ÿè¨ˆç®—æª”æ¡ˆè¡Œæ•¸ï¼Œç”¨æ–¼é€²åº¦æ¢"""
    try:
        if file_path.endswith('.gz'):
            with gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') as f:
                return sum(1 for _ in f)
        else:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return sum(1 for _ in f)
    except:
        return 0

def import_log_file_to_db_optimized(file_path, log_date, conn, config):
    """
    ä½¿ç”¨ LOAD DATA INFILE å„ªåŒ–ç‰ˆæœ¬çš„æ—¥èªŒåŒ¯å…¥å‡½æ•¸ï¼ˆåŠ å…¥é€²åº¦æ¢ï¼‰
    """
    print(f"ğŸš€ é–‹å§‹å„ªåŒ–åŒ¯å…¥ {file_path}...")
    
    # è¨ˆç®—æª”æ¡ˆè¡Œæ•¸ç”¨æ–¼é€²åº¦æ¢
    if TQDM_AVAILABLE:
        print("ğŸ“Š æ­£åœ¨è¨ˆç®—æª”æ¡ˆå¤§å°...")
        total_lines = get_file_line_count(file_path)
        if total_lines == 0:
            print(f"âš ï¸  æª”æ¡ˆ {file_path} æ²’æœ‰è³‡æ–™")
            return
    
    # å»ºç«‹è‡¨æ™‚ CSV æª”æ¡ˆ
    temp_csv = tempfile.NamedTemporaryFile(
        mode='w', 
        suffix='.csv', 
        dir=config.temp_dir,
        delete=False,
        encoding='utf-8'
    )
    
    try:
        # è®€å–åŸå§‹æ—¥èªŒæª”æ¡ˆä¸¦è½‰æ›ç‚ºæ¨™æº– CSV æ ¼å¼
        with (gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') 
              if file_path.endswith('.gz') 
              else open(file_path, 'r', encoding='utf-8', errors='ignore')) as f:
            
            reader = csv.reader(f)
            writer = csv.writer(temp_csv, quoting=csv.QUOTE_ALL)
            
            # å»ºç«‹é€²åº¦æ¢
            if TQDM_AVAILABLE:
                progress_bar = tqdm(
                    total=total_lines,
                    desc="ğŸ“ è™•ç†æ—¥èªŒè³‡æ–™",
                    unit="è¡Œ",
                    unit_scale=True,
                    colour='green'
                )
            
            row_count = 0
            start_time = datetime.now()
            
            for row in reader:
                # ç¢ºä¿æ¬„ä½æ•¸é‡ä¸€è‡´
                row += [''] * (10 - len(row))
                timestamp, server_host, username, host, connection_id, query_id, operation, database, query, retcode = row[:10]
                
                # è™•ç† retcode
                try:
                    retcode = int(retcode) if retcode else 0
                except:
                    retcode = 0
                
                # å¯«å…¥è‡¨æ™‚ CSV æª”æ¡ˆ
                writer.writerow([
                    log_date, timestamp, server_host, username, host, 
                    connection_id, query_id, operation, database, query, retcode
                ])
                row_count += 1
                
                # æ›´æ–°é€²åº¦æ¢
                if TQDM_AVAILABLE:
                    progress_bar.update(1)
                    if row_count % 10000 == 0:  # æ¯ 10000 ç­†æ›´æ–°ä¸€æ¬¡æè¿°
                        progress_bar.set_postfix({
                            'å·²è™•ç†': f'{row_count:,}',
                            'é€Ÿåº¦': f'{row_count/(datetime.now()-start_time).total_seconds():.0f}/ç§’'
                        })
            
            if TQDM_AVAILABLE:
                progress_bar.close()
        
        temp_csv.close()
        
        if row_count == 0:
            print(f"âš ï¸  æª”æ¡ˆ {file_path} æ²’æœ‰è³‡æ–™")
            return
        
        processing_time = (datetime.now() - start_time).total_seconds()
        print(f"âœ… è³‡æ–™è™•ç†å®Œæˆ: {row_count:,} ç­†ï¼Œè€—æ™‚ {processing_time:.2f} ç§’")
        
        # ä½¿ç”¨ MySQL 5.7.27 å„ªåŒ–çš„ LOAD DATA LOCAL INFILE æ‰¹é‡è¼‰å…¥
        print("ğŸ’¾ æ­£åœ¨ä½¿ç”¨ MySQL 5.7.27 å„ªåŒ–è¼‰å…¥è³‡æ–™åˆ°è³‡æ–™åº«...")
        db_start_time = datetime.now()
        
        with conn.cursor() as cur:
            # å…ˆæª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨è©²æ—¥æœŸçš„è³‡æ–™ï¼Œå¦‚æœæœ‰å‰‡å…ˆåˆªé™¤
            delete_sql = "DELETE FROM audit_log WHERE log_date = %s"
            cur.execute(delete_sql, (log_date,))
            deleted_count = cur.rowcount
            if deleted_count > 0:
                print(f"ğŸ—‘ï¸  åˆªé™¤èˆŠè³‡æ–™ {deleted_count:,} ç­†")
            
            # MySQL 5.7.27 ç‰¹å®šçš„ LOAD DATA LOCAL INFILE èªæ³•
            load_sql = f"""
            LOAD DATA LOCAL INFILE '{temp_csv.name.replace(chr(92), '/')}'
            INTO TABLE audit_log
            CHARACTER SET utf8mb4
            FIELDS TERMINATED BY ','
            OPTIONALLY ENCLOSED BY '"'
            ESCAPED BY '"'
            LINES TERMINATED BY '\\n'
            (log_date, timestamp, server_host, username, host, connection_id, query_id, operation, dbname, query, retcode)
            """
            
            # åŸ·è¡Œ LOAD DATA ä¸¦æ•æ‰å¯èƒ½çš„éŒ¯èª¤
            try:
                cur.execute(load_sql)
                # ç²å–å¯¦éš›è¼‰å…¥çš„è¡Œæ•¸
                cur.execute("SELECT ROW_COUNT()")
                loaded_rows = cur.fetchone()[0]
                
                # ç²å– MySQL è­¦å‘Šè¨Šæ¯
                cur.execute("SHOW WARNINGS")
                warnings = cur.fetchall()
                if warnings:
                    print(f"âš ï¸  MySQL è­¦å‘Š ({len(warnings)} å€‹):")
                    for level, code, message in warnings[:5]:  # åªé¡¯ç¤ºå‰5å€‹è­¦å‘Š
                        print(f"   {level} {code}: {message}")
                    if len(warnings) > 5:
                        print(f"   ... åŠå…¶ä»– {len(warnings) - 5} å€‹è­¦å‘Š")
                        
            except pymysql.Error as mysql_error:
                error_code = mysql_error.args[0] if mysql_error.args else 0
                error_msg = mysql_error.args[1] if len(mysql_error.args) > 1 else str(mysql_error)
                
                print(f"âŒ MySQL LOAD DATA éŒ¯èª¤ {error_code}: {error_msg}")
                
                # å¸¸è¦‹éŒ¯èª¤çš„ç‰¹å®šè™•ç†
                if error_code in [1148, 1290, 2061]:  # LOAD DATA LOCAL INFILE ç›¸é—œéŒ¯èª¤
                    print("ğŸ”„ LOAD DATA LOCAL INFILE è¢«ç¦ç”¨æˆ–ä¸æ”¯æŒï¼Œå°‡å›é€€åˆ°åŸå§‹æ–¹æ³•")
                    return import_log_file_to_db_fallback(file_path, log_date, conn)
                elif error_code in [1062]:  # é‡è¤‡éµéŒ¯èª¤
                    print(f"âš ï¸  åµæ¸¬åˆ°é‡è¤‡è³‡æ–™ï¼Œå˜—è©¦å…ˆæ¸…ç†å¾Œå†è¼‰å…¥")
                    cur.execute("DELETE FROM audit_log WHERE log_date = %s", (log_date,))
                    cur.execute(load_sql)  # é‡è©¦
                    cur.execute("SELECT ROW_COUNT()")
                    loaded_rows = cur.fetchone()[0]
                else:
                    # å…¶ä»–éŒ¯èª¤ï¼Œæ‹‹å‡ºç•°å¸¸
                    raise mysql_error
            
            db_duration = (datetime.now() - db_start_time).total_seconds()
            total_duration = (datetime.now() - start_time).total_seconds()
            
            print(f"âœ… å„ªåŒ–åŒ¯å…¥ {os.path.basename(file_path)} å®Œæˆ")
            print(f"   ğŸ“Š è™•ç†è³‡æ–™: {row_count:,} ç­†")
            print(f"   ğŸ“¥ è¼‰å…¥è³‡æ–™: {loaded_rows:,} ç­†")
            print(f"   â±ï¸  è™•ç†è€—æ™‚: {processing_time:.2f} ç§’")
            print(f"   ğŸ’¾ è¼‰å…¥è€—æ™‚: {db_duration:.2f} ç§’")
            print(f"   ğŸ•’ ç¸½è€—æ™‚: {total_duration:.2f} ç§’")
            print(f"   ğŸš€ ç¸½é€Ÿåº¦: {loaded_rows/total_duration:.0f} ç­†/ç§’")
            
    except Exception as e:
        print(f"âŒ å„ªåŒ–åŒ¯å…¥å¤±æ•—: {e}")
        # å¦‚æœ LOAD DATA INFILE å¤±æ•—ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•
        print("ğŸ”„ å›é€€åˆ°åŸå§‹åŒ¯å…¥æ–¹æ³•...")
        import_log_file_to_db_fallback(file_path, log_date, conn)
        
    finally:
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        try:
            os.unlink(temp_csv.name)
        except:
            pass

def import_log_file_to_db_fallback(file_path, log_date, conn):
    """
    åŸå§‹çš„é€ç­†æ’å…¥æ–¹æ³•ï¼ˆä½œç‚ºå‚™ç”¨æ–¹æ¡ˆï¼ŒåŠ å…¥é€²åº¦æ¢ï¼‰
    """
    print(f"ğŸ“ ä½¿ç”¨åŸå§‹æ–¹æ³•åŒ¯å…¥ {file_path}...")
    
    # è¨ˆç®—æª”æ¡ˆè¡Œæ•¸ç”¨æ–¼é€²åº¦æ¢
    if TQDM_AVAILABLE:
        print("ğŸ“Š æ­£åœ¨è¨ˆç®—æª”æ¡ˆå¤§å°...")
        total_lines = get_file_line_count(file_path)
        if total_lines == 0:
            print(f"âš ï¸  æª”æ¡ˆ {file_path} æ²’æœ‰è³‡æ–™")
            return
    
    with (gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') 
          if file_path.endswith('.gz') 
          else open(file_path, 'r', encoding='utf-8', errors='ignore')) as f:
        
        reader = csv.reader(f)
        data = []
        
        # å»ºç«‹é€²åº¦æ¢
        if TQDM_AVAILABLE:
            progress_bar = tqdm(
                total=total_lines,
                desc="ğŸ“ è®€å–æ—¥èªŒè³‡æ–™",
                unit="è¡Œ",
                unit_scale=True,
                colour='blue'
            )
        
        start_time = datetime.now()
        
        for row in reader:
            row += [''] * (10 - len(row))
            timestamp, server_host, username, host, connection_id, query_id, operation, database, query, retcode = row[:10]
            try:
                retcode = int(retcode) if retcode else 0
            except:
                retcode = 0
            data.append((log_date, timestamp, server_host, username, host, connection_id, query_id, operation, database, query, retcode))
            
            if TQDM_AVAILABLE:
                progress_bar.update(1)
        
        if TQDM_AVAILABLE:
            progress_bar.close()
        
        if not data:
            print(f"âš ï¸  æª”æ¡ˆ {file_path} æ²’æœ‰è³‡æ–™")
            return
        
        processing_time = (datetime.now() - start_time).total_seconds()
        print(f"âœ… è³‡æ–™è®€å–å®Œæˆ: {len(data):,} ç­†ï¼Œè€—æ™‚ {processing_time:.2f} ç§’")
        
        # è³‡æ–™åº«æ“ä½œ
        print("ğŸ’¾ æ­£åœ¨å¯«å…¥è³‡æ–™åº«...")
        db_start_time = datetime.now()
        
        with conn.cursor() as cur:
            # å…ˆåˆªé™¤è©²æ—¥æœŸçš„èˆŠè³‡æ–™
            cur.execute("DELETE FROM audit_log WHERE log_date = %s", (log_date,))
            deleted_count = cur.rowcount
            if deleted_count > 0:
                print(f"ğŸ—‘ï¸  åˆªé™¤èˆŠè³‡æ–™ {deleted_count:,} ç­†")
            
            # æ‰¹é‡æ’å…¥ï¼ˆä½¿ç”¨ executemanyï¼‰
            sql = """INSERT INTO audit_log
                    (log_date, timestamp, server_host, username, host, connection_id, query_id, operation, dbname, query, retcode)
                    VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)"""
            
            # å¦‚æœè³‡æ–™é‡å¾ˆå¤§ï¼Œå¯ä»¥åˆ†æ‰¹è™•ç†
            batch_size = 10000
            if len(data) > batch_size:
                if TQDM_AVAILABLE:
                    batch_progress = tqdm(
                        total=len(data),
                        desc="ğŸ’¾ æ‰¹é‡å¯«å…¥",
                        unit="ç­†",
                        unit_scale=True,
                        colour='cyan'
                    )
                
                for i in range(0, len(data), batch_size):
                    batch = data[i:i+batch_size]
                    cur.executemany(sql, batch)
                    if TQDM_AVAILABLE:
                        batch_progress.update(len(batch))
                
                if TQDM_AVAILABLE:
                    batch_progress.close()
            else:
                cur.executemany(sql, data)
            
            db_duration = (datetime.now() - db_start_time).total_seconds()
            total_duration = (datetime.now() - start_time).total_seconds()
            
            print(f"âœ… åŸå§‹æ–¹æ³•åŒ¯å…¥ {os.path.basename(file_path)} å®Œæˆ")
            print(f"   ğŸ“Š è¼‰å…¥è³‡æ–™: {len(data):,} ç­†")
            print(f"   â±ï¸  è®€å–è€—æ™‚: {processing_time:.2f} ç§’")
            print(f"   ğŸ’¾ å¯«å…¥è€—æ™‚: {db_duration:.2f} ç§’")
            print(f"   ğŸ•’ ç¸½è€—æ™‚: {total_duration:.2f} ç§’")
            print(f"   ğŸŒ ç¸½é€Ÿåº¦: {len(data)/total_duration:.0f} ç­†/ç§’")

def import_log_file_to_db(file_path, log_date, conn, config=None):
    """
    ä¸»è¦çš„æ—¥èªŒåŒ¯å…¥å‡½æ•¸ - æ ¹æ“šè¨­å®šé¸æ“‡å„ªåŒ–æˆ–åŸå§‹æ–¹æ³•
    """
    if config and config.use_load_data_infile:
        import_log_file_to_db_optimized(file_path, log_date, conn, config)
    else:
        import_log_file_to_db_fallback(file_path, log_date, conn)

def get_log_files_for_month(config, month_str):
    log_files = []
    year, month = map(int, month_str.split('-'))
    days_in_month = calendar.monthrange(year, month)[1]
    for day in range(1, days_in_month + 1):
        date_str = f"{year:04d}-{month:02d}-{day:02d}"
        log_path = config.get_log_file_path(date_str)
        if os.path.exists(log_path):
            log_files.append((log_path, date_str))
        elif os.path.exists(log_path + '.gz'):
            log_files.append((log_path + '.gz', date_str))
    return log_files

def get_log_file_for_date(config, date_str):
    log_path = config.get_log_file_path(date_str)
    if os.path.exists(log_path):
        return (log_path, date_str)
    elif os.path.exists(log_path + '.gz'):
        return (log_path + '.gz', date_str)
    else:
        return None

# ========== åˆ†ææŸ¥è©¢ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰ ==========

def run_analysis_with_progress(analysis_functions, conn, date_filter, date_filter_value, config, use_simple=False):
    """
    åŸ·è¡Œæ‰€æœ‰åˆ†æåŠŸèƒ½ä¸¦é¡¯ç¤ºé€²åº¦
    """
    results = {}
    
    if TQDM_AVAILABLE:
        progress_bar = tqdm(
            total=len(analysis_functions),
            desc="ğŸ” åŸ·è¡Œå®‰å…¨åˆ†æ",
            unit="é …ç›®",
            colour='magenta'
        )
    
    for name, func, args in analysis_functions:
        start_time = datetime.now()
        
        if TQDM_AVAILABLE:
            progress_bar.set_description(f"ğŸ” åˆ†æ: {name}")
        
        try:
            if args:
                results[name] = func(conn, date_filter, date_filter_value, *args, config=config, use_simple=use_simple)
            else:
                results[name] = func(conn, date_filter, date_filter_value, config=config, use_simple=use_simple)
            
            duration = (datetime.now() - start_time).total_seconds()
            
            if not TQDM_AVAILABLE:
                print(f"âœ… {name} å®Œæˆ ({duration:.2f}ç§’)")
                
        except Exception as e:
            print(f"âŒ {name} å¤±æ•—: {e}")
            results[name] = None
        
        if TQDM_AVAILABLE:
            progress_bar.update(1)
    
    if TQDM_AVAILABLE:
        progress_bar.close()
    
    return results

def analyze_summary(conn, date_filter, date_filter_value, config=None, use_simple=False):
    """åŸºæœ¬çµ±è¨ˆåˆ†æ - ä½¿ç”¨å„ªåŒ–æŸ¥è©¢"""
    if isinstance(date_filter_value, tuple):
        params = date_filter_value
    else:
        params = (date_filter_value,)
    
    query = f"SELECT COUNT(*) as total_events, COUNT(DISTINCT username) as unique_users, COUNT(DISTINCT host) as unique_hosts FROM audit_log WHERE {date_filter}"
    result = execute_analysis_query(conn, query, params, use_simple)
    
    if result and len(result) > 0:
        row = result[0]
        return {
            'total_events': row['total_events'],
            'unique_users': row['unique_users'],
            'unique_hosts': row['unique_hosts']
        }
    return {'total_events': 0, 'unique_users': 0, 'unique_hosts': 0}

def analyze_failed_logins(conn, date_filter, date_filter_value, threshold=5, config=None, use_simple=False):
    """å¤±æ•—ç™»å…¥åˆ†æ - ä½¿ç”¨å„ªåŒ–æŸ¥è©¢å’Œçµæœé™åˆ¶"""
    if isinstance(date_filter_value, tuple):
        params = date_filter_value + (threshold,)
        params2 = date_filter_value
    else:
        params = (date_filter_value, threshold)
        params2 = (date_filter_value,)
    
    # æŸ¥è©¢å¯ç–‘ä½¿ç”¨è€… (ä½¿ç”¨ FORCE INDEX å’Œ MySQL 5.7.27 å„ªåŒ–)
    query1 = f"""SELECT username, COUNT(*) as fail_count
                FROM audit_log FORCE INDEX (operation, username)
                WHERE operation='CONNECT' AND retcode!=0 AND {date_filter}
                GROUP BY username
                HAVING fail_count >= %s
                ORDER BY fail_count DESC
                LIMIT 1000"""
    by_user = execute_analysis_query(conn, query1, params, use_simple)
    
    # æŸ¥è©¢å¯ç–‘IP (ä½¿ç”¨ FORCE INDEX å’Œ MySQL 5.7.27 å„ªåŒ–)
    query2 = f"""SELECT host, COUNT(*) as fail_count
                FROM audit_log FORCE INDEX (operation, host)
                WHERE operation='CONNECT' AND retcode!=0 AND {date_filter}
                GROUP BY host
                HAVING fail_count >= %s
                ORDER BY fail_count DESC
                LIMIT 1000"""
    by_ip = execute_analysis_query(conn, query2, params, use_simple)
    
    # ç¸½å¤±æ•—æ¬¡æ•¸ (ä½¿ç”¨ FORCE INDEX)
    query3 = f"SELECT COUNT(*) FROM audit_log FORCE INDEX (operation) WHERE operation='CONNECT' AND retcode!=0 AND {date_filter}"
    total_result = execute_analysis_query(conn, query3, params2, use_simple)
    if total_result and len(total_result) > 0:
        total = total_result[0]['COUNT(*)'] if use_simple else total_result[0][0]
    else:
        total = 0
    
    return {
        'total': total,
        'by_user': by_user or [],
        'by_ip': by_ip or []
    }

def analyze_privileged_operations(conn, date_filter, date_filter_value, keywords, config=None, use_simple=False):
    """ç‰¹æ¬Šæ“ä½œåˆ†æ - ä½¿ç”¨å„ªåŒ–æŸ¥è©¢å’Œçµæœé™åˆ¶"""
    like_clauses = " OR ".join(["UPPER(query) LIKE %s" for _ in keywords])
    like_params = [f"%{k.upper()}%" for k in keywords]
    if isinstance(date_filter_value, tuple):
        params = like_params + list(date_filter_value)
    else:
        params = like_params + [date_filter_value]

    # æŒ‰ä½¿ç”¨è€…çµ±è¨ˆç‰¹æ¬Šæ“ä½œ (ä½¿ç”¨ FORCE INDEX å’Œ MySQL 5.7.27 å„ªåŒ–)
    query1 = f"""SELECT username, COUNT(*) as cnt
                FROM audit_log FORCE INDEX (operation, username)
                WHERE operation='QUERY' AND ({like_clauses}) AND {date_filter}
                GROUP BY username
                ORDER BY cnt DESC
                LIMIT 500"""
    by_user = execute_query_with_retry(conn, query1, params, config, 'fetchall_safe')

    # ç¸½ç‰¹æ¬Šæ“ä½œæ•¸é‡ (ä½¿ç”¨ FORCE INDEX)
    query2 = f"""SELECT COUNT(*) FROM audit_log FORCE INDEX (operation)
                WHERE operation='QUERY' AND ({like_clauses}) AND {date_filter}"""
    total_result = execute_query_with_retry(conn, query2, params, config, 'fetchone')
    total = total_result[0] if total_result else 0

    # è©³ç´°ç‰¹æ¬Šæ“ä½œè¨˜éŒ„ (ä½¿ç”¨ FORCE INDEX å’ŒæŸ¥è©¢å„ªåŒ–)
    query3 = f"""SELECT username, 
                        CASE WHEN CHAR_LENGTH(query) > 200 THEN CONCAT(LEFT(query, 200), '...') ELSE query END as query_short,
                        timestamp
                FROM audit_log FORCE INDEX (operation)
                WHERE operation='QUERY' AND ({like_clauses}) AND {date_filter}
                ORDER BY timestamp DESC
                LIMIT 1000"""
    details = execute_query_with_retry(conn, query3, params, config, 'fetchall_safe')

    return {
        'total': total,
        'by_user': by_user or [],
        'details': details or []
    }

def analyze_operation_stats(conn, date_filter, date_filter_value, config=None, use_simple=False):
    """æ“ä½œé¡å‹çµ±è¨ˆåˆ†æ - ä½¿ç”¨å„ªåŒ–æŸ¥è©¢"""
    if isinstance(date_filter_value, tuple):
        params = date_filter_value
    else:
        params = (date_filter_value,)
    
    # ä½¿ç”¨ FORCE INDEX å’Œ MySQL 5.7.27 å„ªåŒ–
    query = f"""SELECT operation, COUNT(*) as cnt
                FROM audit_log FORCE INDEX (operation)
                WHERE {date_filter}
                GROUP BY operation
                ORDER BY cnt DESC
                LIMIT 100"""
    
    result = execute_query_with_retry(conn, query, params, config, 'fetchall_safe')
    return result or []

def analyze_error_codes(conn, date_filter, date_filter_value, config=None, use_simple=False):
    """éŒ¯èª¤ä»£ç¢¼åˆ†æ - ä½¿ç”¨å„ªåŒ–æŸ¥è©¢"""
    if isinstance(date_filter_value, tuple):
        params = date_filter_value
    else:
        params = (date_filter_value,)
    
    # éŒ¯èª¤ä»£ç¢¼çµ±è¨ˆ (ä½¿ç”¨ FORCE INDEX å’Œ MySQL 5.7.27 å„ªåŒ–)
    query1 = f"""SELECT retcode, COUNT(*) as cnt
                FROM audit_log FORCE INDEX (operation)
                WHERE retcode!=0 AND operation!='CHANGEUSER' AND {date_filter}
                GROUP BY retcode
                ORDER BY cnt DESC
                LIMIT 100"""
    error_codes = execute_query_with_retry(conn, query1, params, config, 'fetchall_safe')
    
    # ç¸½éŒ¯èª¤æ•¸é‡ (ä½¿ç”¨ FORCE INDEX)
    query2 = f"SELECT COUNT(*) FROM audit_log FORCE INDEX (operation) WHERE retcode!=0 AND operation!='CHANGEUSER' AND {date_filter}"
    total_result = execute_query_with_retry(conn, query2, params, config, 'fetchone')
    total_errors = total_result[0] if total_result else 0
    
    return {
        'total_errors': total_errors,
        'error_codes': error_codes or []
    }

def analyze_after_hours_access(conn, date_filter, date_filter_value, users, wh_start, wh_end, config=None, use_simple=False):
    if not users:
        return {'total': 0, 'details': []}
    user_list = ','.join(["'%s'" % u for u in users])
    with conn.cursor() as cur:
        if isinstance(date_filter_value, tuple):
            params = date_filter_value
        else:
            params = (date_filter_value,)
        cur.execute(
            f"""SELECT username, host, operation, timestamp
                FROM audit_log
                WHERE username IN ({user_list}) AND {date_filter}
            """,
            params
        )
        rows = cur.fetchall()
        after_hours = []
        for username, host, operation, ts in rows:
            try:
                dt = datetime.strptime(ts, "%Y%m%d %H:%M:%S")
            except:
                continue
            if dt.weekday() >= 5 or not (wh_start <= dt.hour < wh_end):
                after_hours.append((username, host, operation, dt.strftime('%Y-%m-%d %H:%M:%S')))
        return {'total': len(after_hours), 'details': after_hours[:50]}

def analyze_privileged_user_logins(conn, date_filter, date_filter_value, users, config=None, use_simple=False):
    if not users:
        return {'total': 0, 'by_user': [], 'details': []}
    user_list = ','.join(["'%s'" % u for u in users])
    with conn.cursor() as cur:
        if isinstance(date_filter_value, tuple):
            params = date_filter_value
        else:
            params = (date_filter_value,)
        cur.execute(
            f"""SELECT username, COUNT(*) as cnt
                FROM audit_log
                WHERE operation='CONNECT' AND username IN ({user_list}) AND {date_filter}
                GROUP BY username
                ORDER BY cnt DESC
            """,
            params
        )
        by_user = cur.fetchall()
        cur.execute(
            f"""SELECT username, host, timestamp
                FROM audit_log
                WHERE operation='CONNECT' AND username IN ({user_list}) AND {date_filter}
                ORDER BY timestamp DESC
            """,
            params
        )
        details = cur.fetchall()
        cur.execute(
            f"""SELECT COUNT(*) FROM audit_log
                WHERE operation='CONNECT' AND username IN ({user_list}) AND {date_filter}
            """,
            params
        )
        total = cur.fetchone()[0]
        return {'total': total, 'by_user': by_user, 'details': details}

def analyze_non_whitelisted_ips(conn, date_filter, date_filter_value, allowed_ips, config=None, use_simple=False):
    if not allowed_ips:
        return {'total': 0, 'by_ip': [], 'details': []}
    ip_list = ','.join(["'%s'" % ip for ip in allowed_ips])
    with conn.cursor() as cur:
        if isinstance(date_filter_value, tuple):
            params = date_filter_value
        else:
            params = (date_filter_value,)
# çºŒå‰é¢çš„ç¨‹å¼ç¢¼...

        cur.execute(
            f"""SELECT host, COUNT(*) as cnt
                FROM audit_log
                WHERE host NOT IN ({ip_list}) AND operation!='CHANGEUSER' AND {date_filter}
                GROUP BY host
                ORDER BY cnt DESC
            """,
            params
        )
        by_ip = cur.fetchall()
        cur.execute(
            f"""SELECT username, host, operation, timestamp
                FROM audit_log
                WHERE host NOT IN ({ip_list}) AND operation!='CHANGEUSER' AND {date_filter}
                ORDER BY timestamp DESC
            """,
            params
        )
        details = cur.fetchall()
        cur.execute(
            f"""SELECT COUNT(*) FROM audit_log
                WHERE host NOT IN ({ip_list}) AND operation!='CHANGEUSER' AND {date_filter}
            """,
            params
        )
        total = cur.fetchone()[0]
        return {'total': total, 'by_ip': by_ip, 'details': details}


# ========== å ±è¡¨ç”¢ç”Ÿï¼ˆCSVï¼‰ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰ ==========

def generate_csv_report(output_dir, report_title, summary, failed, priv_ops, priv_user_logins, op_stats, err, after_hours, non_whitelisted, period_label):
    """
    ç”¢ç”Ÿ CSV å ±è¡¨ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰
    """
    os.makedirs(output_dir, exist_ok=True)
    csv_file = os.path.join(output_dir, f'mysql_audit_analysis_{period_label}.csv')
    
    # è¨ˆç®—ç¸½å…±è¦å¯«å…¥çš„å€å¡Šæ•¸é‡
    total_sections = 9
    
    if TQDM_AVAILABLE:
        progress_bar = tqdm(
            total=total_sections,
            desc="ğŸ“Š ç”¢ç”Ÿ CSV å ±è¡¨",
            unit="å€å¡Š",
            colour='yellow'
        )
    
    w = lambda row: writer.writerow(row)
    
    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # æ¨™é¡Œ
        w([f'{report_title} - {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}'])
        w([])
        
        # åŸºæœ¬çµ±è¨ˆ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥åŸºæœ¬çµ±è¨ˆ")
        w(['=== Basic Statistics ==='])
        w(['Total Events', summary['total_events']])
        w(['Unique Users', summary['unique_users']])
        w(['Unique Hosts', summary['unique_hosts']])
        w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # å¤±æ•—ç™»å…¥åˆ†æ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥å¤±æ•—ç™»å…¥åˆ†æ")
        w(['=== Failed Login Analysis ==='])
        w(['Total Failed Logins', failed['total']])
        w([])
        if failed['by_user']:
            w(['Suspicious Users (Above Threshold)'])
            w(['Username', 'Failed Count'])
            for row in failed['by_user']: 
                w(list(row))
            w([])
        if failed['by_ip']:
            w(['Suspicious IPs (Above Threshold)'])
            w(['IP Address', 'Failed Count'])
            for row in failed['by_ip']: 
                w(list(row))
            w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # ç‰¹æ¬Šæ“ä½œåˆ†æ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥ç‰¹æ¬Šæ“ä½œåˆ†æ")
        w(['=== Privileged Operations Analysis ==='])
        w(['Total Privileged Operations', priv_ops['total']])
        w([])
        if priv_ops['by_user']:
            w(['By User Statistics'])
            w(['Username', 'Operation Count'])
            for row in priv_ops['by_user']: 
                w(list(row))
            w([])
        if priv_ops.get('details'):
            w(['Detailed Privileged Operations (SQL)'])
            w(['Username', 'SQL', 'Timestamp'])
            for row in priv_ops['details']:
                w(list(row))
            w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # ç‰¹æ¬Šå¸³è™Ÿç™»å…¥åˆ†æ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥ç‰¹æ¬Šå¸³è™Ÿç™»å…¥åˆ†æ")
        w(['=== Privileged Account Login Analysis ==='])
        w(['Total Privileged Account Logins', priv_user_logins['total']])
        if priv_user_logins['by_user']:
            w(['Username', 'Login Count'])
            for row in priv_user_logins['by_user']: 
                w(list(row))
        w([])
        if priv_user_logins['details']:
            w(['Detailed Privileged Account Login Records'])
            w(['Username', 'Host', 'Timestamp'])
            for row in priv_user_logins['details']:
                w(list(row))
            w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # æ“ä½œé¡å‹çµ±è¨ˆ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥æ“ä½œé¡å‹çµ±è¨ˆ")
        w(['=== Operation Type Statistics ==='])
        w(['Operation Type', 'Count'])
        for row in op_stats: 
            w(list(row))
        w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # éŒ¯èª¤åˆ†æ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥éŒ¯èª¤åˆ†æ")
        w(['=== Error Analysis ==='])
        w(['Total Errors', err['total_errors']])
        w([])
        if err['error_codes']:
            w(['Error Code Statistics'])
            w(['Error Code', 'Count'])
            for row in err['error_codes']: 
                w(list(row))
        w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # éä¸Šç­æ™‚é–“å­˜å–åˆ†æ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥éä¸Šç­æ™‚é–“å­˜å–åˆ†æ")
        w(['=== After-hours Access (Specify account) ==='])
        w(['Total After-hours Access', after_hours['total']])
        if after_hours['details']:
            w(['Username', 'Host', 'Operation', 'Time'])
            for row in after_hours['details']:
                w(list(row))
        w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # éç™½åå–® IP å­˜å–åˆ†æ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥éç™½åå–® IP åˆ†æ")
        w(['=== Non-whitelisted IP Access Analysis ==='])
        w(['Total Events from Non-whitelisted IPs', non_whitelisted['total']])
        if non_whitelisted['by_ip']:
            w(['Non-whitelisted IPs'])
            w(['IP Address', 'Event Count'])
            for row in non_whitelisted['by_ip']: 
                w(list(row))
            w([])
        if non_whitelisted['details']:
            w(['Details (Username, Host, Operation, Time)'])
            for row in non_whitelisted['details']:
                w(list(row))
        w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # å®Œæˆ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š CSV å ±è¡¨å®Œæˆ")
            progress_bar.update(1)
    
    if TQDM_AVAILABLE:
        progress_bar.close()
    
    print(f"âœ… CSV report generated: {csv_file}")
    return csv_file

# ========== å ±è¡¨ç”¢ç”Ÿï¼ˆPDFï¼‰ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰ ==========

def generate_pdf_report(output_dir, report_title, summary, failed, priv_ops, priv_user_logins, op_stats, err, after_hours, non_whitelisted, period_label):
    """
    ç”¢ç”Ÿ PDF å ±è¡¨ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰
    """
    if not REPORTLAB_AVAILABLE:
        print("âŒ ReportLab not installed, PDF report cannot be generated.")
        return None
    
    os.makedirs(output_dir, exist_ok=True)
    pdf_file = os.path.join(output_dir, f'mysql_audit_analysis_{period_label}.pdf')
    
    print("ğŸ“„ æ­£åœ¨ç”¢ç”Ÿ PDF å ±è¡¨...")
    start_time = datetime.now()
    
    doc = SimpleDocTemplate(pdf_file, pagesize=A4)
    styles = getSampleStyleSheet()
    story = []
    
    # æ¨™é¡Œ
    story.append(Paragraph(f"<b>{report_title}</b>", styles['Title']))
    story.append(Spacer(1, 12))
    story.append(Paragraph(f"Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", styles['Normal']))
    story.append(Spacer(1, 8))

    def add_table(title, data, colnames):
        story.append(Spacer(1, 8))
        story.append(Paragraph(f"<b>{title}</b>", styles['Heading3']))
        if not data:
            story.append(Paragraph("(No data)", styles['Normal']))
        else:
            table = Table([colnames] + list(data), hAlign='LEFT')
            table.setStyle(TableStyle([
                ('BACKGROUND', (0,0), (-1,0), colors.lightgrey),
                ('GRID', (0,0), (-1,-1), 0.5, colors.grey),
                ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),
            ]))
            story.append(table)

    # åŸºæœ¬çµ±è¨ˆ
    story.append(Paragraph("<b>=== Basic Statistics ===</b>", styles['Heading2']))
    story.append(Paragraph(f"Total Events: {summary['total_events']}<br />"
                        f"Unique Users: {summary['unique_users']}<br />"
                        f"Unique Hosts: {summary['unique_hosts']}", styles['Normal']))
    
    # å„ç¨®åˆ†æè¡¨æ ¼
    add_table("Suspicious Users (Failed Logins)", failed['by_user'], ['Username', 'Failed Count'])
    add_table("Suspicious IPs (Failed Logins)", failed['by_ip'], ['IP Address', 'Failed Count'])
    add_table("Privileged Operations by User", priv_ops['by_user'], ['Username', 'Operation Count'])
    add_table("Detailed Privileged Operations (SQL)", priv_ops.get('details', []), ['Username', 'SQL', 'Timestamp'])
    add_table("Privileged Account Login Statistics", priv_user_logins['by_user'], ['Username', 'Login Count'])
    add_table("Privileged Account Login Details", priv_user_logins['details'], ['Username', 'IP Address', 'Timestamp'])
    add_table("Operation Type Statistics", op_stats, ['Operation', 'Count'])
    add_table("Error Code Statistics", err['error_codes'], ['Error Code', 'Count'])
    add_table("After-hours Access (Specify account)", after_hours['details'], ['Username', 'IP Address', 'Operation', 'Timestamp'])
    add_table("Non-whitelisted IPs", non_whitelisted['by_ip'], ['IP Address', 'Event Count'])
    add_table("Non-whitelisted IP Access Details", non_whitelisted['details'], ['Username', 'IP Address', 'Operation', 'Timestamp'])

    doc.build(story)
    
    duration = (datetime.now() - start_time).total_seconds()
    print(f"âœ… PDF report generated: {pdf_file} (è€—æ™‚ {duration:.2f} ç§’)")
    return pdf_file

# ========== éƒµä»¶å¯„é€ ==========

def send_email_with_attachment(config: Config, subject, body, attachment_path):
    if not (config.smtp_server and config.mail_from and config.mail_to):
        print("âŒ SMTP æˆ–æ”¶ä»¶äººè¨­å®šä¸å®Œæ•´ï¼Œç„¡æ³•å¯„ä¿¡ã€‚")
        return
    
    print("ğŸ“§ æ­£åœ¨å¯„é€éƒµä»¶...")
    start_time = datetime.now()
    
    msg = EmailMessage()
    msg['Subject'] = subject
    msg['From'] = config.mail_from
    msg['To'] = ', '.join(config.mail_to)
    msg.set_content(body)
    
    with open(attachment_path, 'rb') as f:
        file_data = f.read()
        file_name = os.path.basename(attachment_path)
    
    maintype, subtype = ('application', 'pdf') if file_name.endswith('.pdf') else ('application', 'octet-stream')
    msg.add_attachment(file_data, maintype=maintype, subtype=subtype, filename=file_name)
    
    try:
        with smtplib.SMTP(config.smtp_server, config.smtp_port) as server:
            server.send_message(msg)
        
        duration = (datetime.now() - start_time).total_seconds()
        print(f"ğŸ“§ éƒµä»¶å·²å¯„å‡ºè‡³: {', '.join(config.mail_to)} (è€—æ™‚ {duration:.2f} ç§’)")
        
    except Exception as e:
        print(f"âŒ éƒµä»¶å¯„é€å¤±æ•—: {e}")

# ========== ä¸»ç¨‹å¼ï¼ˆåŠ å…¥å®Œæ•´çš„é€²åº¦è¿½è¹¤ï¼‰ ==========

def main():
    parser = argparse.ArgumentParser(description='MySQL Audit Log Security Analyzer (MySQL backend) - Enhanced with Progress Tracking')
    parser.add_argument('--import-date', help='Import logs for specific date (format: YYYY-MM-DD)')
    parser.add_argument('--import-month', help='Import logs for specific month (format: YYYY-MM)')
    parser.add_argument('--analyze-date', help='Analyze logs for specific date (format: YYYY-MM-DD)')
    parser.add_argument('--analyze-month', help='Analyze logs for specific month (format: YYYY-MM)')
    parser.add_argument('--output-dir', help='Output directory')
    parser.add_argument('--csv-only', action='store_true', help='Generate CSV report only')
    parser.add_argument('--show-env', action='store_true', help='Show all env/config parameters and exit')
    parser.add_argument('--disable-load-data', action='store_true', help='Disable LOAD DATA INFILE optimization')
    parser.add_argument('--disable-progress', action='store_true', help='Disable progress bars (useful for automation)')
    
    args = parser.parse_args()
    config = Config()
    
    # å¦‚æœæŒ‡å®šäº† --disable-progressï¼Œå‰‡å…¨åŸŸåœç”¨ tqdm
    if args.disable_progress:
        global TQDM_AVAILABLE
        TQDM_AVAILABLE = False
        print("âš ï¸  å·²åœç”¨é€²åº¦æ¢é¡¯ç¤º")
    
    # å¦‚æœæŒ‡å®šäº† --disable-load-dataï¼Œå‰‡é—œé–‰å„ªåŒ–
    if args.disable_load_data:
        config.use_load_data_infile = False
        print("âš ï¸  å·²åœç”¨ LOAD DATA INFILE å„ªåŒ–")

    # é¡¯ç¤ºæ‰€æœ‰ env è¨­å®š
    if args.show_env:
        print("ğŸ” ç›®å‰æŠ“åˆ°çš„ .env/ç’°å¢ƒè®Šæ•¸åƒæ•¸å¦‚ä¸‹ï¼š\n")
        for k, v in config.as_dict().items():
            print(f"{k}: {v}")
        return

    # ç¨‹å¼é–‹å§‹åŸ·è¡Œæ™‚é–“å’Œçµ±è¨ˆåˆå§‹åŒ–
    program_start_time = datetime.now()
    print(f"ğŸš€ MySQL 5.7.27 ç¨¿æ ¸æ—¥èªŒåˆ†æç¨‹å¼é–‹å§‹åŸ·è¡Œ: {program_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ç¨‹å¼åŸ·è¡Œçµ±è¨ˆ
    execution_stats = {
        'start_time': program_start_time,
        'import_files': 0,
        'import_success': 0,
        'import_failed': 0,
        'total_records_imported': 0,
        'analysis_queries': 0,
        'analysis_time': 0,
        'errors': [],
        'warnings': []
    }
    
    # åˆå§‹åŒ–è³‡æºç›£æ§
    try:
        init_resource_monitoring(config)
    except Exception as e:
        print(f"âš ï¸  è³‡æºç›£æ§åˆå§‹åŒ–å¤±æ•—: {e}")
    
    # åˆå§‹åŒ–é€£ç·šæ± 
    try:
        init_connection_pool(config)
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«é€£ç·šæ± åˆå§‹åŒ–å¤±æ•—: {e}")
        print("ğŸ”„ å˜—è©¦ä½¿ç”¨å‚³çµ±é€£ç·šæ–¹å¼...")
        try:
            test_conn = get_legacy_db_conn(config)
            test_conn.close()
            print("âœ… è³‡æ–™åº«é€£ç·šæ¸¬è©¦æˆåŠŸ")
        except Exception as e2:
            print(f"âŒ è³‡æ–™åº«å®Œå…¨ç„¡æ³•é€£ç·š: {e2}")
            return

    # åŒ¯å…¥æ—¥èªŒ
    if args.import_date:
        print(f"\nğŸ“… é–‹å§‹åŒ¯å…¥å–®æ—¥æ—¥èªŒ: {args.import_date}")
        log = get_log_file_for_date(config, args.import_date)
        if log:
            try:
                conn = get_legacy_db_conn(config)  # åŒ¯å…¥ä½¿ç”¨å‚³çµ±é€£ç·š
                import_log_file_to_db(log[0], log[1], conn, config)
                conn.close()
            except Exception as e:
                print(f"âŒ åŒ¯å…¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
                if 'conn' in locals():
                    conn.close()
        else:
            print(f"âŒ No log file found for {args.import_date}")
        
        total_duration = (datetime.now() - program_start_time).total_seconds()
        print(f"\nğŸ‰ å–®æ—¥åŒ¯å…¥å®Œæˆï¼ç¸½è€—æ™‚: {total_duration:.2f} ç§’")
        return
        
    elif args.import_month:
        print(f"\nğŸ“… é–‹å§‹åŒ¯å…¥æœˆä»½æ—¥èªŒ: {args.import_month}")
        logs = get_log_files_for_month(config, args.import_month)
        if not logs:
            print(f"âŒ No log files found for {args.import_month}")
            return
        
        total_files = len(logs)
        print(f"ğŸ“ æ‰¾åˆ° {total_files} å€‹æ—¥èªŒæª”æ¡ˆ")
        
        # æœˆä»½åŒ¯å…¥é€²åº¦æ¢
        if TQDM_AVAILABLE:
            month_progress = tqdm(
                total=total_files,
                desc="ğŸ“‚ æœˆä»½åŒ¯å…¥é€²åº¦",
                unit="æª”æ¡ˆ",
                colour='green'
            )
        
        import_stats = {
            'total_files': 0,
            'success_files': 0,
            'failed_files': 0,
            'total_records': 0
        }
        
        conn = None
        try:
            conn = get_legacy_db_conn(config)  # æœˆä»½åŒ¯å…¥ä½¿ç”¨å‚³çµ±é€£ç·š
            
            for i, (log_path, log_date) in enumerate(logs, 1):
                if not TQDM_AVAILABLE:
                    print(f"\nğŸ“ è™•ç†æª”æ¡ˆ {i}/{total_files}: {os.path.basename(log_path)}")
                else:
                    month_progress.set_description(f"ğŸ“ è™•ç†: {os.path.basename(log_path)}")
                
                try:
                    import_log_file_to_db(log_path, log_date, conn, config)
                    import_stats['success_files'] += 1
                except Exception as e:
                    print(f"âŒ æª”æ¡ˆ {log_path} åŒ¯å…¥å¤±æ•—: {e}")
                    import_stats['failed_files'] += 1
        except Exception as e:
            print(f"âŒ æœˆä»½åŒ¯å…¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            if conn:
                conn.close()
        
        # çµ±è¨ˆè™•ç†
        import_stats['total_files'] = len(logs)
        
        if TQDM_AVAILABLE:
            month_progress.close()
        
        total_duration = (datetime.now() - program_start_time).total_seconds()
        print(f"\nğŸ‰ æœˆä»½åŒ¯å…¥å®Œæˆï¼")
        print(f"   ğŸ“ ç¸½æª”æ¡ˆæ•¸: {import_stats['total_files']}")
        print(f"   âœ… æˆåŠŸæª”æ¡ˆ: {import_stats['success_files']}")
        print(f"   âŒ å¤±æ•—æª”æ¡ˆ: {import_stats['failed_files']}")
        print(f"   â±ï¸  ç¸½è€—æ™‚: {total_duration:.2f} ç§’")
        print(f"   ğŸ“Š å¹³å‡æ¯æª”: {total_duration/import_stats['total_files']:.2f} ç§’")
        return

    # åˆ†æéšæ®µ
    print(f"\nğŸ” é–‹å§‹é€²è¡Œå®‰å…¨åˆ†æ...")
    analysis_start_time = datetime.now()
    
  # ä»¥ timestamp æ¬„ä½ç‚ºä¸»é€²è¡ŒæŸ¥è©¢
    if args.analyze_month:
        year, month = map(int, args.analyze_month.split('-'))
        days_in_month = calendar.monthrange(year, month)[1]
        ts_start = f"{year:04d}{month:02d}01 00:00:00"
        ts_end = f"{year:04d}{month:02d}{days_in_month:02d} 23:59:59"
        period_label = args.analyze_month.replace('-', '')
        date_filter = "timestamp BETWEEN %s AND %s"
        date_filter_value = (ts_start, ts_end)
        print(f"ğŸ“Š åˆ†ææœŸé–“: {args.analyze_month} ({ts_start} åˆ° {ts_end})")
    else:
        date_str = args.analyze_date if args.analyze_date else datetime.now().strftime('%Y-%m-%d')
        y, m, d = map(int, date_str.split('-'))
        ts_start = f"{y:04d}{m:02d}{d:02d} 00:00:00"
        ts_end = f"{y:04d}{m:02d}{d:02d} 23:59:59"
        period_label = date_str.replace('-', '')
        date_filter = "timestamp BETWEEN %s AND %s"
        date_filter_value = (ts_start, ts_end)
        print(f"ğŸ“Š åˆ†ææ—¥æœŸ: {date_str} ({ts_start} åˆ° {ts_end})")

    # å®šç¾©æ‰€æœ‰åˆ†æåŠŸèƒ½
    analysis_functions = [
        ("åŸºæœ¬çµ±è¨ˆ", analyze_summary, None),
        ("å¤±æ•—ç™»å…¥åˆ†æ", analyze_failed_logins, (config.failed_login_threshold,)),
        ("ç‰¹æ¬Šæ“ä½œåˆ†æ", analyze_privileged_operations, (config.privileged_keywords,)),
        ("æ“ä½œé¡å‹çµ±è¨ˆ", analyze_operation_stats, None),
        ("éŒ¯èª¤ä»£ç¢¼åˆ†æ", analyze_error_codes, None),
        ("éä¸Šç­æ™‚é–“å­˜å–", analyze_after_hours_access, (config.after_hours_users, config.work_hour_start, config.work_hour_end)),
        ("ç‰¹æ¬Šå¸³è™Ÿç™»å…¥", analyze_privileged_user_logins, (config.privileged_users,)),
        ("éç™½åå–®IPåˆ†æ", analyze_non_whitelisted_ips, (config.allowed_ips,))
    ]
    
    # åŸ·è¡Œåˆ†æ (ä½¿ç”¨ç°¡å–®é€£ç·š)
    conn = None
    try:
        print("ğŸ”— å»ºç«‹è³‡æ–™åº«é€£ç·š...")
        conn = get_simple_db_conn(config)
        print("âœ… è³‡æ–™åº«é€£ç·šæˆåŠŸ")
        
        # å…ˆæª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™
        print("ğŸ” æª¢æŸ¥è³‡æ–™æ˜¯å¦å­˜åœ¨...")
        check_query = f"SELECT COUNT(*) as count FROM audit_log WHERE {date_filter}"
        check_result = execute_simple_query(conn, check_query, date_filter_value)
        
        if not check_result or check_result[0]['count'] == 0:
            print("âš ï¸  æŒ‡å®šæ—¥æœŸç¯„åœå…§æ²’æœ‰è³‡æ–™ï¼Œè·³éåˆ†æ")
            return
            
        print(f"âœ… æ‰¾åˆ° {check_result[0]['count']:,} ç­†è³‡æ–™ï¼Œé–‹å§‹åˆ†æ...")
        results = run_analysis_with_progress(analysis_functions, conn, date_filter, date_filter_value, config, use_simple=True)
    except Exception as e:
        print(f"âŒ åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        return
    finally:
        if conn:
            conn.close()
            print("ğŸ”— è³‡æ–™åº«é€£ç·šå·²é—œé–‰")
    
    # è§£æ§‹çµæœ
    summary = results.get("åŸºæœ¬çµ±è¨ˆ", {})
    failed = results.get("å¤±æ•—ç™»å…¥åˆ†æ", {})
    priv_ops = results.get("ç‰¹æ¬Šæ“ä½œåˆ†æ", {})
    op_stats = results.get("æ“ä½œé¡å‹çµ±è¨ˆ", [])
    err = results.get("éŒ¯èª¤ä»£ç¢¼åˆ†æ", {})
    after_hours = results.get("éä¸Šç­æ™‚é–“å­˜å–", {})
    priv_user_logins = results.get("ç‰¹æ¬Šå¸³è™Ÿç™»å…¥", {})
    non_whitelisted = results.get("éç™½åå–®IPåˆ†æ", {})
    
    analysis_duration = (datetime.now() - analysis_start_time).total_seconds()
    print(f"âœ… å®‰å…¨åˆ†æå®Œæˆï¼Œè€—æ™‚ {analysis_duration:.2f} ç§’")

    # å ±è¡¨ç”¢ç”Ÿéšæ®µ
    print(f"\nğŸ“Š é–‹å§‹ç”¢ç”Ÿå ±è¡¨...")
    report_start_time = datetime.now()
    
    output_dir = args.output_dir or config.output_dir
    csv_file = None
    pdf_file = None
    
    if config.generate_csv:
        csv_file = generate_csv_report(output_dir, config.report_title, summary, failed, priv_ops, priv_user_logins, op_stats, err, after_hours, non_whitelisted, period_label)
    
    if config.generate_pdf and not args.csv_only:
        pdf_file = generate_pdf_report(output_dir, config.report_title, summary, failed, priv_ops, priv_user_logins, op_stats, err, after_hours, non_whitelisted, period_label)
        
        if pdf_file and config.send_email:
            # å–å¾—åˆ†ææœŸé–“çš„æ—¥æœŸè³‡è¨Š
            if args.analyze_month:
                year, month = map(int, args.analyze_month.split('-'))
                period_text = f"{year}å¹´{month:02d}æœˆ"
            else:
                date_str = args.analyze_date if args.analyze_date else datetime.now().strftime('%Y-%m-%d')
                year, month, day = map(int, date_str.split('-'))
                period_text = f"{year}å¹´{month:02d}æœˆ{day:02d}æ—¥"
            
            send_email_with_attachment(
                config,
                subject=f"HamiPass MySQL ç¨½æ ¸æ—¥èªŒå®‰å…¨åˆ†æå ±å‘Š ({period_label})",
                body=f"æª¢é™„ HamiPass MySQL ç¨½æ ¸æ—¥èªŒå®‰å…¨åˆ†æå ±å‘Šï¼Œåˆ†ææœŸé–“ç‚º {period_text}ã€‚",
                attachment_path=pdf_file
            )
        elif pdf_file:
            print("âœ‰ï¸  SEND_EMAIL=falseï¼Œæœªé€²è¡Œéƒµä»¶å¯„é€ã€‚")
    elif args.csv_only:
        print("âœ‰ï¸  å·²æŒ‡å®š --csv-onlyï¼Œä¸ç”¢ç”ŸPDFäº¦ä¸å¯„ä¿¡ã€‚")
    else:
        print("âœ‰ï¸  æœªç”¢ç”ŸPDFï¼Œä¸é€²è¡Œå¯„ä¿¡ã€‚")

    report_duration = (datetime.now() - report_start_time).total_seconds()
    print(f"âœ… å ±è¡¨ç”¢ç”Ÿå®Œæˆï¼Œè€—æ™‚ {report_duration:.2f} ç§’")

    # ç¸½çµ
    total_duration = (datetime.now() - program_start_time).total_seconds()
    
    print("\n" + "="*60)
    print("ğŸ“Š Analysis Result Summary")
    print("="*60)
    print(f"ğŸ“… åˆ†ææœŸé–“: {period_label}")
    print(f"ğŸ“ˆ ç¸½äº‹ä»¶æ•¸: {summary.get('total_events', 0):,}")
    print(f"ğŸ‘¥ ç¨ç‰¹ä½¿ç”¨è€…: {summary.get('unique_users', 0):,}")
    print(f"ğŸ–¥ï¸  ç¨ç‰¹ä¸»æ©Ÿ: {summary.get('unique_hosts', 0):,}")
    print(f"âŒ å¤±æ•—ç™»å…¥: {failed.get('total', 0):,}")
    print(f"ğŸ” ç‰¹æ¬Šæ“ä½œ: {priv_ops.get('total', 0):,}")
    print(f"ğŸ‘‘ ç‰¹æ¬Šå¸³è™Ÿç™»å…¥: {priv_user_logins.get('total', 0):,}")
    print(f"âš ï¸  éŒ¯èª¤äº‹ä»¶: {err.get('total_errors', 0):,}")
    print(f"ğŸš« éç™½åå–®IPäº‹ä»¶: {non_whitelisted.get('total', 0):,}")
    
    if failed.get('by_user'):
        print(f"âš ï¸  å¯ç–‘ä½¿ç”¨è€…: {len(failed['by_user'])}")
    if failed.get('by_ip'):
        print(f"âš ï¸  å¯ç–‘IP: {len(failed['by_ip'])}")
    if non_whitelisted.get('by_ip'):
        print(f"âš ï¸  éç™½åå–®IP: {len(non_whitelisted['by_ip'])}")
    if after_hours.get('total'):
        print(f"âš ï¸  éä¸Šç­æ™‚é–“å­˜å–: {after_hours['total']}")
    
    print("\n" + "="*60)
    print("â±ï¸  åŸ·è¡Œæ™‚é–“çµ±è¨ˆ")
    print("="*60)
    print(f"ğŸ” åˆ†æè€—æ™‚: {analysis_duration:.2f} ç§’")
    print(f"ğŸ“Š å ±è¡¨è€—æ™‚: {report_duration:.2f} ç§’")
    print(f"ğŸ•’ ç¸½åŸ·è¡Œæ™‚é–“: {total_duration:.2f} ç§’")
    print(f"ğŸ ç¨‹å¼çµæŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æœ€çµ‚æ¸…ç†å’Œçµ±è¨ˆè¼¸å‡º
    execution_stats['end_time'] = datetime.now()
    execution_stats['total_duration'] = (execution_stats['end_time'] - execution_stats['start_time']).total_seconds()
    
    # é¡¯ç¤ºåŸ·è¡Œçµ±è¨ˆ
    print("\n" + "="*60)
    print("ğŸ“ˆ MySQL 5.7.27 Program Execution Statistics")
    print("="*60)
    print(f"â° åŸ·è¡Œæ™‚é–“: {execution_stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')} - {execution_stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ•’ ç¸½è€—æ™‚: {execution_stats['total_duration']:.2f} ç§’")
    if execution_stats['import_files'] > 0:
        print(f"ğŸ“ åŒ¯å…¥çµ±è¨ˆ: {execution_stats['import_success']}/{execution_stats['import_files']} æª”æ¡ˆæˆåŠŸ")
        print(f"ğŸ“ˆ åŒ¯å…¥æˆåŠŸç‡: {execution_stats['import_success']/execution_stats['import_files']*100:.1f}%")
    if execution_stats['total_records_imported'] > 0:
        print(f"ğŸ“Š ç¸½åŒ¯å…¥è¨˜éŒ„: {execution_stats['total_records_imported']:,} ç­†")
        print(f"âš¡ åŒ¯å…¥é€Ÿåº¦: {execution_stats['total_records_imported']/execution_stats['total_duration']:.0f} ç­†/ç§’")
    if execution_stats['analysis_queries'] > 0:
        print(f"ğŸ” åˆ†ææŸ¥è©¢: {execution_stats['analysis_queries']} å€‹ (è€—æ™‚ {execution_stats['analysis_time']:.2f} ç§’)")
        if execution_stats['analysis_time'] > 0:
            print(f"âš¡ æŸ¥è©¢æ•ˆç‡: {execution_stats['analysis_queries']/execution_stats['analysis_time']:.1f} æŸ¥è©¢/ç§’")
    
    # éŒ¯èª¤å’Œè­¦å‘Šç¸½çµ
    if execution_stats['errors']:
        print(f"\nâŒ éŒ¯èª¤ç¸½æ•¸: {len(execution_stats['errors'])}")
        for i, error in enumerate(execution_stats['errors'][:3], 1):  # åªé¡¯ç¤ºå‰3å€‹éŒ¯èª¤
            print(f"   {i}. {error[:100]}{'...' if len(error) > 100 else ''}")
        if len(execution_stats['errors']) > 3:
            print(f"   ... åŠå…¶ä»– {len(execution_stats['errors']) - 3} å€‹éŒ¯èª¤")
            
    if execution_stats['warnings']:
        print(f"\nâš ï¸  è­¦å‘Šç¸½æ•¸: {len(execution_stats['warnings'])}")
        for i, warning in enumerate(execution_stats['warnings'][:3], 1):  # åªé¡¯ç¤ºå‰3å€‹è­¦å‘Š
            print(f"   {i}. {warning}")
        if len(execution_stats['warnings']) > 3:
            print(f"   ... åŠå…¶ä»– {len(execution_stats['warnings']) - 3} å€‹è­¦å‘Š")
    
    # æœ€çµ‚æ¸…ç†
    try:
        close_connection_pool()
    except Exception as e:
        print(f"âš ï¸  é€£ç·šæ± æ¸…ç†è­¦å‘Š: {e}")
    
    # æ ¹æ“šåŸ·è¡Œçµæœè¿”å›é©ç•¶çš„é€€å‡ºç¢¼
    if execution_stats['errors']:
        print("\nâš ï¸  ç¨‹å¼åŸ·è¡Œå®Œæˆï¼Œä½†æœ‰éŒ¯èª¤ç™¼ç”Ÿ")
        sys.exit(1)
    elif execution_stats['warnings']:
        print("\nâœ… ç¨‹å¼åŸ·è¡Œå®Œæˆï¼Œä½†æœ‰è­¦å‘Š")
    else:
        print("\nğŸ‰ ç¨‹å¼åŸ·è¡Œå®Œæˆï¼Œç„¡éŒ¯èª¤")

if __name__ == "__main__":
    main()
