


#!/usr/bin/env python3




# -*- coding: utf-8 -*-

é¦–å…ˆæª¢æŸ¥ç¾æœ‰ç´¢å¼•ç‹€æ³ï¼š
USE auditdb;
SHOW INDEX FROM audit_log;




æŒ‰é‡è¦æ€§é †åºæ‰‹å‹•å»ºç«‹ç´¢å¼•ï¼š




1.
æ™‚é–“æˆ³è¨˜ç´¢å¼•ï¼ˆæœ€é‡è¦ï¼‰ï¼š
CREATE INDEX idx_timestamp ON audit_log (timestamp);ç”¨é€”ï¼šæ‰€æœ‰æ—¥æœŸç¯„åœæŸ¥è©¢çš„åŸºç¤ç´¢å¼•

2.
å¤±æ•—ç™»å…¥åˆ†æç´¢å¼•ï¼š
CREATE INDEX idx_failed_login ON audit_log (operation,retcode, timestamp);
ç”¨é€”ï¼šåµæ¸¬æš´åŠ›ç ´è§£æ”»æ“Šå’Œå¯ç–‘ç™»å…¥æ´»å‹•

3.
ç‰¹æ¬Šæ“ä½œåˆ†æç´¢å¼•ï¼š
CREATE INDEX idx_privileged_ops ON audit_log
(operation, timestamp);
ç”¨é€”ï¼šç›£æ§ç®¡ç†å“¡æ¬Šé™æ“ä½œå’Œç‰¹æ¬Šæå‡

4.
ä½¿ç”¨è€…æ´»å‹•åˆ†æç´¢å¼•ï¼š
CREATE INDEX idx_username_timestamp ON audit_log
(username, timestamp);
ç”¨é€”ï¼šä½¿ç”¨è€…è¡Œç‚ºåˆ†æå’Œå…§éƒ¨å¨è„…åµæ¸¬

5.
ä¸»æ©Ÿ/IPåˆ†æç´¢å¼•ï¼š
CREATE INDEX idx_host_operation ON audit_log (host,operation, timestamp);
ç”¨é€”ï¼šç¶²è·¯å®‰å…¨åˆ†æå’Œæœªæˆæ¬ŠIPåµæ¸¬

6.
éŒ¯èª¤åˆ†æç´¢å¼•ï¼š
CREATE INDEX idx_retcode_operation ON audit_log
(retcode, operation, timestamp);
ç”¨é€”ï¼šç³»çµ±éŒ¯èª¤ç›£æ§å’Œç•°å¸¸è¡Œç‚ºåˆ†æ




å»ºç«‹å®Œæˆå¾Œçš„æª¢æŸ¥ï¼š
-- æŸ¥çœ‹æ‰€æœ‰ç´¢å¼•
SHOW INDEX FROM audit_log;




-- æ›´æ–°çµ±è¨ˆè³‡è¨Š
ANALYZE TABLE audit_log;




ä½ å¯ä»¥å…ˆåŸ·è¡Œå‰ 3
å€‹æœ€é‡è¦çš„ç´¢å¼•ï¼Œæ¸¬è©¦æ•ˆèƒ½å¾Œå†æ±ºå®šæ˜¯å¦ç¹¼çºŒå»ºç«‹å…¶ä»–ç´¢å¼•ã€‚æ¯åŸ·è¡Œä¸€å€‹å°±å¯ä»¥æ¸¬è©¦ä¸€ä¸‹æŸ¥è©¢æ•ˆèƒ½çš„æ”¹å–„ã€‚







import os, sys, argparse, gzip, csv, calendar, tempfile, time, psutil, threading




from datetime import datetime, timedelta




from typing import List, Optional




import pymysql




import smtplib




from email.message import EmailMessage




from contextlib import contextmanager




import queue




import threading




# å…¨åŸŸè³‡æºç›£æ§è®Šæ•¸




_resource_monitor = None




_query_semaphore = None




class ResourceMonitor:




Â  Â  """è³‡æºç›£æ§é¡åˆ¥ - ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨é‡ä¸¦æä¾›å‘Šè­¦"""




Â  Â  def __init__(self, max_memory_mb=1024):




Â  Â  Â  Â  self.max_memory_mb = max_memory_mb




Â  Â  Â  Â  self.max_memory_bytes = max_memory_mb * 1024 * 1024




Â  Â  Â  Â  self.process = psutil.Process()




Â  Â  Â  Â  self.monitoring = False




Â  Â  def get_memory_usage_mb(self):




Â  Â  Â  Â  """å–å¾—ç›®å‰è¨˜æ†¶é«”ä½¿ç”¨é‡ (MB)"""




Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  memory_info = self.process.memory_info()




Â  Â  Â  Â  Â  Â  return memory_info.rss / 1024 / 1024




Â  Â  Â  Â  except:




Â  Â  Â  Â  Â  Â  return 0




Â  Â  def check_memory_limit(self):




Â  Â  Â  Â  """æª¢æŸ¥è¨˜æ†¶é«”æ˜¯å¦è¶…éé™åˆ¶"""




Â  Â  Â  Â  current_mb = self.get_memory_usage_mb()




Â  Â  Â  Â  if current_mb > self.max_memory_mb:




Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Â è¨˜æ†¶é«”ä½¿ç”¨é‡è­¦å‘Š: {current_mb:.1f}MB / {self.max_memory_mb}MB")




Â  Â  Â  Â  Â  Â  return False




Â  Â  Â  Â  return True




Â  Â  def force_gc_if_needed(self):




Â  Â  Â  Â  """å¿…è¦æ™‚å¼·åˆ¶åƒåœ¾å›æ”¶"""




Â  Â  Â  Â  import gc




Â  Â  Â  Â  if not self.check_memory_limit():




Â  Â  Â  Â  Â  Â  print("ğŸ—‘ï¸ Â åŸ·è¡Œåƒåœ¾å›æ”¶ä»¥é‡‹æ”¾è¨˜æ†¶é«”...")




Â  Â  Â  Â  Â  Â  gc.collect()




Â  Â  Â  Â  Â  Â  time.sleep(0.5) Â # çµ¦ç³»çµ±æ™‚é–“é‡‹æ”¾è¨˜æ†¶é«”




Â  Â  Â  Â  Â  Â  return True




Â  Â  Â  Â  return False




class SimpleConnectionPool:




Â  Â  """ç°¡å–®çš„ MySQL é€£æ¥æ± å¯¦ç¾"""




Â  Â  def __init__(self, host, port, user, password, database, charset='utf8mb4',




Â  Â  Â  Â  Â  Â  Â  Â  Â max_connections=10, **kwargs):




Â  Â  Â  Â  self.host = host




Â  Â  Â  Â  self.port = port




Â  Â  Â  Â  self.user = user




Â  Â  Â  Â  self.password = password




Â  Â  Â  Â  self.database = database




Â  Â  Â  Â  self.charset = charset




Â  Â  Â  Â  self.max_connections = max_connections




Â  Â  Â  Â  self.kwargs = kwargs




Â  Â  Â  Â  self._pool = queue.Queue(maxsize=max_connections)




Â  Â  Â  Â  self._lock = threading.Lock()




Â  Â  Â  Â  self._created_connections = 0




Â  Â  def get_connection(self):




Â  Â  Â  Â  """å¾é€£æ¥æ± ç²å–é€£æ¥"""




Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  # å˜—è©¦å¾æ± ä¸­ç²å–ç¾æœ‰é€£æ¥




Â  Â  Â  Â  Â  Â  conn = self._pool.get_nowait()




Â  Â  Â  Â  Â  Â  # æª¢æŸ¥é€£æ¥æ˜¯å¦ä»ç„¶æœ‰æ•ˆ




Â  Â  Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  Â  Â  conn.ping(reconnect=True)




Â  Â  Â  Â  Â  Â  Â  Â  return conn




Â  Â  Â  Â  Â  Â  except:




Â  Â  Â  Â  Â  Â  Â  Â  # é€£æ¥å·²å¤±æ•ˆï¼Œå‰µå»ºæ–°é€£æ¥




Â  Â  Â  Â  Â  Â  Â  Â  pass




Â  Â  Â  Â  except queue.Empty:




Â  Â  Â  Â  Â  Â  pass




Â  Â  Â  Â  # å‰µå»ºæ–°é€£æ¥




Â  Â  Â  Â  with self._lock:




Â  Â  Â  Â  Â  Â  if self._created_connections < self.max_connections:




Â  Â  Â  Â  Â  Â  Â  Â  conn = pymysql.connect(




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  host=self.host,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  port=self.port,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  user=self.user,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  password=self.password,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  database=self.database,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  charset=self.charset,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  autocommit=True,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  local_infile=True,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  **self.kwargs




Â  Â  Â  Â  Â  Â  Â  Â  )




Â  Â  Â  Â  Â  Â  Â  Â  self._created_connections += 1




Â  Â  Â  Â  Â  Â  Â  Â  return conn




Â  Â  Â  Â  # æ± å·²æ»¿ï¼Œç­‰å¾…å¯ç”¨é€£æ¥




Â  Â  Â  Â  return self._pool.get(timeout=30)




Â  Â  def release_connection(self, conn):




Â  Â  Â  Â  """é‡‹æ”¾é€£æ¥å›æ± ä¸­"""




Â  Â  Â  Â  if conn and conn.open:




Â  Â  Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  Â  Â  self._pool.put_nowait(conn)




Â  Â  Â  Â  Â  Â  except queue.Full:




Â  Â  Â  Â  Â  Â  Â  Â  # æ± å·²æ»¿ï¼Œé—œé–‰é€£æ¥




Â  Â  Â  Â  Â  Â  Â  Â  conn.close()




# åŠ å…¥ tqdm æ”¯æ´




try:




Â  Â  from tqdm import tqdm




Â  Â  TQDM_AVAILABLE = True




except ImportError:




Â  Â  TQDM_AVAILABLE = False




Â  Â  print("âš ï¸ Â å»ºè­°å®‰è£ tqdm ä»¥ç²å¾—æ›´å¥½çš„é€²åº¦é¡¯ç¤ºï¼špip install tqdm")




# åŠ å…¥ dotenv æ”¯æ´




try:




Â  Â  from dotenv import load_dotenv




Â  Â  load_dotenv()




except ImportError:




Â  Â  print("âŒ è«‹å…ˆå®‰è£ python-dotenvï¼špip install python-dotenv")




Â  Â  sys.exit(1)




try:




Â  Â  from reportlab.lib.pagesizes import A4




Â  Â  from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle




Â  Â  from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle




Â  Â  from reportlab.lib import colors




Â  Â  REPORTLAB_AVAILABLE = True




except ImportError:




Â  Â  REPORTLAB_AVAILABLE = False




class Config:




Â  Â  def __init__(self):




Â  Â  Â  Â  self.mysql_host = os.getenv('MYSQL_HOST', 'localhost')




Â  Â  Â  Â  self.mysql_port = int(os.getenv('MYSQL_PORT', '3306'))




Â  Â  Â  Â  self.mysql_user = os.getenv('MYSQL_USER', 'root')




Â  Â  Â  Â  self.mysql_password = os.getenv('MYSQL_PASSWORD', '')




Â  Â  Â  Â  self.mysql_db = os.getenv('MYSQL_DB', 'auditdb')




Â  Â  Â  Â  # æ–°å¢è³‡æ–™åº«é€£ç·šæ± å’Œæ•ˆèƒ½ç›¸é—œè¨­å®š




Â  Â  Â  Â  self.db_pool_size = int(os.getenv('DB_POOL_SIZE', '5'))




Â  Â  Â  Â  self.db_max_overflow = int(os.getenv('DB_MAX_OVERFLOW', '10'))




Â  Â  Â  Â  self.db_pool_timeout = int(os.getenv('DB_POOL_TIMEOUT', '30'))




Â  Â  Â  Â  self.db_query_timeout = int(os.getenv('DB_QUERY_TIMEOUT', '300'))




Â  Â  Â  Â  self.max_fetch_size = int(os.getenv('MAX_FETCH_SIZE', '100000'))




Â  Â  Â  Â  self.batch_fetch_size = int(os.getenv('BATCH_FETCH_SIZE', '10000'))




Â  Â  Â  Â  self.query_retry_count = int(os.getenv('QUERY_RETRY_COUNT', '3'))




Â  Â  Â  Â  self.retry_delay = float(os.getenv('RETRY_DELAY', '1.0'))




Â  Â  Â  Â  # è³‡æºé™åˆ¶å’Œç¯€æµæ§åˆ¶




Â  Â  Â  Â  self.max_memory_usage_mb = int(os.getenv('MAX_MEMORY_USAGE_MB', '1024'))




Â  Â  Â  Â  self.query_throttle_delay = float(os.getenv('QUERY_THROTTLE_DELAY', '0.1'))




Â  Â  Â  Â  self.enable_resource_monitoring = os.getenv('ENABLE_RESOURCE_MONITORING', 'true').lower() == 'true'




Â  Â  Â  Â  self.max_concurrent_queries = int(os.getenv('MAX_CONCURRENT_QUERIES', '3'))




Â  Â  Â  Â  self.log_base_path = os.getenv('LOG_BASE_PATH', '/var/log/mysql/audit')




Â  Â  Â  Â  self.log_file_prefix = os.getenv('LOG_FILE_PREFIX', 'server_audit.log')




Â  Â  Â  Â  self.output_dir = os.getenv('OUTPUT_DIR', '/tmp/mysql_reports')




Â  Â  Â  Â  self.failed_login_threshold = int(os.getenv('FAILED_LOGIN_THRESHOLD', '5'))




Â  Â  Â  Â  self.allowed_ips = [ip.strip() for ip in os.getenv('ALLOWED_IPS', '').split(',') if ip.strip()]




Â  Â  Â  Â  self.after_hours_users = [u.strip() for u in os.getenv('AFTER_HOURS_USERS', '').split(',') if u.strip()]




Â  Â  Â  Â  self.work_hour_start = int(os.getenv('WORK_HOUR_START', '9'))




Â  Â  Â  Â  self.work_hour_end = int(os.getenv('WORK_HOUR_END', '18'))




Â  Â  Â  Â  self.privileged_users = [u.strip() for u in os.getenv('PRIVILEGED_USERS', '').split(',') if u.strip()]




Â  Â  Â  Â  self.report_title = os.getenv('REPORT_TITLE', 'MySQL Audit Log Security Analysis Report')




Â  Â  Â  Â  self.company_name = os.getenv('COMPANY_NAME', 'Your Company')




Â  Â  Â  Â  self.generate_pdf = os.getenv('GENERATE_PDF', 'true').lower() == 'true'




Â  Â  Â  Â  self.generate_csv = os.getenv('GENERATE_CSV', 'true').lower() == 'true'




Â  Â  Â  Â  self.privileged_keywords = [k.strip() for k in os.getenv(




Â  Â  Â  Â  Â  Â  'PRIVILEGED_KEYWORDS',




Â  Â  Â  Â  Â  Â  'CREATE USER,DROP USER,GRANT,REVOKE,CREATE DATABASE,DROP DATABASE,CREATE TABLE,DROP TABLE,ALTER USER,SET PASSWORD'




Â  Â  Â  Â  ).split(',') if k.strip()]




Â  Â  Â  Â  self.send_email = os.getenv('SEND_EMAIL', 'false').lower() == 'true'




Â  Â  Â  Â  self.smtp_server = os.getenv('SMTP_SERVER', '')




Â  Â  Â  Â  self.smtp_port = int(os.getenv('SMTP_PORT', '25'))




Â  Â  Â  Â  self.mail_from = os.getenv('MAIL_FROM', '')




Â  Â  Â  Â  self.mail_to = [m.strip() for m in os.getenv('MAIL_TO', '').split(',') if m.strip()]




Â  Â  Â  Â  # æ–°å¢ LOAD DATA INFILE ç›¸é—œè¨­å®š




Â  Â  Â  Â  self.use_load_data_infile = os.getenv('USE_LOAD_DATA_INFILE', 'true').lower() == 'true'




Â  Â  Â  Â  self.temp_dir = os.getenv('TEMP_DIR', '/tmp')




Â  Â  def get_log_file_path(self, date_str: str = None) -> str:




Â  Â  Â  Â  return os.path.join(self.log_base_path, self.log_file_prefix if not date_str else f"{self.log_file_prefix}-{date_str}")




Â  Â  def as_dict(self):




Â  Â  Â  Â  return {




Â  Â  Â  Â  Â  Â  "MYSQL_HOST": self.mysql_host,




Â  Â  Â  Â  Â  Â  "MYSQL_PORT": self.mysql_port,




Â  Â  Â  Â  Â  Â  "MYSQL_USER": self.mysql_user,




Â  Â  Â  Â  Â  Â  "MYSQL_PASSWORD": self.mysql_password,




Â  Â  Â  Â  Â  Â  "MYSQL_DB": self.mysql_db,




Â  Â  Â  Â  Â  Â  "LOG_BASE_PATH": self.log_base_path,




Â  Â  Â  Â  Â  Â  "LOG_FILE_PREFIX": self.log_file_prefix,




Â  Â  Â  Â  Â  Â  "OUTPUT_DIR": self.output_dir,




Â  Â  Â  Â  Â  Â  "FAILED_LOGIN_THRESHOLD": self.failed_login_threshold,




Â  Â  Â  Â  Â  Â  "ALLOWED_IPS": self.allowed_ips,




Â  Â  Â  Â  Â  Â  "AFTER_HOURS_USERS": self.after_hours_users,




Â  Â  Â  Â  Â  Â  "WORK_HOUR_START": self.work_hour_start,




Â  Â  Â  Â  Â  Â  "WORK_HOUR_END": self.work_hour_end,




Â  Â  Â  Â  Â  Â  "PRIVILEGED_USERS": self.privileged_users,




Â  Â  Â  Â  Â  Â  "REPORT_TITLE": self.report_title,




Â  Â  Â  Â  Â  Â  "COMPANY_NAME": self.company_name,




Â  Â  Â  Â  Â  Â  "GENERATE_PDF": self.generate_pdf,




Â  Â  Â  Â  Â  Â  "GENERATE_CSV": self.generate_csv,




Â  Â  Â  Â  Â  Â  "PRIVILEGED_KEYWORDS": self.privileged_keywords,




Â  Â  Â  Â  Â  Â  "SEND_EMAIL": self.send_email,




Â  Â  Â  Â  Â  Â  "SMTP_SERVER": self.smtp_server,




Â  Â  Â  Â  Â  Â  "SMTP_PORT": self.smtp_port,




Â  Â  Â  Â  Â  Â  "MAIL_FROM": self.mail_from,




Â  Â  Â  Â  Â  Â  "MAIL_TO": self.mail_to,




Â  Â  Â  Â  Â  Â  "USE_LOAD_DATA_INFILE": self.use_load_data_infile,




Â  Â  Â  Â  Â  Â  "TEMP_DIR": self.temp_dir,




Â  Â  Â  Â  }




def init_resource_monitoring(config: Config):




Â  Â  """åˆå§‹åŒ–è³‡æºç›£æ§"""




Â  Â  global _resource_monitor, _query_semaphore




Â  Â  if config.enable_resource_monitoring:




Â  Â  Â  Â  _resource_monitor = ResourceMonitor(config.max_memory_usage_mb)




Â  Â  Â  Â  print(f"âœ… è³‡æºç›£æ§åˆå§‹åŒ–å®Œæˆ (è¨˜æ†¶é«”é™åˆ¶: {config.max_memory_usage_mb}MB)")




Â  Â  _query_semaphore = threading.Semaphore(config.max_concurrent_queries)




Â  Â  print(f"âœ… æŸ¥è©¢ä¸¦è¡Œæ§åˆ¶åˆå§‹åŒ–å®Œæˆ (æœ€å¤§ä¸¦è¡Œ: {config.max_concurrent_queries})")




# å…¨åŸŸé€£ç·šæ± è®Šæ•¸




_connection_pool = None




def init_connection_pool(config: Config):




Â  Â  """




Â  Â  åˆå§‹åŒ–è³‡æ–™åº«é€£ç·šæ± 




Â  Â  """




Â  Â  global _connection_pool




Â  Â  if _connection_pool is None:




Â  Â  Â  Â  _connection_pool = SimpleConnectionPool(




Â  Â  Â  Â  Â  Â  host=config.mysql_host,




Â  Â  Â  Â  Â  Â  port=config.mysql_port,




Â  Â  Â  Â  Â  Â  user=config.mysql_user,




Â  Â  Â  Â  Â  Â  password=config.mysql_password,




Â  Â  Â  Â  Â  Â  database=config.mysql_db,




Â  Â  Â  Â  Â  Â  charset='utf8mb4',




Â  Â  Â  Â  Â  Â  max_connections=config.db_pool_size + config.db_max_overflow,




Â  Â  Â  Â  Â  Â  # é€£ç·šè¶…æ™‚è¨­å®š




Â  Â  Â  Â  Â  Â  connect_timeout=30,




Â  Â  Â  Â  Â  Â  read_timeout=config.db_query_timeout,




Â  Â  Â  Â  Â  Â  write_timeout=config.db_query_timeout,




Â  Â  Â  Â  Â  Â  # å„ªåŒ–è¨­å®š




Â  Â  Â  Â  Â  Â  cursorclass=pymysql.cursors.SSCursor, Â # ä½¿ç”¨ä¼ºæœå™¨ç«¯æ¸¸æ¨™




Â  Â  Â  Â  )




Â  Â  Â  Â  print(f"âœ… è³‡æ–™åº«é€£ç·šæ± åˆå§‹åŒ–å®Œæˆ (æ± å¤§å°: {config.db_pool_size}, æœ€å¤§é€£ç·š: {config.db_pool_size + config.db_max_overflow})")




@contextmanager




def get_db_conn(config: Config):




Â  Â  """




Â  Â  å–å¾—è³‡æ–™åº«é€£ç·š (ä½¿ç”¨é€£ç·šæ± å’Œè‡ªå‹•é‡‹æ”¾)




Â  Â  """




Â  Â  if _connection_pool is None:




Â  Â  Â  Â  init_connection_pool(config)




Â  Â  conn = None




Â  Â  try:




Â  Â  Â  Â  conn = _connection_pool.get_connection()




Â  Â  Â  Â  yield conn




Â  Â  except Exception as e:




Â  Â  Â  Â  if conn:




Â  Â  Â  Â  Â  Â  conn.rollback()




Â  Â  Â  Â  raise e




Â  Â  finally:




Â  Â  Â  Â  if conn:




Â  Â  Â  Â  Â  Â  _connection_pool.release_connection(conn)




def get_legacy_db_conn(config: Config):




Â  Â  """




Â  Â  å–å¾—å‚³çµ±è³‡æ–™åº«é€£ç·š (ç”¨æ–¼ä¸æ”¯æ´ context manager çš„èˆŠç¨‹å¼ç¢¼)




Â  Â  """




Â  Â  return pymysql.connect(




Â  Â  Â  Â  host=config.mysql_host,




Â  Â  Â  Â  port=config.mysql_port,




Â  Â  Â  Â  user=config.mysql_user,




Â  Â  Â  Â  password=config.mysql_password,




Â  Â  Â  Â  database=config.mysql_db,




Â  Â  Â  Â  charset='utf8mb4',




Â  Â  Â  Â  autocommit=True,




Â  Â  Â  Â  local_infile=True,




Â  Â  Â  Â  connect_timeout=30,




Â  Â  Â  Â  read_timeout=config.db_query_timeout,




Â  Â  Â  Â  write_timeout=config.db_query_timeout




Â  Â  )




def execute_query_with_retry(conn, query, params=None, config=None, fetch_mode='fetchall'):




Â  Â  """




Â  Â  åŸ·è¡ŒæŸ¥è©¢ä¸¦åŠ å…¥é‡è©¦æ©Ÿåˆ¶ã€è¨˜æ†¶é«”ç®¡ç†å’Œè³‡æºç›£æ§




Â  Â  Args:




Â  Â  Â  Â  conn: è³‡æ–™åº«é€£ç·š




Â  Â  Â  Â  query: SQLæŸ¥è©¢èªå¥




Â  Â  Â  Â  params: æŸ¥è©¢åƒæ•¸




Â  Â  Â  Â  config: è¨­å®šç‰©ä»¶




Â  Â  Â  Â  fetch_mode: 'fetchall', 'fetchone', 'fetchmany', 'iterator'




Â  Â  Returns:




Â  Â  Â  Â  æŸ¥è©¢çµæœ




Â  Â  """




Â  Â  global _resource_monitor, _query_semaphore




Â  Â  retry_count = config.query_retry_count if config else 3




Â  Â  retry_delay = config.retry_delay if config else 1.0




Â  Â  max_fetch_size = config.max_fetch_size if config else 100000




Â  Â  batch_size = config.batch_fetch_size if config else 10000




Â  Â  throttle_delay = config.query_throttle_delay if config else 0.1




Â  Â  # è³‡æºç›£æ§å’Œä¸¦è¡Œæ§åˆ¶




Â  Â  if _resource_monitor:




Â  Â  Â  Â  _resource_monitor.force_gc_if_needed()




Â  Â  # ä½¿ç”¨ä¿¡è™Ÿé‡æ§åˆ¶ä¸¦è¡ŒæŸ¥è©¢æ•¸é‡




Â  Â  if _query_semaphore:




Â  Â  Â  Â  _query_semaphore.acquire()




Â  Â  try:




Â  Â  Â  Â  for attempt in range(retry_count):




Â  Â  Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  Â  Â  # æŸ¥è©¢ç¯€æµ




Â  Â  Â  Â  Â  Â  Â  Â  if throttle_delay > 0:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(throttle_delay)




Â  Â  Â  Â  Â  Â  Â  Â  with conn.cursor() as cur:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # è¨­å®šæŸ¥è©¢è¶…æ™‚å’Œå„ªåŒ–åƒæ•¸




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if config:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cur.execute(f"SET SESSION innodb_lock_wait_timeout = {min(config.db_query_timeout, 50)}")




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cur.execute("SET SESSION query_cache_type = ON")




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cur.execute(f"SET SESSION max_execution_time = {config.db_query_timeout * 1000}")




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cur.execute(query, params)




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if fetch_mode == 'fetchone':




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = cur.fetchone()




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif fetch_mode == 'fetchmany':




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = cur.fetchmany(batch_size)




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif fetch_mode == 'iterator':




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = cur Â # å›å‚³æ¸¸æ¨™è¿­ä»£å™¨ï¼Œç¯€çœè¨˜æ†¶é«”




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif fetch_mode == 'fetchall_safe':




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # å®‰å…¨çš„ fetchallï¼Œé™åˆ¶çµæœæ•¸é‡ä¸¦ç›£æ§è¨˜æ†¶é«”




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results = []




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  count = 0




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  while True:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨é‡




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if _resource_monitor and not _resource_monitor.check_memory_limit():




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Â è¨˜æ†¶é«”ä¸è¶³ï¼ŒæŸ¥è©¢çµæœæˆªæ–·æ–¼ {count:,} ç­†")




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  batch = cur.fetchmany(batch_size)




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not batch:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results.extend(batch)




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  count += len(batch)




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if count >= max_fetch_size:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Â æŸ¥è©¢çµæœè¶…éé™åˆ¶ ({max_fetch_size:,} ç­†)ï¼Œå·²æˆªæ–·")




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = results




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: Â # fetchall (default)




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = cur.fetchall()




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return result




Â  Â  Â  Â  Â  Â  except (pymysql.Error, Exception) as e:




Â  Â  Â  Â  Â  Â  Â  Â  if attempt < retry_count - 1:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Â æŸ¥è©¢å¤±æ•—ï¼Œç¬¬ {attempt + 1}/{retry_count} æ¬¡é‡è©¦... éŒ¯èª¤: {str(e)}")




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(retry_delay * (attempt + 1)) Â # æŒ‡æ•¸é€€é¿




Â  Â  Â  Â  Â  Â  Â  Â  else:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"âŒ æŸ¥è©¢æœ€çµ‚å¤±æ•—: {str(e)}")




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise e




Â  Â  finally:




Â  Â  Â  Â  # é‡‹æ”¾ä¿¡è™Ÿé‡




Â  Â  Â  Â  if _query_semaphore:




Â  Â  Â  Â  Â  Â  _query_semaphore.release()




def get_file_line_count(file_path):




Â  Â  """å¿«é€Ÿè¨ˆç®—æª”æ¡ˆè¡Œæ•¸ï¼Œç”¨æ–¼é€²åº¦æ¢"""




Â  Â  try:




Â  Â  Â  Â  if file_path.endswith('.gz'):




Â  Â  Â  Â  Â  Â  with gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') as f:




Â  Â  Â  Â  Â  Â  Â  Â  return sum(1 for _ in f)




Â  Â  Â  Â  else:




Â  Â  Â  Â  Â  Â  with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:




Â  Â  Â  Â  Â  Â  Â  Â  return sum(1 for _ in f)




Â  Â  except:




Â  Â  Â  Â  return 0




def import_log_file_to_db_optimized(file_path, log_date, conn, config):




Â  Â  """




Â  Â  ä½¿ç”¨ LOAD DATA INFILE å„ªåŒ–ç‰ˆæœ¬çš„æ—¥èªŒåŒ¯å…¥å‡½æ•¸ï¼ˆåŠ å…¥é€²åº¦æ¢ï¼‰




Â  Â  """




Â  Â  print(f"ğŸš€ é–‹å§‹å„ªåŒ–åŒ¯å…¥ {file_path}...")




Â  Â  # è¨ˆç®—æª”æ¡ˆè¡Œæ•¸ç”¨æ–¼é€²åº¦æ¢




Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  print("ğŸ“Š æ­£åœ¨è¨ˆç®—æª”æ¡ˆå¤§å°...")




Â  Â  Â  Â  total_lines = get_file_line_count(file_path)




Â  Â  Â  Â  if total_lines == 0:




Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Â æª”æ¡ˆ {file_path} æ²’æœ‰è³‡æ–™")




Â  Â  Â  Â  Â  Â  return




Â  Â  # å»ºç«‹è‡¨æ™‚ CSV æª”æ¡ˆ




Â  Â  temp_csv = tempfile.NamedTemporaryFile(




Â  Â  Â  Â  mode='w',




Â  Â  Â  Â  suffix='.csv',




Â  Â  Â  Â  dir=config.temp_dir,




Â  Â  Â  Â  delete=False,




Â  Â  Â  Â  encoding='utf-8'




Â  Â  )




Â  Â  try:




Â  Â  Â  Â  # è®€å–åŸå§‹æ—¥èªŒæª”æ¡ˆä¸¦è½‰æ›ç‚ºæ¨™æº– CSV æ ¼å¼




Â  Â  Â  Â  with (gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore')




Â  Â  Â  Â  Â  Â  Â  if file_path.endswith('.gz')




Â  Â  Â  Â  Â  Â  Â  else open(file_path, 'r', encoding='utf-8', errors='ignore')) as f:




Â  Â  Â  Â  Â  Â  reader = csv.reader(f)




Â  Â  Â  Â  Â  Â  writer = csv.writer(temp_csv, quoting=csv.QUOTE_ALL)




Â  Â  Â  Â  Â  Â  # å»ºç«‹é€²åº¦æ¢




Â  Â  Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  Â  Â  progress_bar = tqdm(




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  total=total_lines,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  desc="ğŸ“ è™•ç†æ—¥èªŒè³‡æ–™",




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  unit="è¡Œ",




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  unit_scale=True,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  colour='green'




Â  Â  Â  Â  Â  Â  Â  Â  )




Â  Â  Â  Â  Â  Â  row_count = 0




Â  Â  Â  Â  Â  Â  start_time = datetime.now()




Â  Â  Â  Â  Â  Â  for row in reader:




Â  Â  Â  Â  Â  Â  Â  Â  # ç¢ºä¿æ¬„ä½æ•¸é‡ä¸€è‡´




Â  Â  Â  Â  Â  Â  Â  Â  row += [''] * (10 - len(row))




Â  Â  Â  Â  Â  Â  Â  Â  timestamp, server_host, username, host, connection_id, query_id, operation, database, query, retcode = row[:10]




Â  Â  Â  Â  Â  Â  Â  Â  # è™•ç† retcode




Â  Â  Â  Â  Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  retcode = int(retcode) if retcode else 0




Â  Â  Â  Â  Â  Â  Â  Â  except:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  retcode = 0




Â  Â  Â  Â  Â  Â  Â  Â  # å¯«å…¥è‡¨æ™‚ CSV æª”æ¡ˆ




Â  Â  Â  Â  Â  Â  Â  Â  writer.writerow([




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  log_date, timestamp, server_host, username, host,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  connection_id, query_id, operation, database, query, retcode




Â  Â  Â  Â  Â  Â  Â  Â  ])




Â  Â  Â  Â  Â  Â  Â  Â  row_count += 1




Â  Â  Â  Â  Â  Â  Â  Â  # æ›´æ–°é€²åº¦æ¢




Â  Â  Â  Â  Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  progress_bar.update(1)




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if row_count % 10000 == 0: Â # æ¯ 10000 ç­†æ›´æ–°ä¸€æ¬¡æè¿°




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  progress_bar.set_postfix({




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'å·²è™•ç†': f'{row_count:,}',




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'é€Ÿåº¦': f'{row_count/(datetime.now()-start_time).total_seconds():.0f}/ç§’'




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })




Â  Â  Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  Â  Â  progress_bar.close()




Â  Â  Â  Â  temp_csv.close()




Â  Â  Â  Â  if row_count == 0:




Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Â æª”æ¡ˆ {file_path} æ²’æœ‰è³‡æ–™")




Â  Â  Â  Â  Â  Â  return




Â  Â  Â  Â  processing_time = (datetime.now() - start_time).total_seconds()




Â  Â  Â  Â  print(f"âœ… è³‡æ–™è™•ç†å®Œæˆ: {row_count:,} ç­†ï¼Œè€—æ™‚ {processing_time:.2f} ç§’")




Â  Â  Â  Â  # ä½¿ç”¨ LOAD DATA LOCAL INFILE æ‰¹é‡è¼‰å…¥




Â  Â  Â  Â  print("ğŸ’¾ æ­£åœ¨è¼‰å…¥è³‡æ–™åˆ°è³‡æ–™åº«...")




Â  Â  Â  Â  db_start_time = datetime.now()




Â  Â  Â  Â  with conn.cursor() as cur:




Â  Â  Â  Â  Â  Â  # å…ˆæª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨è©²æ—¥æœŸçš„è³‡æ–™ï¼Œå¦‚æœæœ‰å‰‡å…ˆåˆªé™¤




Â  Â  Â  Â  Â  Â  cur.execute("DELETE FROM audit_log WHERE log_date = %s", (log_date,))




Â  Â  Â  Â  Â  Â  deleted_count = cur.rowcount




Â  Â  Â  Â  Â  Â  if deleted_count > 0:




Â  Â  Â  Â  Â  Â  Â  Â  print(f"ğŸ—‘ï¸ Â åˆªé™¤èˆŠè³‡æ–™ {deleted_count:,} ç­†")




Â  Â  Â  Â  Â  Â  # åŸ·è¡Œ LOAD DATA LOCAL INFILE




Â  Â  Â  Â  Â  Â  load_sql = f"""




Â  Â  Â  Â  Â  Â  LOAD DATA LOCAL INFILE '{temp_csv.name}'




Â  Â  Â  Â  Â  Â  INTO TABLE audit_log




Â  Â  Â  Â  Â  Â  FIELDS TERMINATED BY ','




Â  Â  Â  Â  Â  Â  OPTIONALLY ENCLOSED BY '"'




Â  Â  Â  Â  Â  Â  LINES TERMINATED BY '\\n'




Â  Â  Â  Â  Â  Â  (log_date, timestamp, server_host, username, host, connection_id, query_id, operation, dbname, query, retcode)




Â  Â  Â  Â  Â  Â  """




Â  Â  Â  Â  Â  Â  cur.execute(load_sql)




Â  Â  Â  Â  Â  Â  # ç²å–å¯¦éš›è¼‰å…¥çš„è¡Œæ•¸




Â  Â  Â  Â  Â  Â  cur.execute("SELECT ROW_COUNT()")




Â  Â  Â  Â  Â  Â  loaded_rows = cur.fetchone()[0]




Â  Â  Â  Â  Â  Â  db_duration = (datetime.now() - db_start_time).total_seconds()




Â  Â  Â  Â  Â  Â  total_duration = (datetime.now() - start_time).total_seconds()




Â  Â  Â  Â  Â  Â  print(f"âœ… å„ªåŒ–åŒ¯å…¥ {os.path.basename(file_path)} å®Œæˆ")




Â  Â  Â  Â  Â  Â  print(f" Â  ğŸ“Š è™•ç†è³‡æ–™: {row_count:,} ç­†")




Â  Â  Â  Â  Â  Â  print(f" Â  ğŸ“¥ è¼‰å…¥è³‡æ–™: {loaded_rows:,} ç­†")




Â  Â  Â  Â  Â  Â  print(f" Â  â±ï¸ Â è™•ç†è€—æ™‚: {processing_time:.2f} ç§’")




Â  Â  Â  Â  Â  Â  print(f" Â  ğŸ’¾ è¼‰å…¥è€—æ™‚: {db_duration:.2f} ç§’")




Â  Â  Â  Â  Â  Â  print(f" Â  ğŸ•’ ç¸½è€—æ™‚: {total_duration:.2f} ç§’")




Â  Â  Â  Â  Â  Â  print(f" Â  ğŸš€ ç¸½é€Ÿåº¦: {loaded_rows/total_duration:.0f} ç­†/ç§’")




Â  Â  except Exception as e:




Â  Â  Â  Â  print(f"âŒ å„ªåŒ–åŒ¯å…¥å¤±æ•—: {e}")




Â  Â  Â  Â  # å¦‚æœ LOAD DATA INFILE å¤±æ•—ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•




Â  Â  Â  Â  print("ğŸ”„ å›é€€åˆ°åŸå§‹åŒ¯å…¥æ–¹æ³•...")




Â  Â  Â  Â  import_log_file_to_db_fallback(file_path, log_date, conn)




Â  Â  finally:




Â  Â  Â  Â  # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ




Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  os.unlink(temp_csv.name)




Â  Â  Â  Â  except:




Â  Â  Â  Â  Â  Â  pass




def import_log_file_to_db_fallback(file_path, log_date, conn):




Â  Â  """




Â  Â  åŸå§‹çš„é€ç­†æ’å…¥æ–¹æ³•ï¼ˆä½œç‚ºå‚™ç”¨æ–¹æ¡ˆï¼ŒåŠ å…¥é€²åº¦æ¢ï¼‰




Â  Â  """




Â  Â  print(f"ğŸ“ ä½¿ç”¨åŸå§‹æ–¹æ³•åŒ¯å…¥ {file_path}...")




Â  Â  # è¨ˆç®—æª”æ¡ˆè¡Œæ•¸ç”¨æ–¼é€²åº¦æ¢




Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  print("ğŸ“Š æ­£åœ¨è¨ˆç®—æª”æ¡ˆå¤§å°...")




Â  Â  Â  Â  total_lines = get_file_line_count(file_path)




Â  Â  Â  Â  if total_lines == 0:




Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Â æª”æ¡ˆ {file_path} æ²’æœ‰è³‡æ–™")




Â  Â  Â  Â  Â  Â  return




Â  Â  with (gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore')




Â  Â  Â  Â  Â  if file_path.endswith('.gz')




Â  Â  Â  Â  Â  else open(file_path, 'r', encoding='utf-8', errors='ignore')) as f:




Â  Â  Â  Â  reader = csv.reader(f)




Â  Â  Â  Â  data = []




Â  Â  Â  Â  # å»ºç«‹é€²åº¦æ¢




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar = tqdm(




Â  Â  Â  Â  Â  Â  Â  Â  total=total_lines,




Â  Â  Â  Â  Â  Â  Â  Â  desc="ğŸ“ è®€å–æ—¥èªŒè³‡æ–™",




Â  Â  Â  Â  Â  Â  Â  Â  unit="è¡Œ",




Â  Â  Â  Â  Â  Â  Â  Â  unit_scale=True,




Â  Â  Â  Â  Â  Â  Â  Â  colour='blue'




Â  Â  Â  Â  Â  Â  )




Â  Â  Â  Â  start_time = datetime.now()




Â  Â  Â  Â  for row in reader:




Â  Â  Â  Â  Â  Â  row += [''] * (10 - len(row))




Â  Â  Â  Â  Â  Â  timestamp, server_host, username, host, connection_id, query_id, operation, database, query, retcode = row[:10]




Â  Â  Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  Â  Â  retcode = int(retcode) if retcode else 0




Â  Â  Â  Â  Â  Â  except:




Â  Â  Â  Â  Â  Â  Â  Â  retcode = 0




Â  Â  Â  Â  Â  Â  data.append((log_date, timestamp, server_host, username, host, connection_id, query_id, operation, database, query, retcode))




Â  Â  Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  Â  Â  progress_bar.update(1)




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.close()




Â  Â  Â  Â  if not data:




Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Â æª”æ¡ˆ {file_path} æ²’æœ‰è³‡æ–™")




Â  Â  Â  Â  Â  Â  return




Â  Â  Â  Â  processing_time = (datetime.now() - start_time).total_seconds()




Â  Â  Â  Â  print(f"âœ… è³‡æ–™è®€å–å®Œæˆ: {len(data):,} ç­†ï¼Œè€—æ™‚ {processing_time:.2f} ç§’")




Â  Â  Â  Â  # è³‡æ–™åº«æ“ä½œ




Â  Â  Â  Â  print("ğŸ’¾ æ­£åœ¨å¯«å…¥è³‡æ–™åº«...")




Â  Â  Â  Â  db_start_time = datetime.now()




Â  Â  Â  Â  with conn.cursor() as cur:




Â  Â  Â  Â  Â  Â  # å…ˆåˆªé™¤è©²æ—¥æœŸçš„èˆŠè³‡æ–™




Â  Â  Â  Â  Â  Â  cur.execute("DELETE FROM audit_log WHERE log_date = %s", (log_date,))




Â  Â  Â  Â  Â  Â  deleted_count = cur.rowcount




Â  Â  Â  Â  Â  Â  if deleted_count > 0:




Â  Â  Â  Â  Â  Â  Â  Â  print(f"ğŸ—‘ï¸ Â åˆªé™¤èˆŠè³‡æ–™ {deleted_count:,} ç­†")




Â  Â  Â  Â  Â  Â  # æ‰¹é‡æ’å…¥ï¼ˆä½¿ç”¨ executemanyï¼‰




Â  Â  Â  Â  Â  Â  sql = """INSERT INTO audit_log




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (log_date, timestamp, server_host, username, host, connection_id, query_id, operation, dbname, query, retcode)




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)"""




Â  Â  Â  Â  Â  Â  # å¦‚æœè³‡æ–™é‡å¾ˆå¤§ï¼Œå¯ä»¥åˆ†æ‰¹è™•ç†




Â  Â  Â  Â  Â  Â  batch_size = 10000




Â  Â  Â  Â  Â  Â  if len(data) > batch_size:




Â  Â  Â  Â  Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  batch_progress = tqdm(




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  total=len(data),




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  desc="ğŸ’¾ æ‰¹é‡å¯«å…¥",




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  unit="ç­†",




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  unit_scale=True,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  colour='cyan'




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )




Â  Â  Â  Â  Â  Â  Â  Â  for i in range(0, len(data), batch_size):




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  batch = data[i:i+batch_size]




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cur.executemany(sql, batch)




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  batch_progress.update(len(batch))




Â  Â  Â  Â  Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  batch_progress.close()




Â  Â  Â  Â  Â  Â  else:




Â  Â  Â  Â  Â  Â  Â  Â  cur.executemany(sql, data)




Â  Â  Â  Â  Â  Â  db_duration = (datetime.now() - db_start_time).total_seconds()




Â  Â  Â  Â  Â  Â  total_duration = (datetime.now() - start_time).total_seconds()




Â  Â  Â  Â  Â  Â  print(f"âœ… åŸå§‹æ–¹æ³•åŒ¯å…¥ {os.path.basename(file_path)} å®Œæˆ")




Â  Â  Â  Â  Â  Â  print(f" Â  ğŸ“Š è¼‰å…¥è³‡æ–™: {len(data):,} ç­†")




Â  Â  Â  Â  Â  Â  print(f" Â  â±ï¸ Â è®€å–è€—æ™‚: {processing_time:.2f} ç§’")




Â  Â  Â  Â  Â  Â  print(f" Â  ğŸ’¾ å¯«å…¥è€—æ™‚: {db_duration:.2f} ç§’")




Â  Â  Â  Â  Â  Â  print(f" Â  ğŸ•’ ç¸½è€—æ™‚: {total_duration:.2f} ç§’")




Â  Â  Â  Â  Â  Â  print(f" Â  ğŸŒ ç¸½é€Ÿåº¦: {len(data)/total_duration:.0f} ç­†/ç§’")




def import_log_file_to_db(file_path, log_date, conn, config=None):




Â  Â  """




Â  Â  ä¸»è¦çš„æ—¥èªŒåŒ¯å…¥å‡½æ•¸ - æ ¹æ“šè¨­å®šé¸æ“‡å„ªåŒ–æˆ–åŸå§‹æ–¹æ³•




Â  Â  """




Â  Â  if config and config.use_load_data_infile:




Â  Â  Â  Â  import_log_file_to_db_optimized(file_path, log_date, conn, config)




Â  Â  else:




Â  Â  Â  Â  import_log_file_to_db_fallback(file_path, log_date, conn)




def get_log_files_for_month(config, month_str):




Â  Â  log_files = []




Â  Â  year, month = map(int, month_str.split('-'))




Â  Â  days_in_month = calendar.monthrange(year, month)[1]




Â  Â  for day in range(1, days_in_month + 1):




Â  Â  Â  Â  date_str = f"{year:04d}-{month:02d}-{day:02d}"




Â  Â  Â  Â  log_path = config.get_log_file_path(date_str)




Â  Â  Â  Â  if os.path.exists(log_path):




Â  Â  Â  Â  Â  Â  log_files.append((log_path, date_str))




Â  Â  Â  Â  elif os.path.exists(log_path + '.gz'):




Â  Â  Â  Â  Â  Â  log_files.append((log_path + '.gz', date_str))




Â  Â  return log_files




def get_log_file_for_date(config, date_str):




Â  Â  log_path = config.get_log_file_path(date_str)




Â  Â  if os.path.exists(log_path):




Â  Â  Â  Â  return (log_path, date_str)




Â  Â  elif os.path.exists(log_path + '.gz'):




Â  Â  Â  Â  return (log_path + '.gz', date_str)




Â  Â  else:




Â  Â  Â  Â  return None




# ========== åˆ†ææŸ¥è©¢ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰ ==========




def run_analysis_with_progress(analysis_functions, conn, date_filter, date_filter_value, config):




Â  Â  """




Â  Â  åŸ·è¡Œæ‰€æœ‰åˆ†æåŠŸèƒ½ä¸¦é¡¯ç¤ºé€²åº¦




Â  Â  """




Â  Â  results = {}




Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  progress_bar = tqdm(




Â  Â  Â  Â  Â  Â  total=len(analysis_functions),




Â  Â  Â  Â  Â  Â  desc="ğŸ” åŸ·è¡Œå®‰å…¨åˆ†æ",




Â  Â  Â  Â  Â  Â  unit="é …ç›®",




Â  Â  Â  Â  Â  Â  colour='magenta'




Â  Â  Â  Â  )




Â  Â  for name, func, args in analysis_functions:




Â  Â  Â  Â  start_time = datetime.now()




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.set_description(f"ğŸ” åˆ†æ: {name}")




Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  if args:




Â  Â  Â  Â  Â  Â  Â  Â  results[name] = func(conn, date_filter, date_filter_value, *args, config=config)




Â  Â  Â  Â  Â  Â  else:




Â  Â  Â  Â  Â  Â  Â  Â  results[name] = func(conn, date_filter, date_filter_value, config=config)




Â  Â  Â  Â  Â  Â  duration = (datetime.now() - start_time).total_seconds()




Â  Â  Â  Â  Â  Â  if not TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  Â  Â  print(f"âœ… {name} å®Œæˆ ({duration:.2f}ç§’)")




Â  Â  Â  Â  except Exception as e:




Â  Â  Â  Â  Â  Â  print(f"âŒ {name} å¤±æ•—: {e}")




Â  Â  Â  Â  Â  Â  results[name] = None




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.update(1)




Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  progress_bar.close()




Â  Â  return results




def analyze_summary(conn, date_filter, date_filter_value, config=None):




Â  Â  """åŸºæœ¬çµ±è¨ˆåˆ†æ - ä½¿ç”¨å„ªåŒ–æŸ¥è©¢"""




Â  Â  if isinstance(date_filter_value, tuple):




Â  Â  Â  Â  params = date_filter_value




Â  Â  else:




Â  Â  Â  Â  params = (date_filter_value,)




Â  Â  query = f"SELECT COUNT(*), COUNT(DISTINCT username), COUNT(DISTINCT host) FROM audit_log WHERE {date_filter}"




Â  Â  result = execute_query_with_retry(conn, query, params, config, 'fetchone')




Â  Â  if result:




Â  Â  Â  Â  total_events, unique_users, unique_hosts = result




Â  Â  Â  Â  return {




Â  Â  Â  Â  Â  Â  'total_events': total_events,




Â  Â  Â  Â  Â  Â  'unique_users': unique_users,




Â  Â  Â  Â  Â  Â  'unique_hosts': unique_hosts




Â  Â  Â  Â  }




Â  Â  return {'total_events': 0, 'unique_users': 0, 'unique_hosts': 0}




def analyze_failed_logins(conn, date_filter, date_filter_value, threshold=5, config=None):




Â  Â  """å¤±æ•—ç™»å…¥åˆ†æ - ä½¿ç”¨å„ªåŒ–æŸ¥è©¢å’Œçµæœé™åˆ¶"""




Â  Â  if isinstance(date_filter_value, tuple):




Â  Â  Â  Â  params = date_filter_value + (threshold,)




Â  Â  Â  Â  params2 = date_filter_value




Â  Â  else:




Â  Â  Â  Â  params = (date_filter_value, threshold)




Â  Â  Â  Â  params2 = (date_filter_value,)




Â  Â  # æŸ¥è©¢å¯ç–‘ä½¿ç”¨è€… (åŠ å…¥ LIMIT é™åˆ¶)




Â  Â  query1 = f"""SELECT username, COUNT(*) as fail_count




Â  Â  Â  Â  Â  Â  Â  Â  FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE operation='CONNECT' AND retcode!=0 AND {date_filter}




Â  Â  Â  Â  Â  Â  Â  Â  GROUP BY username




Â  Â  Â  Â  Â  Â  Â  Â  HAVING fail_count >= %s




Â  Â  Â  Â  Â  Â  Â  Â  ORDER BY fail_count DESC




Â  Â  Â  Â  Â  Â  Â  Â  LIMIT 1000"""




Â  Â  by_user = execute_query_with_retry(conn, query1, params, config, 'fetchall_safe')




Â  Â  # æŸ¥è©¢å¯ç–‘IP (åŠ å…¥ LIMIT é™åˆ¶)




Â  Â  query2 = f"""SELECT host, COUNT(*) as fail_count




Â  Â  Â  Â  Â  Â  Â  Â  FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE operation='CONNECT' AND retcode!=0 AND {date_filter}




Â  Â  Â  Â  Â  Â  Â  Â  GROUP BY host




Â  Â  Â  Â  Â  Â  Â  Â  HAVING fail_count >= %s




Â  Â  Â  Â  Â  Â  Â  Â  ORDER BY fail_count DESC




Â  Â  Â  Â  Â  Â  Â  Â  LIMIT 1000"""




Â  Â  by_ip = execute_query_with_retry(conn, query2, params, config, 'fetchall_safe')




Â  Â  # ç¸½å¤±æ•—æ¬¡æ•¸




Â  Â  query3 = f"SELECT COUNT(*) FROM audit_log WHERE operation='CONNECT' AND retcode!=0 AND {date_filter}"




Â  Â  total_result = execute_query_with_retry(conn, query3, params2, config, 'fetchone')




Â  Â  total = total_result[0] if total_result else 0




Â  Â  return {




Â  Â  Â  Â  'total': total,




Â  Â  Â  Â  'by_user': by_user or [],




Â  Â  Â  Â  'by_ip': by_ip or []




Â  Â  }




def analyze_privileged_operations(conn, date_filter, date_filter_value, keywords, config=None):




Â  Â  """ç‰¹æ¬Šæ“ä½œåˆ†æ - ä½¿ç”¨å„ªåŒ–æŸ¥è©¢å’Œçµæœé™åˆ¶"""




Â  Â  like_clauses = " OR ".join(["UPPER(query) LIKE %s" for _ in keywords])




Â  Â  like_params = [f"%{k.upper()}%" for k in keywords]




Â  Â  if isinstance(date_filter_value, tuple):




Â  Â  Â  Â  params = like_params + list(date_filter_value)




Â  Â  else:




Â  Â  Â  Â  params = like_params + [date_filter_value]




Â  Â  # æŒ‰ä½¿ç”¨è€…çµ±è¨ˆç‰¹æ¬Šæ“ä½œ (é™åˆ¶çµæœæ•¸é‡)




Â  Â  query1 = f"""SELECT username, COUNT(*) as cnt




Â  Â  Â  Â  Â  Â  Â  Â  FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE operation='QUERY' AND ({like_clauses}) AND {date_filter}




Â  Â  Â  Â  Â  Â  Â  Â  GROUP BY username




Â  Â  Â  Â  Â  Â  Â  Â  ORDER BY cnt DESC




Â  Â  Â  Â  Â  Â  Â  Â  LIMIT 500"""




Â  Â  by_user = execute_query_with_retry(conn, query1, params, config, 'fetchall_safe')




Â  Â  # ç¸½ç‰¹æ¬Šæ“ä½œæ•¸é‡




Â  Â  query2 = f"""SELECT COUNT(*) FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE operation='QUERY' AND ({like_clauses}) AND {date_filter}"""




Â  Â  total_result = execute_query_with_retry(conn, query2, params, config, 'fetchone')




Â  Â  total = total_result[0] if total_result else 0




Â  Â  # è©³ç´°ç‰¹æ¬Šæ“ä½œè¨˜éŒ„ (é™åˆ¶æ•¸é‡ä¸¦æˆªçŸ­æŸ¥è©¢å…§å®¹)




Â  Â  query3 = f"""SELECT username,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  CASE WHEN LENGTH(query) > 200 THEN CONCAT(LEFT(query, 200), '...') ELSE query END as query_short,




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  timestamp




Â  Â  Â  Â  Â  Â  Â  Â  FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE operation='QUERY' AND ({like_clauses}) AND {date_filter}




Â  Â  Â  Â  Â  Â  Â  Â  ORDER BY timestamp DESC




Â  Â  Â  Â  Â  Â  Â  Â  LIMIT 1000"""




Â  Â  details = execute_query_with_retry(conn, query3, params, config, 'fetchall_safe')




Â  Â  return {




Â  Â  Â  Â  'total': total,




Â  Â  Â  Â  'by_user': by_user or [],




Â  Â  Â  Â  'details': details or []




Â  Â  }




def analyze_operation_stats(conn, date_filter, date_filter_value, config=None):




Â  Â  """æ“ä½œé¡å‹çµ±è¨ˆåˆ†æ - ä½¿ç”¨å„ªåŒ–æŸ¥è©¢"""




Â  Â  if isinstance(date_filter_value, tuple):




Â  Â  Â  Â  params = date_filter_value




Â  Â  else:




Â  Â  Â  Â  params = (date_filter_value,)




Â  Â  query = f"""SELECT operation, COUNT(*) as cnt




Â  Â  Â  Â  Â  Â  Â  Â  FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE {date_filter}




Â  Â  Â  Â  Â  Â  Â  Â  GROUP BY operation




Â  Â  Â  Â  Â  Â  Â  Â  ORDER BY cnt DESC




Â  Â  Â  Â  Â  Â  Â  Â  LIMIT 100"""




Â  Â  result = execute_query_with_retry(conn, query, params, config, 'fetchall_safe')




Â  Â  return result or []




def analyze_error_codes(conn, date_filter, date_filter_value, config=None):




Â  Â  """éŒ¯èª¤ä»£ç¢¼åˆ†æ - ä½¿ç”¨å„ªåŒ–æŸ¥è©¢"""




Â  Â  if isinstance(date_filter_value, tuple):




Â  Â  Â  Â  params = date_filter_value




Â  Â  else:




Â  Â  Â  Â  params = (date_filter_value,)




Â  Â  # éŒ¯èª¤ä»£ç¢¼çµ±è¨ˆ (é™åˆ¶çµæœæ•¸é‡)




Â  Â  query1 = f"""SELECT retcode, COUNT(*) as cnt




Â  Â  Â  Â  Â  Â  Â  Â  FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE retcode!=0 AND operation!='CHANGEUSER' AND {date_filter}




Â  Â  Â  Â  Â  Â  Â  Â  GROUP BY retcode




Â  Â  Â  Â  Â  Â  Â  Â  ORDER BY cnt DESC




Â  Â  Â  Â  Â  Â  Â  Â  LIMIT 100"""




Â  Â  error_codes = execute_query_with_retry(conn, query1, params, config, 'fetchall_safe')




Â  Â  # ç¸½éŒ¯èª¤æ•¸é‡




Â  Â  query2 = f"SELECT COUNT(*) FROM audit_log WHERE retcode!=0 AND operation!='CHANGEUSER' AND {date_filter}"




Â  Â  total_result = execute_query_with_retry(conn, query2, params, config, 'fetchone')




Â  Â  total_errors = total_result[0] if total_result else 0




Â  Â  return {




Â  Â  Â  Â  'total_errors': total_errors,




Â  Â  Â  Â  'error_codes': error_codes or []




Â  Â  }




def analyze_after_hours_access(conn, date_filter, date_filter_value, users, wh_start, wh_end):




Â  Â  if not users:




Â  Â  Â  Â  return {'total': 0, 'details': []}




Â  Â  user_list = ','.join(["'%s'" % u for u in users])




Â  Â  with conn.cursor() as cur:




Â  Â  Â  Â  if isinstance(date_filter_value, tuple):




Â  Â  Â  Â  Â  Â  params = date_filter_value




Â  Â  Â  Â  else:




Â  Â  Â  Â  Â  Â  params = (date_filter_value,)




Â  Â  Â  Â  cur.execute(




Â  Â  Â  Â  Â  Â  f"""SELECT username, host, operation, timestamp




Â  Â  Â  Â  Â  Â  Â  Â  FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE username IN ({user_list}) AND {date_filter}




Â  Â  Â  Â  Â  Â  """,




Â  Â  Â  Â  Â  Â  params




Â  Â  Â  Â  )




Â  Â  Â  Â  rows = cur.fetchall()




Â  Â  Â  Â  after_hours = []




Â  Â  Â  Â  for username, host, operation, ts in rows:




Â  Â  Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  Â  Â  dt = datetime.strptime(ts, "%Y%m%d %H:%M:%S")




Â  Â  Â  Â  Â  Â  except:




Â  Â  Â  Â  Â  Â  Â  Â  continue




Â  Â  Â  Â  Â  Â  if dt.weekday() >= 5 or not (wh_start <= dt.hour < wh_end):




Â  Â  Â  Â  Â  Â  Â  Â  after_hours.append((username, host, operation, dt.strftime('%Y-%m-%d %H:%M:%S')))




Â  Â  Â  Â  return {'total': len(after_hours), 'details': after_hours[:50]}




def analyze_privileged_user_logins(conn, date_filter, date_filter_value, users):




Â  Â  if not users:




Â  Â  Â  Â  return {'total': 0, 'by_user': [], 'details': []}




Â  Â  user_list = ','.join(["'%s'" % u for u in users])




Â  Â  with conn.cursor() as cur:




Â  Â  Â  Â  if isinstance(date_filter_value, tuple):




Â  Â  Â  Â  Â  Â  params = date_filter_value




Â  Â  Â  Â  else:




Â  Â  Â  Â  Â  Â  params = (date_filter_value,)




Â  Â  Â  Â  cur.execute(




Â  Â  Â  Â  Â  Â  f"""SELECT username, COUNT(*) as cnt




Â  Â  Â  Â  Â  Â  Â  Â  FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE operation='CONNECT' AND username IN ({user_list}) AND {date_filter}




Â  Â  Â  Â  Â  Â  Â  Â  GROUP BY username




Â  Â  Â  Â  Â  Â  Â  Â  ORDER BY cnt DESC




Â  Â  Â  Â  Â  Â  """,




Â  Â  Â  Â  Â  Â  params




Â  Â  Â  Â  )




Â  Â  Â  Â  by_user = cur.fetchall()




Â  Â  Â  Â  cur.execute(




Â  Â  Â  Â  Â  Â  f"""SELECT username, host, timestamp




Â  Â  Â  Â  Â  Â  Â  Â  FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE operation='CONNECT' AND username IN ({user_list}) AND {date_filter}




Â  Â  Â  Â  Â  Â  Â  Â  ORDER BY timestamp DESC




Â  Â  Â  Â  Â  Â  """,




Â  Â  Â  Â  Â  Â  params




Â  Â  Â  Â  )




Â  Â  Â  Â  details = cur.fetchall()




Â  Â  Â  Â  cur.execute(




Â  Â  Â  Â  Â  Â  f"""SELECT COUNT(*) FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE operation='CONNECT' AND username IN ({user_list}) AND {date_filter}




Â  Â  Â  Â  Â  Â  """,




Â  Â  Â  Â  Â  Â  params




Â  Â  Â  Â  )




Â  Â  Â  Â  total = cur.fetchone()[0]




Â  Â  Â  Â  return {'total': total, 'by_user': by_user, 'details': details}




def analyze_non_whitelisted_ips(conn, date_filter, date_filter_value, allowed_ips):




Â  Â  if not allowed_ips:




Â  Â  Â  Â  return {'total': 0, 'by_ip': [], 'details': []}




Â  Â  ip_list = ','.join(["'%s'" % ip for ip in allowed_ips])




Â  Â  with conn.cursor() as cur:




Â  Â  Â  Â  if isinstance(date_filter_value, tuple):




Â  Â  Â  Â  Â  Â  params = date_filter_value




Â  Â  Â  Â  else:




Â  Â  Â  Â  Â  Â  params = (date_filter_value,)




# çºŒå‰é¢çš„ç¨‹å¼ç¢¼...




Â  Â  Â  Â  cur.execute(




Â  Â  Â  Â  Â  Â  f"""SELECT host, COUNT(*) as cnt




Â  Â  Â  Â  Â  Â  Â  Â  FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE host NOT IN ({ip_list}) AND operation!='CHANGEUSER' AND {date_filter}




Â  Â  Â  Â  Â  Â  Â  Â  GROUP BY host




Â  Â  Â  Â  Â  Â  Â  Â  ORDER BY cnt DESC




Â  Â  Â  Â  Â  Â  """,




Â  Â  Â  Â  Â  Â  params




Â  Â  Â  Â  )




Â  Â  Â  Â  by_ip = cur.fetchall()




Â  Â  Â  Â  cur.execute(




Â  Â  Â  Â  Â  Â  f"""SELECT username, host, operation, timestamp




Â  Â  Â  Â  Â  Â  Â  Â  FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE host NOT IN ({ip_list}) AND operation!='CHANGEUSER' AND {date_filter}




Â  Â  Â  Â  Â  Â  Â  Â  ORDER BY timestamp DESC




Â  Â  Â  Â  Â  Â  """,




Â  Â  Â  Â  Â  Â  params




Â  Â  Â  Â  )




Â  Â  Â  Â  details = cur.fetchall()




Â  Â  Â  Â  cur.execute(




Â  Â  Â  Â  Â  Â  f"""SELECT COUNT(*) FROM audit_log




Â  Â  Â  Â  Â  Â  Â  Â  WHERE host NOT IN ({ip_list}) AND operation!='CHANGEUSER' AND {date_filter}




Â  Â  Â  Â  Â  Â  """,




Â  Â  Â  Â  Â  Â  params




Â  Â  Â  Â  )




Â  Â  Â  Â  total = cur.fetchone()[0]




Â  Â  Â  Â  return {'total': total, 'by_ip': by_ip, 'details': details}




# ========== å ±è¡¨ç”¢ç”Ÿï¼ˆCSVï¼‰ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰ ==========




def generate_csv_report(output_dir, report_title, summary, failed, priv_ops, priv_user_logins, op_stats, err, after_hours, non_whitelisted, period_label):




Â  Â  """




Â  Â  ç”¢ç”Ÿ CSV å ±è¡¨ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰




Â  Â  """




Â  Â  os.makedirs(output_dir, exist_ok=True)




Â  Â  csv_file = os.path.join(output_dir, f'mysql_audit_analysis_{period_label}.csv')




Â  Â  # è¨ˆç®—ç¸½å…±è¦å¯«å…¥çš„å€å¡Šæ•¸é‡




Â  Â  total_sections = 9




Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  progress_bar = tqdm(




Â  Â  Â  Â  Â  Â  total=total_sections,




Â  Â  Â  Â  Â  Â  desc="ğŸ“Š ç”¢ç”Ÿ CSV å ±è¡¨",




Â  Â  Â  Â  Â  Â  unit="å€å¡Š",




Â  Â  Â  Â  Â  Â  colour='yellow'




Â  Â  Â  Â  )




Â  Â  w = lambda row: writer.writerow(row)




Â  Â  with open(csv_file, 'w', newline='', encoding='utf-8') as f:




Â  Â  Â  Â  writer = csv.writer(f)




Â  Â  Â  Â  # æ¨™é¡Œ




Â  Â  Â  Â  w([f'{report_title} - {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}'])




Â  Â  Â  Â  w([])




Â  Â  Â  Â  # åŸºæœ¬çµ±è¨ˆ




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.set_description("ğŸ“Š å¯«å…¥åŸºæœ¬çµ±è¨ˆ")




Â  Â  Â  Â  w(['=== Basic Statistics ==='])




Â  Â  Â  Â  w(['Total Events', summary['total_events']])




Â  Â  Â  Â  w(['Unique Users', summary['unique_users']])




Â  Â  Â  Â  w(['Unique Hosts', summary['unique_hosts']])




Â  Â  Â  Â  w([])




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.update(1)




Â  Â  Â  Â  # å¤±æ•—ç™»å…¥åˆ†æ




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.set_description("ğŸ“Š å¯«å…¥å¤±æ•—ç™»å…¥åˆ†æ")




Â  Â  Â  Â  w(['=== Failed Login Analysis ==='])




Â  Â  Â  Â  w(['Total Failed Logins', failed['total']])




Â  Â  Â  Â  w([])




Â  Â  Â  Â  if failed['by_user']:




Â  Â  Â  Â  Â  Â  w(['Suspicious Users (Above Threshold)'])




Â  Â  Â  Â  Â  Â  w(['Username', 'Failed Count'])




Â  Â  Â  Â  Â  Â  for row in failed['by_user']:




Â  Â  Â  Â  Â  Â  Â  Â  w(list(row))




Â  Â  Â  Â  Â  Â  w([])




Â  Â  Â  Â  if failed['by_ip']:




Â  Â  Â  Â  Â  Â  w(['Suspicious IPs (Above Threshold)'])




Â  Â  Â  Â  Â  Â  w(['IP Address', 'Failed Count'])




Â  Â  Â  Â  Â  Â  for row in failed['by_ip']:




Â  Â  Â  Â  Â  Â  Â  Â  w(list(row))




Â  Â  Â  Â  Â  Â  w([])




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.update(1)




Â  Â  Â  Â  # ç‰¹æ¬Šæ“ä½œåˆ†æ




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.set_description("ğŸ“Š å¯«å…¥ç‰¹æ¬Šæ“ä½œåˆ†æ")




Â  Â  Â  Â  w(['=== Privileged Operations Analysis ==='])




Â  Â  Â  Â  w(['Total Privileged Operations', priv_ops['total']])




Â  Â  Â  Â  w([])




Â  Â  Â  Â  if priv_ops['by_user']:




Â  Â  Â  Â  Â  Â  w(['By User Statistics'])




Â  Â  Â  Â  Â  Â  w(['Username', 'Operation Count'])




Â  Â  Â  Â  Â  Â  for row in priv_ops['by_user']:




Â  Â  Â  Â  Â  Â  Â  Â  w(list(row))




Â  Â  Â  Â  Â  Â  w([])




Â  Â  Â  Â  if priv_ops.get('details'):




Â  Â  Â  Â  Â  Â  w(['Detailed Privileged Operations (SQL)'])




Â  Â  Â  Â  Â  Â  w(['Username', 'SQL', 'Timestamp'])




Â  Â  Â  Â  Â  Â  for row in priv_ops['details']:




Â  Â  Â  Â  Â  Â  Â  Â  w(list(row))




Â  Â  Â  Â  Â  Â  w([])




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.update(1)




Â  Â  Â  Â  # ç‰¹æ¬Šå¸³è™Ÿç™»å…¥åˆ†æ




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.set_description("ğŸ“Š å¯«å…¥ç‰¹æ¬Šå¸³è™Ÿç™»å…¥åˆ†æ")




Â  Â  Â  Â  w(['=== Privileged Account Login Analysis ==='])




Â  Â  Â  Â  w(['Total Privileged Account Logins', priv_user_logins['total']])




Â  Â  Â  Â  if priv_user_logins['by_user']:




Â  Â  Â  Â  Â  Â  w(['Username', 'Login Count'])




Â  Â  Â  Â  Â  Â  for row in priv_user_logins['by_user']:




Â  Â  Â  Â  Â  Â  Â  Â  w(list(row))




Â  Â  Â  Â  w([])




Â  Â  Â  Â  if priv_user_logins['details']:




Â  Â  Â  Â  Â  Â  w(['Detailed Privileged Account Login Records'])




Â  Â  Â  Â  Â  Â  w(['Username', 'Host', 'Timestamp'])




Â  Â  Â  Â  Â  Â  for row in priv_user_logins['details']:




Â  Â  Â  Â  Â  Â  Â  Â  w(list(row))




Â  Â  Â  Â  Â  Â  w([])




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.update(1)




Â  Â  Â  Â  # æ“ä½œé¡å‹çµ±è¨ˆ




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.set_description("ğŸ“Š å¯«å…¥æ“ä½œé¡å‹çµ±è¨ˆ")




Â  Â  Â  Â  w(['=== Operation Type Statistics ==='])




Â  Â  Â  Â  w(['Operation Type', 'Count'])




Â  Â  Â  Â  for row in op_stats:




Â  Â  Â  Â  Â  Â  w(list(row))




Â  Â  Â  Â  w([])




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.update(1)




Â  Â  Â  Â  # éŒ¯èª¤åˆ†æ




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.set_description("ğŸ“Š å¯«å…¥éŒ¯èª¤åˆ†æ")




Â  Â  Â  Â  w(['=== Error Analysis ==='])




Â  Â  Â  Â  w(['Total Errors', err['total_errors']])




Â  Â  Â  Â  w([])




Â  Â  Â  Â  if err['error_codes']:




Â  Â  Â  Â  Â  Â  w(['Error Code Statistics'])




Â  Â  Â  Â  Â  Â  w(['Error Code', 'Count'])




Â  Â  Â  Â  Â  Â  for row in err['error_codes']:




Â  Â  Â  Â  Â  Â  Â  Â  w(list(row))




Â  Â  Â  Â  w([])




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.update(1)




Â  Â  Â  Â  # éä¸Šç­æ™‚é–“å­˜å–åˆ†æ




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.set_description("ğŸ“Š å¯«å…¥éä¸Šç­æ™‚é–“å­˜å–åˆ†æ")




Â  Â  Â  Â  w(['=== After-hours Access (Specify account) ==='])




Â  Â  Â  Â  w(['Total After-hours Access', after_hours['total']])




Â  Â  Â  Â  if after_hours['details']:




Â  Â  Â  Â  Â  Â  w(['Username', 'Host', 'Operation', 'Time'])




Â  Â  Â  Â  Â  Â  for row in after_hours['details']:




Â  Â  Â  Â  Â  Â  Â  Â  w(list(row))




Â  Â  Â  Â  w([])




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.update(1)




Â  Â  Â  Â  # éç™½åå–® IP å­˜å–åˆ†æ




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.set_description("ğŸ“Š å¯«å…¥éç™½åå–® IP åˆ†æ")




Â  Â  Â  Â  w(['=== Non-whitelisted IP Access Analysis ==='])




Â  Â  Â  Â  w(['Total Events from Non-whitelisted IPs', non_whitelisted['total']])




Â  Â  Â  Â  if non_whitelisted['by_ip']:




Â  Â  Â  Â  Â  Â  w(['Non-whitelisted IPs'])




Â  Â  Â  Â  Â  Â  w(['IP Address', 'Event Count'])




Â  Â  Â  Â  Â  Â  for row in non_whitelisted['by_ip']:




Â  Â  Â  Â  Â  Â  Â  Â  w(list(row))




Â  Â  Â  Â  Â  Â  w([])




Â  Â  Â  Â  if non_whitelisted['details']:




Â  Â  Â  Â  Â  Â  w(['Details (Username, Host, Operation, Time)'])




Â  Â  Â  Â  Â  Â  for row in non_whitelisted['details']:




Â  Â  Â  Â  Â  Â  Â  Â  w(list(row))




Â  Â  Â  Â  w([])




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.update(1)




Â  Â  Â  Â  # å®Œæˆ




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  progress_bar.set_description("ğŸ“Š CSV å ±è¡¨å®Œæˆ")




Â  Â  Â  Â  Â  Â  progress_bar.update(1)




Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  progress_bar.close()




Â  Â  print(f"âœ… CSV report generated: {csv_file}")




Â  Â  return csv_file




# ========== å ±è¡¨ç”¢ç”Ÿï¼ˆPDFï¼‰ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰ ==========




def generate_pdf_report(output_dir, report_title, summary, failed, priv_ops, priv_user_logins, op_stats, err, after_hours, non_whitelisted, period_label):




Â  Â  """




Â  Â  ç”¢ç”Ÿ PDF å ±è¡¨ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰




Â  Â  """




Â  Â  if not REPORTLAB_AVAILABLE:




Â  Â  Â  Â  print("âŒ ReportLab not installed, PDF report cannot be generated.")




Â  Â  Â  Â  return None




Â  Â  os.makedirs(output_dir, exist_ok=True)




Â  Â  pdf_file = os.path.join(output_dir, f'mysql_audit_analysis_{period_label}.pdf')




Â  Â  print("ğŸ“„ æ­£åœ¨ç”¢ç”Ÿ PDF å ±è¡¨...")




Â  Â  start_time = datetime.now()




Â  Â  doc = SimpleDocTemplate(pdf_file, pagesize=A4)




Â  Â  styles = getSampleStyleSheet()




Â  Â  story = []




Â  Â  # æ¨™é¡Œ




Â  Â  story.append(Paragraph(f"<b>{report_title}</b>", styles['Title']))




Â  Â  story.append(Spacer(1, 12))




Â  Â  story.append(Paragraph(f"Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", styles['Normal']))




Â  Â  story.append(Spacer(1, 8))




Â  Â  def add_table(title, data, colnames):




Â  Â  Â  Â  story.append(Spacer(1, 8))




Â  Â  Â  Â  story.append(Paragraph(f"<b>{title}</b>", styles['Heading3']))




Â  Â  Â  Â  if not data:




Â  Â  Â  Â  Â  Â  story.append(Paragraph("(No data)", styles['Normal']))




Â  Â  Â  Â  else:




Â  Â  Â  Â  Â  Â  table = Table([colnames] + list(data), hAlign='LEFT')




Â  Â  Â  Â  Â  Â  table.setStyle(TableStyle([




Â  Â  Â  Â  Â  Â  Â  Â  ('BACKGROUND', (0,0), (-1,0), colors.lightgrey),




Â  Â  Â  Â  Â  Â  Â  Â  ('GRID', (0,0), (-1,-1), 0.5, colors.grey),




Â  Â  Â  Â  Â  Â  Â  Â  ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),




Â  Â  Â  Â  Â  Â  ]))




Â  Â  Â  Â  Â  Â  story.append(table)




Â  Â  # åŸºæœ¬çµ±è¨ˆ




Â  Â  story.append(Paragraph("<b>=== Basic Statistics ===</b>", styles['Heading2']))




Â  Â  story.append(Paragraph(f"Total Events: {summary['total_events']}<br />"




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Unique Users: {summary['unique_users']}<br />"




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Unique Hosts: {summary['unique_hosts']}", styles['Normal']))




Â  Â  # å„ç¨®åˆ†æè¡¨æ ¼




Â  Â  add_table("Suspicious Users (Failed Logins)", failed['by_user'], ['Username', 'Failed Count'])




Â  Â  add_table("Suspicious IPs (Failed Logins)", failed['by_ip'], ['IP Address', 'Failed Count'])




Â  Â  add_table("Privileged Operations by User", priv_ops['by_user'], ['Username', 'Operation Count'])




Â  Â  add_table("Detailed Privileged Operations (SQL)", priv_ops.get('details', []), ['Username', 'SQL', 'Timestamp'])




Â  Â  add_table("Privileged Account Login Statistics", priv_user_logins['by_user'], ['Username', 'Login Count'])




Â  Â  add_table("Privileged Account Login Details", priv_user_logins['details'], ['Username', 'IP Address', 'Timestamp'])




Â  Â  add_table("Operation Type Statistics", op_stats, ['Operation', 'Count'])




Â  Â  add_table("Error Code Statistics", err['error_codes'], ['Error Code', 'Count'])




Â  Â  add_table("After-hours Access (Specify account)", after_hours['details'], ['Username', 'IP Address', 'Operation', 'Timestamp'])




Â  Â  add_table("Non-whitelisted IPs", non_whitelisted['by_ip'], ['IP Address', 'Event Count'])




Â  Â  add_table("Non-whitelisted IP Access Details", non_whitelisted['details'], ['Username', 'IP Address', 'Operation', 'Timestamp'])




Â  Â  doc.build(story)




Â  Â  duration = (datetime.now() - start_time).total_seconds()




Â  Â  print(f"âœ… PDF report generated: {pdf_file} (è€—æ™‚ {duration:.2f} ç§’)")




Â  Â  return pdf_file




# ========== éƒµä»¶å¯„é€ ==========




def send_email_with_attachment(config: Config, subject, body, attachment_path):




Â  Â  if not (config.smtp_server and config.mail_from and config.mail_to):




Â  Â  Â  Â  print("âŒ SMTP æˆ–æ”¶ä»¶äººè¨­å®šä¸å®Œæ•´ï¼Œç„¡æ³•å¯„ä¿¡ã€‚")




Â  Â  Â  Â  return




Â  Â  print("ğŸ“§ æ­£åœ¨å¯„é€éƒµä»¶...")




Â  Â  start_time = datetime.now()




Â  Â  msg = EmailMessage()




Â  Â  msg['Subject'] = subject




Â  Â  msg['From'] = config.mail_from




Â  Â  msg['To'] = ', '.join(config.mail_to)




Â  Â  msg.set_content(body)




Â  Â  with open(attachment_path, 'rb') as f:




Â  Â  Â  Â  file_data = f.read()




Â  Â  Â  Â  file_name = os.path.basename(attachment_path)




Â  Â  maintype, subtype = ('application', 'pdf') if file_name.endswith('.pdf') else ('application', 'octet-stream')




Â  Â  msg.add_attachment(file_data, maintype=maintype, subtype=subtype, filename=file_name)




Â  Â  try:




Â  Â  Â  Â  with smtplib.SMTP(config.smtp_server, config.smtp_port) as server:




Â  Â  Â  Â  Â  Â  server.send_message(msg)




Â  Â  Â  Â  duration = (datetime.now() - start_time).total_seconds()




Â  Â  Â  Â  print(f"ğŸ“§ éƒµä»¶å·²å¯„å‡ºè‡³: {', '.join(config.mail_to)} (è€—æ™‚ {duration:.2f} ç§’)")




Â  Â  except Exception as e:




Â  Â  Â  Â  print(f"âŒ éƒµä»¶å¯„é€å¤±æ•—: {e}")




# ========== ä¸»ç¨‹å¼ï¼ˆåŠ å…¥å®Œæ•´çš„é€²åº¦è¿½è¹¤ï¼‰ ==========




def main():




Â  Â  parser = argparse.ArgumentParser(description='MySQL Audit Log Security Analyzer (MySQL backend) - Enhanced with Progress Tracking')




Â  Â  parser.add_argument('--import-date', help='Import logs for specific date (format: YYYY-MM-DD)')




Â  Â  parser.add_argument('--import-month', help='Import logs for specific month (format: YYYY-MM)')




Â  Â  parser.add_argument('--analyze-date', help='Analyze logs for specific date (format: YYYY-MM-DD)')




Â  Â  parser.add_argument('--analyze-month', help='Analyze logs for specific month (format: YYYY-MM)')




Â  Â  parser.add_argument('--output-dir', help='Output directory')




Â  Â  parser.add_argument('--csv-only', action='store_true', help='Generate CSV report only')




Â  Â  parser.add_argument('--show-env', action='store_true', help='Show all env/config parameters and exit')




Â  Â  parser.add_argument('--disable-load-data', action='store_true', help='Disable LOAD DATA INFILE optimization')




Â  Â  parser.add_argument('--disable-progress', action='store_true', help='Disable progress bars (useful for automation)')




Â  Â  args = parser.parse_args()




Â  Â  config = Config()




Â  Â  # å¦‚æœæŒ‡å®šäº† --disable-progressï¼Œå‰‡å…¨åŸŸåœç”¨ tqdm




Â  Â  if args.disable_progress:




Â  Â  Â  Â  global TQDM_AVAILABLE




Â  Â  Â  Â  TQDM_AVAILABLE = False




Â  Â  Â  Â  print("âš ï¸ Â å·²åœç”¨é€²åº¦æ¢é¡¯ç¤º")




Â  Â  # å¦‚æœæŒ‡å®šäº† --disable-load-dataï¼Œå‰‡é—œé–‰å„ªåŒ–




Â  Â  if args.disable_load_data:




Â  Â  Â  Â  config.use_load_data_infile = False




Â  Â  Â  Â  print("âš ï¸ Â å·²åœç”¨ LOAD DATA INFILE å„ªåŒ–")




Â  Â  # é¡¯ç¤ºæ‰€æœ‰ env è¨­å®š




Â  Â  if args.show_env:




Â  Â  Â  Â  print("ğŸ” ç›®å‰æŠ“åˆ°çš„ .env/ç’°å¢ƒè®Šæ•¸åƒæ•¸å¦‚ä¸‹ï¼š\n")




Â  Â  Â  Â  for k, v in config.as_dict().items():




Â  Â  Â  Â  Â  Â  print(f"{k}: {v}")




Â  Â  Â  Â  return




Â  Â  # ç¨‹å¼é–‹å§‹åŸ·è¡Œæ™‚é–“




Â  Â  program_start_time = datetime.now()




Â  Â  print(f"ğŸš€ ç¨‹å¼é–‹å§‹åŸ·è¡Œ: {program_start_time.strftime('%Y-%m-%d %H:%M:%S')}")




Â  Â  # åˆå§‹åŒ–è³‡æºç›£æ§




Â  Â  try:




Â  Â  Â  Â  init_resource_monitoring(config)




Â  Â  except Exception as e:




Â  Â  Â  Â  print(f"âš ï¸ Â è³‡æºç›£æ§åˆå§‹åŒ–å¤±æ•—: {e}")




Â  Â  # åˆå§‹åŒ–é€£ç·šæ± 




Â  Â  try:




Â  Â  Â  Â  init_connection_pool(config)




Â  Â  except Exception as e:




Â  Â  Â  Â  print(f"âŒ è³‡æ–™åº«é€£ç·šæ± åˆå§‹åŒ–å¤±æ•—: {e}")




Â  Â  Â  Â  print("ğŸ”„ å˜—è©¦ä½¿ç”¨å‚³çµ±é€£ç·šæ–¹å¼...")




Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  test_conn = get_legacy_db_conn(config)




Â  Â  Â  Â  Â  Â  test_conn.close()




Â  Â  Â  Â  Â  Â  print("âœ… è³‡æ–™åº«é€£ç·šæ¸¬è©¦æˆåŠŸ")




Â  Â  Â  Â  except Exception as e2:




Â  Â  Â  Â  Â  Â  print(f"âŒ è³‡æ–™åº«å®Œå…¨ç„¡æ³•é€£ç·š: {e2}")




Â  Â  Â  Â  Â  Â  return




Â  Â  # åŒ¯å…¥æ—¥èªŒ




Â  Â  if args.import_date:




Â  Â  Â  Â  print(f"\nğŸ“… é–‹å§‹åŒ¯å…¥å–®æ—¥æ—¥èªŒ: {args.import_date}")




Â  Â  Â  Â  log = get_log_file_for_date(config, args.import_date)




Â  Â  Â  Â  if log:




Â  Â  Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  Â  Â  conn = get_legacy_db_conn(config) Â # åŒ¯å…¥ä½¿ç”¨å‚³çµ±é€£ç·š




Â  Â  Â  Â  Â  Â  Â  Â  import_log_file_to_db(log[0], log[1], conn, config)




Â  Â  Â  Â  Â  Â  Â  Â  conn.close()




Â  Â  Â  Â  Â  Â  except Exception as e:




Â  Â  Â  Â  Â  Â  Â  Â  print(f"âŒ åŒ¯å…¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")




Â  Â  Â  Â  Â  Â  Â  Â  if 'conn' in locals():




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  conn.close()




Â  Â  Â  Â  else:




Â  Â  Â  Â  Â  Â  print(f"âŒ No log file found for {args.import_date}")




Â  Â  Â  Â  total_duration = (datetime.now() - program_start_time).total_seconds()




Â  Â  Â  Â  print(f"\nğŸ‰ å–®æ—¥åŒ¯å…¥å®Œæˆï¼ç¸½è€—æ™‚: {total_duration:.2f} ç§’")




Â  Â  Â  Â  return




Â  Â  elif args.import_month:




Â  Â  Â  Â  print(f"\nğŸ“… é–‹å§‹åŒ¯å…¥æœˆä»½æ—¥èªŒ: {args.import_month}")




Â  Â  Â  Â  logs = get_log_files_for_month(config, args.import_month)




Â  Â  Â  Â  if not logs:




Â  Â  Â  Â  Â  Â  print(f"âŒ No log files found for {args.import_month}")




Â  Â  Â  Â  Â  Â  return




Â  Â  Â  Â  total_files = len(logs)




Â  Â  Â  Â  print(f"ğŸ“ æ‰¾åˆ° {total_files} å€‹æ—¥èªŒæª”æ¡ˆ")




Â  Â  Â  Â  # æœˆä»½åŒ¯å…¥é€²åº¦æ¢




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  month_progress = tqdm(




Â  Â  Â  Â  Â  Â  Â  Â  total=total_files,




Â  Â  Â  Â  Â  Â  Â  Â  desc="ğŸ“‚ æœˆä»½åŒ¯å…¥é€²åº¦",




Â  Â  Â  Â  Â  Â  Â  Â  unit="æª”æ¡ˆ",




Â  Â  Â  Â  Â  Â  Â  Â  colour='green'




Â  Â  Â  Â  Â  Â  )




Â  Â  Â  Â  import_stats = {




Â  Â  Â  Â  Â  Â  'total_files': 0,




Â  Â  Â  Â  Â  Â  'success_files': 0,




Â  Â  Â  Â  Â  Â  'failed_files': 0,




Â  Â  Â  Â  Â  Â  'total_records': 0




Â  Â  Â  Â  }




Â  Â  Â  Â  conn = None




Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  conn = get_legacy_db_conn(config) Â # æœˆä»½åŒ¯å…¥ä½¿ç”¨å‚³çµ±é€£ç·š




Â  Â  Â  Â  Â  Â  for i, (log_path, log_date) in enumerate(logs, 1):




Â  Â  Â  Â  Â  Â  Â  Â  if not TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"\nğŸ“ è™•ç†æª”æ¡ˆ {i}/{total_files}: {os.path.basename(log_path)}")




Â  Â  Â  Â  Â  Â  Â  Â  else:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  month_progress.set_description(f"ğŸ“ è™•ç†: {os.path.basename(log_path)}")




Â  Â  Â  Â  Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  import_log_file_to_db(log_path, log_date, conn, config)




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  import_stats['success_files'] += 1




Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"âŒ æª”æ¡ˆ {log_path} åŒ¯å…¥å¤±æ•—: {e}")




Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  import_stats['failed_files'] += 1




Â  Â  Â  Â  except Exception as e:




Â  Â  Â  Â  Â  Â  print(f"âŒ æœˆä»½åŒ¯å…¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")




Â  Â  Â  Â  finally:




Â  Â  Â  Â  Â  Â  if conn:




Â  Â  Â  Â  Â  Â  Â  Â  conn.close()




Â  Â  Â  Â  # çµ±è¨ˆè™•ç†




Â  Â  Â  Â  import_stats['total_files'] = len(logs)




Â  Â  Â  Â  if TQDM_AVAILABLE:




Â  Â  Â  Â  Â  Â  month_progress.close()




Â  Â  Â  Â  total_duration = (datetime.now() - program_start_time).total_seconds()




Â  Â  Â  Â  print(f"\nğŸ‰ æœˆä»½åŒ¯å…¥å®Œæˆï¼")




Â  Â  Â  Â  print(f" Â  ğŸ“ ç¸½æª”æ¡ˆæ•¸: {import_stats['total_files']}")




Â  Â  Â  Â  print(f" Â  âœ… æˆåŠŸæª”æ¡ˆ: {import_stats['success_files']}")




Â  Â  Â  Â  print(f" Â  âŒ å¤±æ•—æª”æ¡ˆ: {import_stats['failed_files']}")




Â  Â  Â  Â  print(f" Â  â±ï¸ Â ç¸½è€—æ™‚: {total_duration:.2f} ç§’")




Â  Â  Â  Â  print(f" Â  ğŸ“Š å¹³å‡æ¯æª”: {total_duration/import_stats['total_files']:.2f} ç§’")




Â  Â  Â  Â  return




Â  Â  # åˆ†æéšæ®µ




Â  Â  print(f"\nğŸ” é–‹å§‹é€²è¡Œå®‰å…¨åˆ†æ...")




Â  Â  analysis_start_time = datetime.now()




Â  # ä»¥ timestamp æ¬„ä½ç‚ºä¸»é€²è¡ŒæŸ¥è©¢




Â  Â  if args.analyze_month:




Â  Â  Â  Â  year, month = map(int, args.analyze_month.split('-'))




Â  Â  Â  Â  days_in_month = calendar.monthrange(year, month)[1]




Â  Â  Â  Â  ts_start = f"{year:04d}{month:02d}01 00:00:00"




Â  Â  Â  Â  ts_end = f"{year:04d}{month:02d}{days_in_month:02d} 23:59:59"




Â  Â  Â  Â  period_label = args.analyze_month.replace('-', '')




Â  Â  Â  Â  date_filter = "timestamp BETWEEN %s AND %s"




Â  Â  Â  Â  date_filter_value = (ts_start, ts_end)




Â  Â  Â  Â  print(f"ğŸ“Š åˆ†ææœŸé–“: {args.analyze_month} ({ts_start} åˆ° {ts_end})")




Â  Â  else:




Â  Â  Â  Â  date_str = args.analyze_date if args.analyze_date else datetime.now().strftime('%Y-%m-%d')




Â  Â  Â  Â  y, m, d = map(int, date_str.split('-'))




Â  Â  Â  Â  ts_start = f"{y:04d}{m:02d}{d:02d} 00:00:00"




Â  Â  Â  Â  ts_end = f"{y:04d}{m:02d}{d:02d} 23:59:59"




Â  Â  Â  Â  period_label = date_str.replace('-', '')




Â  Â  Â  Â  date_filter = "timestamp BETWEEN %s AND %s"




Â  Â  Â  Â  date_filter_value = (ts_start, ts_end)




Â  Â  Â  Â  print(f"ğŸ“Š åˆ†ææ—¥æœŸ: {date_str} ({ts_start} åˆ° {ts_end})")




Â  Â  # å®šç¾©æ‰€æœ‰åˆ†æåŠŸèƒ½




Â  Â  analysis_functions = [




Â  Â  Â  Â  ("åŸºæœ¬çµ±è¨ˆ", analyze_summary, None),




Â  Â  Â  Â  ("å¤±æ•—ç™»å…¥åˆ†æ", analyze_failed_logins, (config.failed_login_threshold,)),




Â  Â  Â  Â  ("ç‰¹æ¬Šæ“ä½œåˆ†æ", analyze_privileged_operations, (config.privileged_keywords,)),




Â  Â  Â  Â  ("æ“ä½œé¡å‹çµ±è¨ˆ", analyze_operation_stats, None),




Â  Â  Â  Â  ("éŒ¯èª¤ä»£ç¢¼åˆ†æ", analyze_error_codes, None),




Â  Â  Â  Â  ("éä¸Šç­æ™‚é–“å­˜å–", analyze_after_hours_access, (config.after_hours_users, config.work_hour_start, config.work_hour_end)),




Â  Â  Â  Â  ("ç‰¹æ¬Šå¸³è™Ÿç™»å…¥", analyze_privileged_user_logins, (config.privileged_users,)),




Â  Â  Â  Â  ("éç™½åå–®IPåˆ†æ", analyze_non_whitelisted_ips, (config.allowed_ips,))




Â  Â  ]




Â  Â  # åŸ·è¡Œåˆ†æ (ä½¿ç”¨é€£ç·šæ± )




Â  Â  try:




Â  Â  Â  Â  with get_db_conn(config) as conn:




Â  Â  Â  Â  Â  Â  results = run_analysis_with_progress(analysis_functions, conn, date_filter, date_filter_value, config)




Â  Â  except Exception as e:




Â  Â  Â  Â  print(f"âŒ åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼Œå˜—è©¦ä½¿ç”¨å‚³çµ±é€£ç·š: {e}")




Â  Â  Â  Â  try:




Â  Â  Â  Â  Â  Â  conn = get_legacy_db_conn(config)




Â  Â  Â  Â  Â  Â  results = run_analysis_with_progress(analysis_functions, conn, date_filter, date_filter_value, config)




Â  Â  Â  Â  Â  Â  conn.close()




Â  Â  Â  Â  except Exception as e2:




Â  Â  Â  Â  Â  Â  print(f"âŒ åˆ†æå®Œå…¨å¤±æ•—: {e2}")




Â  Â  Â  Â  Â  Â  return




Â  Â  # è§£æ§‹çµæœ




Â  Â  summary = results.get("åŸºæœ¬çµ±è¨ˆ", {})




Â  Â  failed = results.get("å¤±æ•—ç™»å…¥åˆ†æ", {})




Â  Â  priv_ops = results.get("ç‰¹æ¬Šæ“ä½œåˆ†æ", {})




Â  Â  op_stats = results.get("æ“ä½œé¡å‹çµ±è¨ˆ", [])




Â  Â  err = results.get("éŒ¯èª¤ä»£ç¢¼åˆ†æ", {})




Â  Â  after_hours = results.get("éä¸Šç­æ™‚é–“å­˜å–", {})




Â  Â  priv_user_logins = results.get("ç‰¹æ¬Šå¸³è™Ÿç™»å…¥", {})




Â  Â  non_whitelisted = results.get("éç™½åå–®IPåˆ†æ", {})




Â  Â  analysis_duration = (datetime.now() - analysis_start_time).total_seconds()




Â  Â  print(f"âœ… å®‰å…¨åˆ†æå®Œæˆï¼Œè€—æ™‚ {analysis_duration:.2f} ç§’")




Â  Â  # å ±è¡¨ç”¢ç”Ÿéšæ®µ




Â  Â  print(f"\nğŸ“Š é–‹å§‹ç”¢ç”Ÿå ±è¡¨...")




Â  Â  report_start_time = datetime.now()




Â  Â  output_dir = args.output_dir or config.output_dir




Â  Â  csv_file = None




Â  Â  pdf_file = None




Â  Â  if config.generate_csv:




Â  Â  Â  Â  csv_file = generate_csv_report(output_dir, config.report_title, summary, failed, priv_ops, priv_user_logins, op_stats, err, after_hours, non_whitelisted, period_label)




Â  Â  if config.generate_pdf and not args.csv_only:




Â  Â  Â  Â  pdf_file = generate_pdf_report(output_dir, config.report_title, summary, failed, priv_ops, priv_user_logins, op_stats, err, after_hours, non_whitelisted, period_label)




Â  Â  Â  Â  if pdf_file and config.send_email:




Â  Â  Â  Â  Â  Â  # å–å¾—åˆ†ææœŸé–“çš„æ—¥æœŸè³‡è¨Š




Â  Â  Â  Â  Â  Â  if args.analyze_month:




Â  Â  Â  Â  Â  Â  Â  Â  year, month = map(int, args.analyze_month.split('-'))




Â  Â  Â  Â  Â  Â  Â  Â  period_text = f"{year}å¹´{month:02d}æœˆ"




Â  Â  Â  Â  Â  Â  else:




Â  Â  Â  Â  Â  Â  Â  Â  date_str = args.analyze_date if args.analyze_date else datetime.now().strftime('%Y-%m-%d')




Â  Â  Â  Â  Â  Â  Â  Â  year, month, day = map(int, date_str.split('-'))




Â  Â  Â  Â  Â  Â  Â  Â  period_text = f"{year}å¹´{month:02d}æœˆ{day:02d}æ—¥"




Â  Â  Â  Â  Â  Â  send_email_with_attachment(




Â  Â  Â  Â  Â  Â  Â  Â  config,




Â  Â  Â  Â  Â  Â  Â  Â  subject=f"HamiPass MySQL ç¨½æ ¸æ—¥èªŒå®‰å…¨åˆ†æå ±å‘Š ({period_label})",




Â  Â  Â  Â  Â  Â  Â  Â  body=f"æª¢é™„ HamiPass MySQL ç¨½æ ¸æ—¥èªŒå®‰å…¨åˆ†æå ±å‘Šï¼Œåˆ†ææœŸé–“ç‚º {period_text}ã€‚",




Â  Â  Â  Â  Â  Â  Â  Â  attachment_path=pdf_file




Â  Â  Â  Â  Â  Â  )




Â  Â  Â  Â  elif pdf_file:




Â  Â  Â  Â  Â  Â  print("âœ‰ï¸ Â SEND_EMAIL=falseï¼Œæœªé€²è¡Œéƒµä»¶å¯„é€ã€‚")




Â  Â  elif args.csv_only:




Â  Â  Â  Â  print("âœ‰ï¸ Â å·²æŒ‡å®š --csv-onlyï¼Œä¸ç”¢ç”ŸPDFäº¦ä¸å¯„ä¿¡ã€‚")




Â  Â  else:




Â  Â  Â  Â  print("âœ‰ï¸ Â æœªç”¢ç”ŸPDFï¼Œä¸é€²è¡Œå¯„ä¿¡ã€‚")




Â  Â  report_duration = (datetime.now() - report_start_time).total_seconds()




Â  Â  print(f"âœ… å ±è¡¨ç”¢ç”Ÿå®Œæˆï¼Œè€—æ™‚ {report_duration:.2f} ç§’")




Â  Â  # ç¸½çµ




Â  Â  total_duration = (datetime.now() - program_start_time).total_seconds()




Â  Â  print("\n" + "="*60)




Â  Â  print("ğŸ“Š Analysis Result Summary")




Â  Â  print("="*60)




Â  Â  print(f"ğŸ“… åˆ†ææœŸé–“: {period_label}")




Â  Â  print(f"ğŸ“ˆ ç¸½äº‹ä»¶æ•¸: {summary.get('total_events', 0):,}")




Â  Â  print(f"ğŸ‘¥ ç¨ç‰¹ä½¿ç”¨è€…: {summary.get('unique_users', 0):,}")




Â  Â  print(f"ğŸ–¥ï¸ Â ç¨ç‰¹ä¸»æ©Ÿ: {summary.get('unique_hosts', 0):,}")




Â  Â  print(f"âŒ å¤±æ•—ç™»å…¥: {failed.get('total', 0):,}")




Â  Â  print(f"ğŸ” ç‰¹æ¬Šæ“ä½œ: {priv_ops.get('total', 0):,}")




Â  Â  print(f"ğŸ‘‘ ç‰¹æ¬Šå¸³è™Ÿç™»å…¥: {priv_user_logins.get('total', 0):,}")




Â  Â  print(f"âš ï¸ Â éŒ¯èª¤äº‹ä»¶: {err.get('total_errors', 0):,}")




Â  Â  print(f"ğŸš« éç™½åå–®IPäº‹ä»¶: {non_whitelisted.get('total', 0):,}")




Â  Â  if failed.get('by_user'):




Â  Â  Â  Â  print(f"âš ï¸ Â å¯ç–‘ä½¿ç”¨è€…: {len(failed['by_user'])}")




Â  Â  if failed.get('by_ip'):




Â  Â  Â  Â  print(f"âš ï¸ Â å¯ç–‘IP: {len(failed['by_ip'])}")




Â  Â  if non_whitelisted.get('by_ip'):




Â  Â  Â  Â  print(f"âš ï¸ Â éç™½åå–®IP: {len(non_whitelisted['by_ip'])}")




Â  Â  if after_hours.get('total'):




Â  Â  Â  Â  print(f"âš ï¸ Â éä¸Šç­æ™‚é–“å­˜å–: {after_hours['total']}")




Â  Â  print("\n" + "="*60)




Â  Â  print("â±ï¸ Â åŸ·è¡Œæ™‚é–“çµ±è¨ˆ")




Â  Â  print("="*60)




Â  Â  print(f"ğŸ” åˆ†æè€—æ™‚: {analysis_duration:.2f} ç§’")




Â  Â  print(f"ğŸ“Š å ±è¡¨è€—æ™‚: {report_duration:.2f} ç§’")




Â  Â  print(f"ğŸ•’ ç¸½åŸ·è¡Œæ™‚é–“: {total_duration:.2f} ç§’")




Â  Â  print(f"ğŸ ç¨‹å¼çµæŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")




if __name__ == "__main__":




Â  Â  main()




é¦–å…ˆæª¢æŸ¥ç¾æœ‰ç´¢å¼•ç‹€æ³ï¼š
USE auditdb;
SHOW INDEX FROM audit_log;




æŒ‰é‡è¦æ€§é †åºæ‰‹å‹•å»ºç«‹ç´¢å¼•ï¼š




1.
æ™‚é–“æˆ³è¨˜ç´¢å¼•ï¼ˆæœ€é‡è¦ï¼‰ï¼š
CREATE INDEX idx_timestamp ON audit_log (timestamp);ç”¨é€”ï¼šæ‰€æœ‰æ—¥æœŸç¯„åœæŸ¥è©¢çš„åŸºç¤ç´¢å¼•

2.
å¤±æ•—ç™»å…¥åˆ†æç´¢å¼•ï¼š
CREATE INDEX idx_failed_login ON audit_log (operation,retcode, timestamp);
ç”¨é€”ï¼šåµæ¸¬æš´åŠ›ç ´è§£æ”»æ“Šå’Œå¯ç–‘ç™»å…¥æ´»å‹•

3.
ç‰¹æ¬Šæ“ä½œåˆ†æç´¢å¼•ï¼š
CREATE INDEX idx_privileged_ops ON audit_log
(operation, timestamp);
ç”¨é€”ï¼šç›£æ§ç®¡ç†å“¡æ¬Šé™æ“ä½œå’Œç‰¹æ¬Šæå‡

4.
ä½¿ç”¨è€…æ´»å‹•åˆ†æç´¢å¼•ï¼š
CREATE INDEX idx_username_timestamp ON audit_log
(username, timestamp);
ç”¨é€”ï¼šä½¿ç”¨è€…è¡Œç‚ºåˆ†æå’Œå…§éƒ¨å¨è„…åµæ¸¬

5.
ä¸»æ©Ÿ/IPåˆ†æç´¢å¼•ï¼š
CREATE INDEX idx_host_operation ON audit_log (host,operation, timestamp);
ç”¨é€”ï¼šç¶²è·¯å®‰å…¨åˆ†æå’Œæœªæˆæ¬ŠIPåµæ¸¬

6.
éŒ¯èª¤åˆ†æç´¢å¼•ï¼š
CREATE INDEX idx_retcode_operation ON audit_log
(retcode, operation, timestamp);
ç”¨é€”ï¼šç³»çµ±éŒ¯èª¤ç›£æ§å’Œç•°å¸¸è¡Œç‚ºåˆ†æ




å»ºç«‹å®Œæˆå¾Œçš„æª¢æŸ¥ï¼š
-- æŸ¥çœ‹æ‰€æœ‰ç´¢å¼•
SHOW INDEX FROM audit_log;




-- æ›´æ–°çµ±è¨ˆè³‡è¨Š
ANALYZE TABLE audit_log;




ä½ å¯ä»¥å…ˆåŸ·è¡Œå‰ 3
å€‹æœ€é‡è¦çš„ç´¢å¼•ï¼Œæ¸¬è©¦æ•ˆèƒ½å¾Œå†æ±ºå®šæ˜¯å¦ç¹¼çºŒå»ºç«‹å…¶ä»–ç´¢å¼•ã€‚æ¯åŸ·è¡Œä¸€å€‹å°±å¯ä»¥æ¸¬è©¦ä¸€ä¸‹æŸ¥è©¢æ•ˆèƒ½çš„æ”¹å–„ã€‚




