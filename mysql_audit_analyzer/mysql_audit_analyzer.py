#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, sys, argparse, gzip, csv, calendar, tempfile
from datetime import datetime, timedelta
from typing import List, Optional
import pymysql
import smtplib
from email.message import EmailMessage

# åŠ å…¥ tqdm æ”¯æ´
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("âš ï¸  å»ºè­°å®‰è£ tqdm ä»¥ç²å¾—æ›´å¥½çš„é€²åº¦é¡¯ç¤ºï¼špip install tqdm")

# åŠ å…¥ dotenv æ”¯æ´
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("âŒ è«‹å…ˆå®‰è£ python-dotenvï¼špip install python-dotenv")
    sys.exit(1)

try:
    from reportlab.lib.pagesizes import A4
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib import colors
    REPORTLAB_AVAILABLE = True
except ImportError:
    REPORTLAB_AVAILABLE = False

class Config:
    def __init__(self):
        self.mysql_host = os.getenv('MYSQL_HOST', 'localhost')
        self.mysql_port = int(os.getenv('MYSQL_PORT', '3306'))
        self.mysql_user = os.getenv('MYSQL_USER', 'root')
        self.mysql_password = os.getenv('MYSQL_PASSWORD', '')
        self.mysql_db = os.getenv('MYSQL_DB', 'auditdb')
        self.log_base_path = os.getenv('LOG_BASE_PATH', '/var/log/mysql/audit')
        self.log_file_prefix = os.getenv('LOG_FILE_PREFIX', 'server_audit.log')
        self.output_dir = os.getenv('OUTPUT_DIR', '/tmp/mysql_reports')
        self.failed_login_threshold = int(os.getenv('FAILED_LOGIN_THRESHOLD', '5'))
        self.allowed_ips = [ip.strip() for ip in os.getenv('ALLOWED_IPS', '').split(',') if ip.strip()]
        self.after_hours_users = [u.strip() for u in os.getenv('AFTER_HOURS_USERS', '').split(',') if u.strip()]
        self.work_hour_start = int(os.getenv('WORK_HOUR_START', '9'))
        self.work_hour_end = int(os.getenv('WORK_HOUR_END', '18'))
        self.privileged_users = [u.strip() for u in os.getenv('PRIVILEGED_USERS', '').split(',') if u.strip()]
        self.report_title = os.getenv('REPORT_TITLE', 'MySQL Audit Log Security Analysis Report')
        self.company_name = os.getenv('COMPANY_NAME', 'Your Company')
        self.generate_pdf = os.getenv('GENERATE_PDF', 'true').lower() == 'true'
        self.generate_csv = os.getenv('GENERATE_CSV', 'true').lower() == 'true'
        self.privileged_keywords = [k.strip() for k in os.getenv(
            'PRIVILEGED_KEYWORDS',
            'CREATE USER,DROP USER,GRANT,REVOKE,CREATE DATABASE,DROP DATABASE,CREATE TABLE,DROP TABLE,ALTER USER,SET PASSWORD'
        ).split(',') if k.strip()]
        self.send_email = os.getenv('SEND_EMAIL', 'false').lower() == 'true'
        self.smtp_server = os.getenv('SMTP_SERVER', '')
        self.smtp_port = int(os.getenv('SMTP_PORT', '25')) 
        self.mail_from = os.getenv('MAIL_FROM', '')
        self.mail_to = [m.strip() for m in os.getenv('MAIL_TO', '').split(',') if m.strip()]
        # æ–°å¢ LOAD DATA INFILE ç›¸é—œè¨­å®š
        self.use_load_data_infile = os.getenv('USE_LOAD_DATA_INFILE', 'true').lower() == 'true'
        self.temp_dir = os.getenv('TEMP_DIR', '/tmp')

    def get_log_file_path(self, date_str: str = None) -> str:
        return os.path.join(self.log_base_path, self.log_file_prefix if not date_str else f"{self.log_file_prefix}-{date_str}")

    def as_dict(self):
        return {
            "MYSQL_HOST": self.mysql_host,
            "MYSQL_PORT": self.mysql_port,
            "MYSQL_USER": self.mysql_user,
            "MYSQL_PASSWORD": self.mysql_password,
            "MYSQL_DB": self.mysql_db,
            "LOG_BASE_PATH": self.log_base_path,
            "LOG_FILE_PREFIX": self.log_file_prefix,
            "OUTPUT_DIR": self.output_dir,
            "FAILED_LOGIN_THRESHOLD": self.failed_login_threshold,
            "ALLOWED_IPS": self.allowed_ips,
            "AFTER_HOURS_USERS": self.after_hours_users,
            "WORK_HOUR_START": self.work_hour_start,
            "WORK_HOUR_END": self.work_hour_end,
            "PRIVILEGED_USERS": self.privileged_users,
            "REPORT_TITLE": self.report_title,
            "COMPANY_NAME": self.company_name,
            "GENERATE_PDF": self.generate_pdf,
            "GENERATE_CSV": self.generate_csv,
            "PRIVILEGED_KEYWORDS": self.privileged_keywords,
            "SEND_EMAIL": self.send_email,
            "SMTP_SERVER": self.smtp_server,
            "SMTP_PORT": self.smtp_port,
            "MAIL_FROM": self.mail_from,
            "MAIL_TO": self.mail_to,
            "USE_LOAD_DATA_INFILE": self.use_load_data_infile,
            "TEMP_DIR": self.temp_dir,
        }

def get_db_conn(config: Config):
    return pymysql.connect(
        host=config.mysql_host,
        port=config.mysql_port,
        user=config.mysql_user,
        password=config.mysql_password,
        database=config.mysql_db,
        charset='utf8mb4',
        autocommit=True,
        local_infile=True  # å•Ÿç”¨ LOAD DATA LOCAL INFILE
    )

def get_file_line_count(file_path):
    """å¿«é€Ÿè¨ˆç®—æª”æ¡ˆè¡Œæ•¸ï¼Œç”¨æ–¼é€²åº¦æ¢"""
    try:
        if file_path.endswith('.gz'):
            with gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') as f:
                return sum(1 for _ in f)
        else:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return sum(1 for _ in f)
    except:
        return 0

def import_log_file_to_db_optimized(file_path, log_date, conn, config):
    """
    ä½¿ç”¨ LOAD DATA INFILE å„ªåŒ–ç‰ˆæœ¬çš„æ—¥èªŒåŒ¯å…¥å‡½æ•¸ï¼ˆåŠ å…¥é€²åº¦æ¢ï¼‰
    """
    print(f"ğŸš€ é–‹å§‹å„ªåŒ–åŒ¯å…¥ {file_path}...")
    
    # è¨ˆç®—æª”æ¡ˆè¡Œæ•¸ç”¨æ–¼é€²åº¦æ¢
    if TQDM_AVAILABLE:
        print("ğŸ“Š æ­£åœ¨è¨ˆç®—æª”æ¡ˆå¤§å°...")
        total_lines = get_file_line_count(file_path)
        if total_lines == 0:
            print(f"âš ï¸  æª”æ¡ˆ {file_path} æ²’æœ‰è³‡æ–™")
            return
    
    # å»ºç«‹è‡¨æ™‚ CSV æª”æ¡ˆ
    temp_csv = tempfile.NamedTemporaryFile(
        mode='w', 
        suffix='.csv', 
        dir=config.temp_dir,
        delete=False,
        encoding='utf-8'
    )
    
    try:
        # è®€å–åŸå§‹æ—¥èªŒæª”æ¡ˆä¸¦è½‰æ›ç‚ºæ¨™æº– CSV æ ¼å¼
        with (gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') 
              if file_path.endswith('.gz') 
              else open(file_path, 'r', encoding='utf-8', errors='ignore')) as f:
            
            reader = csv.reader(f)
            writer = csv.writer(temp_csv, quoting=csv.QUOTE_ALL)
            
            # å»ºç«‹é€²åº¦æ¢
            if TQDM_AVAILABLE:
                progress_bar = tqdm(
                    total=total_lines,
                    desc="ğŸ“ è™•ç†æ—¥èªŒè³‡æ–™",
                    unit="è¡Œ",
                    unit_scale=True,
                    colour='green'
                )
            
            row_count = 0
            start_time = datetime.now()
            
            for row in reader:
                # ç¢ºä¿æ¬„ä½æ•¸é‡ä¸€è‡´
                row += [''] * (10 - len(row))
                timestamp, server_host, username, host, connection_id, query_id, operation, database, query, retcode = row[:10]
                
                # è™•ç† retcode
                try:
                    retcode = int(retcode) if retcode else 0
                except:
                    retcode = 0
                
                # å¯«å…¥è‡¨æ™‚ CSV æª”æ¡ˆ
                writer.writerow([
                    log_date, timestamp, server_host, username, host, 
                    connection_id, query_id, operation, database, query, retcode
                ])
                row_count += 1
                
                # æ›´æ–°é€²åº¦æ¢
                if TQDM_AVAILABLE:
                    progress_bar.update(1)
                    if row_count % 10000 == 0:  # æ¯ 10000 ç­†æ›´æ–°ä¸€æ¬¡æè¿°
                        progress_bar.set_postfix({
                            'å·²è™•ç†': f'{row_count:,}',
                            'é€Ÿåº¦': f'{row_count/(datetime.now()-start_time).total_seconds():.0f}/ç§’'
                        })
            
            if TQDM_AVAILABLE:
                progress_bar.close()
        
        temp_csv.close()
        
        if row_count == 0:
            print(f"âš ï¸  æª”æ¡ˆ {file_path} æ²’æœ‰è³‡æ–™")
            return
        
        processing_time = (datetime.now() - start_time).total_seconds()
        print(f"âœ… è³‡æ–™è™•ç†å®Œæˆ: {row_count:,} ç­†ï¼Œè€—æ™‚ {processing_time:.2f} ç§’")
        
        # ä½¿ç”¨ LOAD DATA LOCAL INFILE æ‰¹é‡è¼‰å…¥
        print("ğŸ’¾ æ­£åœ¨è¼‰å…¥è³‡æ–™åˆ°è³‡æ–™åº«...")
        db_start_time = datetime.now()
        
        with conn.cursor() as cur:
            # å…ˆæª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨è©²æ—¥æœŸçš„è³‡æ–™ï¼Œå¦‚æœæœ‰å‰‡å…ˆåˆªé™¤
            cur.execute("DELETE FROM audit_log WHERE log_date = %s", (log_date,))
            deleted_count = cur.rowcount
            if deleted_count > 0:
                print(f"ğŸ—‘ï¸  åˆªé™¤èˆŠè³‡æ–™ {deleted_count:,} ç­†")
            
            # åŸ·è¡Œ LOAD DATA LOCAL INFILE
            load_sql = f"""
            LOAD DATA LOCAL INFILE '{temp_csv.name}'
            INTO TABLE audit_log
            FIELDS TERMINATED BY ','
            OPTIONALLY ENCLOSED BY '"'
            LINES TERMINATED BY '\\n'
            (log_date, timestamp, server_host, username, host, connection_id, query_id, operation, dbname, query, retcode)
            """
            
            cur.execute(load_sql)
            
            # ç²å–å¯¦éš›è¼‰å…¥çš„è¡Œæ•¸
            cur.execute("SELECT ROW_COUNT()")
            loaded_rows = cur.fetchone()[0]
            
            db_duration = (datetime.now() - db_start_time).total_seconds()
            total_duration = (datetime.now() - start_time).total_seconds()
            
            print(f"âœ… å„ªåŒ–åŒ¯å…¥ {os.path.basename(file_path)} å®Œæˆ")
            print(f"   ğŸ“Š è™•ç†è³‡æ–™: {row_count:,} ç­†")
            print(f"   ğŸ“¥ è¼‰å…¥è³‡æ–™: {loaded_rows:,} ç­†")
            print(f"   â±ï¸  è™•ç†è€—æ™‚: {processing_time:.2f} ç§’")
            print(f"   ğŸ’¾ è¼‰å…¥è€—æ™‚: {db_duration:.2f} ç§’")
            print(f"   ğŸ•’ ç¸½è€—æ™‚: {total_duration:.2f} ç§’")
            print(f"   ğŸš€ ç¸½é€Ÿåº¦: {loaded_rows/total_duration:.0f} ç­†/ç§’")
            
    except Exception as e:
        print(f"âŒ å„ªåŒ–åŒ¯å…¥å¤±æ•—: {e}")
        # å¦‚æœ LOAD DATA INFILE å¤±æ•—ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•
        print("ğŸ”„ å›é€€åˆ°åŸå§‹åŒ¯å…¥æ–¹æ³•...")
        import_log_file_to_db_fallback(file_path, log_date, conn)
        
    finally:
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        try:
            os.unlink(temp_csv.name)
        except:
            pass

def import_log_file_to_db_fallback(file_path, log_date, conn):
    """
    åŸå§‹çš„é€ç­†æ’å…¥æ–¹æ³•ï¼ˆä½œç‚ºå‚™ç”¨æ–¹æ¡ˆï¼ŒåŠ å…¥é€²åº¦æ¢ï¼‰
    """
    print(f"ğŸ“ ä½¿ç”¨åŸå§‹æ–¹æ³•åŒ¯å…¥ {file_path}...")
    
    # è¨ˆç®—æª”æ¡ˆè¡Œæ•¸ç”¨æ–¼é€²åº¦æ¢
    if TQDM_AVAILABLE:
        print("ğŸ“Š æ­£åœ¨è¨ˆç®—æª”æ¡ˆå¤§å°...")
        total_lines = get_file_line_count(file_path)
        if total_lines == 0:
            print(f"âš ï¸  æª”æ¡ˆ {file_path} æ²’æœ‰è³‡æ–™")
            return
    
    with (gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') 
          if file_path.endswith('.gz') 
          else open(file_path, 'r', encoding='utf-8', errors='ignore')) as f:
        
        reader = csv.reader(f)
        data = []
        
        # å»ºç«‹é€²åº¦æ¢
        if TQDM_AVAILABLE:
            progress_bar = tqdm(
                total=total_lines,
                desc="ğŸ“ è®€å–æ—¥èªŒè³‡æ–™",
                unit="è¡Œ",
                unit_scale=True,
                colour='blue'
            )
        
        start_time = datetime.now()
        
        for row in reader:
            row += [''] * (10 - len(row))
            timestamp, server_host, username, host, connection_id, query_id, operation, database, query, retcode = row[:10]
            try:
                retcode = int(retcode) if retcode else 0
            except:
                retcode = 0
            data.append((log_date, timestamp, server_host, username, host, connection_id, query_id, operation, database, query, retcode))
            
            if TQDM_AVAILABLE:
                progress_bar.update(1)
        
        if TQDM_AVAILABLE:
            progress_bar.close()
        
        if not data:
            print(f"âš ï¸  æª”æ¡ˆ {file_path} æ²’æœ‰è³‡æ–™")
            return
        
        processing_time = (datetime.now() - start_time).total_seconds()
        print(f"âœ… è³‡æ–™è®€å–å®Œæˆ: {len(data):,} ç­†ï¼Œè€—æ™‚ {processing_time:.2f} ç§’")
        
        # è³‡æ–™åº«æ“ä½œ
        print("ğŸ’¾ æ­£åœ¨å¯«å…¥è³‡æ–™åº«...")
        db_start_time = datetime.now()
        
        with conn.cursor() as cur:
            # å…ˆåˆªé™¤è©²æ—¥æœŸçš„èˆŠè³‡æ–™
            cur.execute("DELETE FROM audit_log WHERE log_date = %s", (log_date,))
            deleted_count = cur.rowcount
            if deleted_count > 0:
                print(f"ğŸ—‘ï¸  åˆªé™¤èˆŠè³‡æ–™ {deleted_count:,} ç­†")
            
            # æ‰¹é‡æ’å…¥ï¼ˆä½¿ç”¨ executemanyï¼‰
            sql = """INSERT INTO audit_log
                    (log_date, timestamp, server_host, username, host, connection_id, query_id, operation, dbname, query, retcode)
                    VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)"""
            
            # å¦‚æœè³‡æ–™é‡å¾ˆå¤§ï¼Œå¯ä»¥åˆ†æ‰¹è™•ç†
            batch_size = 10000
            if len(data) > batch_size:
                if TQDM_AVAILABLE:
                    batch_progress = tqdm(
                        total=len(data),
                        desc="ğŸ’¾ æ‰¹é‡å¯«å…¥",
                        unit="ç­†",
                        unit_scale=True,
                        colour='cyan'
                    )
                
                for i in range(0, len(data), batch_size):
                    batch = data[i:i+batch_size]
                    cur.executemany(sql, batch)
                    if TQDM_AVAILABLE:
                        batch_progress.update(len(batch))
                
                if TQDM_AVAILABLE:
                    batch_progress.close()
            else:
                cur.executemany(sql, data)
            
            db_duration = (datetime.now() - db_start_time).total_seconds()
            total_duration = (datetime.now() - start_time).total_seconds()
            
            print(f"âœ… åŸå§‹æ–¹æ³•åŒ¯å…¥ {os.path.basename(file_path)} å®Œæˆ")
            print(f"   ğŸ“Š è¼‰å…¥è³‡æ–™: {len(data):,} ç­†")
            print(f"   â±ï¸  è®€å–è€—æ™‚: {processing_time:.2f} ç§’")
            print(f"   ğŸ’¾ å¯«å…¥è€—æ™‚: {db_duration:.2f} ç§’")
            print(f"   ğŸ•’ ç¸½è€—æ™‚: {total_duration:.2f} ç§’")
            print(f"   ğŸŒ ç¸½é€Ÿåº¦: {len(data)/total_duration:.0f} ç­†/ç§’")

def import_log_file_to_db(file_path, log_date, conn, config=None):
    """
    ä¸»è¦çš„æ—¥èªŒåŒ¯å…¥å‡½æ•¸ - æ ¹æ“šè¨­å®šé¸æ“‡å„ªåŒ–æˆ–åŸå§‹æ–¹æ³•
    """
    if config and config.use_load_data_infile:
        import_log_file_to_db_optimized(file_path, log_date, conn, config)
    else:
        import_log_file_to_db_fallback(file_path, log_date, conn)

def get_log_files_for_month(config, month_str):
    log_files = []
    year, month = map(int, month_str.split('-'))
    days_in_month = calendar.monthrange(year, month)[1]
    for day in range(1, days_in_month + 1):
        date_str = f"{year:04d}-{month:02d}-{day:02d}"
        log_path = config.get_log_file_path(date_str)
        if os.path.exists(log_path):
            log_files.append((log_path, date_str))
        elif os.path.exists(log_path + '.gz'):
            log_files.append((log_path + '.gz', date_str))
    return log_files

def get_log_file_for_date(config, date_str):
    log_path = config.get_log_file_path(date_str)
    if os.path.exists(log_path):
        return (log_path, date_str)
    elif os.path.exists(log_path + '.gz'):
        return (log_path + '.gz', date_str)
    else:
        return None

# ========== åˆ†ææŸ¥è©¢ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰ ==========

def run_analysis_with_progress(analysis_functions, conn, date_filter, date_filter_value, config):
    """
    åŸ·è¡Œæ‰€æœ‰åˆ†æåŠŸèƒ½ä¸¦é¡¯ç¤ºé€²åº¦
    """
    results = {}
    
    if TQDM_AVAILABLE:
        progress_bar = tqdm(
            total=len(analysis_functions),
            desc="ğŸ” åŸ·è¡Œå®‰å…¨åˆ†æ",
            unit="é …ç›®",
            colour='magenta'
        )
    
    for name, func, args in analysis_functions:
        start_time = datetime.now()
        
        if TQDM_AVAILABLE:
            progress_bar.set_description(f"ğŸ” åˆ†æ: {name}")
        
        try:
            if args:
                results[name] = func(conn, date_filter, date_filter_value, *args)
            else:
                results[name] = func(conn, date_filter, date_filter_value)
            
            duration = (datetime.now() - start_time).total_seconds()
            
            if not TQDM_AVAILABLE:
                print(f"âœ… {name} å®Œæˆ ({duration:.2f}ç§’)")
                
        except Exception as e:
            print(f"âŒ {name} å¤±æ•—: {e}")
            results[name] = None
        
        if TQDM_AVAILABLE:
            progress_bar.update(1)
    
    if TQDM_AVAILABLE:
        progress_bar.close()
    
    return results

def analyze_summary(conn, date_filter, date_filter_value):
    with conn.cursor() as cur:
        if isinstance(date_filter_value, tuple):
            params = date_filter_value
        else:
            params = (date_filter_value,)
        cur.execute("SELECT COUNT(*), COUNT(DISTINCT username), COUNT(DISTINCT host) FROM audit_log WHERE " + date_filter, params)
        total_events, unique_users, unique_hosts = cur.fetchone()
        return {
            'total_events': total_events,
            'unique_users': unique_users,
            'unique_hosts': unique_hosts
        }

def analyze_failed_logins(conn, date_filter, date_filter_value, threshold=5):
    with conn.cursor() as cur:
        if isinstance(date_filter_value, tuple):
            params = date_filter_value + (threshold,)
            params2 = date_filter_value
        else:
            params = (date_filter_value, threshold)
            params2 = (date_filter_value,)
        cur.execute(
            f"""SELECT username, COUNT(*) as fail_count
                FROM audit_log
                WHERE operation='CONNECT' AND retcode!=0 AND {date_filter}
                GROUP BY username
                HAVING fail_count >= %s
                ORDER BY fail_count DESC
            """,
            params
        )
        by_user = cur.fetchall()
        cur.execute(
            f"""SELECT host, COUNT(*) as fail_count
                FROM audit_log
                WHERE operation='CONNECT' AND retcode!=0 AND {date_filter}
                GROUP BY host
                HAVING fail_count >= %s
                ORDER BY fail_count DESC
            """,
            params
        )
        by_ip = cur.fetchall()
        cur.execute(
            f"SELECT COUNT(*) FROM audit_log WHERE operation='CONNECT' AND retcode!=0 AND {date_filter}",
            params2
        )
        total = cur.fetchone()[0]
        return {
            'total': total,
            'by_user': by_user,
            'by_ip': by_ip
        }

def analyze_privileged_operations(conn, date_filter, date_filter_value, keywords):
    like_clauses = " OR ".join(["UPPER(query) LIKE %s" for _ in keywords])
    like_params = [f"%{k.upper()}%" for k in keywords]
    if isinstance(date_filter_value, tuple):
        params = like_params + list(date_filter_value)
    else:
        params = like_params + [date_filter_value]

    with conn.cursor() as cur:
        cur.execute(
            f"""SELECT username, COUNT(*) as cnt
                FROM audit_log
                WHERE operation='QUERY' AND ({like_clauses}) AND {date_filter}
                GROUP BY username
                ORDER BY cnt DESC
            """,
            params
        )
        by_user = cur.fetchall()
        cur.execute(
            f"""SELECT COUNT(*) FROM audit_log
                WHERE operation='QUERY' AND ({like_clauses}) AND {date_filter}
            """,
            params
        )
        total = cur.fetchone()[0]
        cur.execute(
            f"""SELECT username, query, timestamp
                FROM audit_log
                WHERE operation='QUERY' AND ({like_clauses}) AND {date_filter}
                ORDER BY timestamp DESC
            """,
            params
        )
        details = cur.fetchall()
        return {
            'total': total,
            'by_user': by_user,
            'details': details
        }

def analyze_operation_stats(conn, date_filter, date_filter_value):
    with conn.cursor() as cur:
        if isinstance(date_filter_value, tuple):
            params = date_filter_value
        else:
            params = (date_filter_value,)
        cur.execute(
            f"""SELECT operation, COUNT(*) as cnt
                FROM audit_log
                WHERE {date_filter}
                GROUP BY operation
                ORDER BY cnt DESC
            """,
            params
        )
        return cur.fetchall()

def analyze_error_codes(conn, date_filter, date_filter_value):
    with conn.cursor() as cur:
        if isinstance(date_filter_value, tuple):
            params = date_filter_value
        else:
            params = (date_filter_value,)
        cur.execute(
            f"""SELECT retcode, COUNT(*) as cnt
                FROM audit_log
                WHERE retcode!=0 AND operation!='CHANGEUSER' AND {date_filter}
                GROUP BY retcode
                ORDER BY cnt DESC
            """,
            params
        )
        error_codes = cur.fetchall()
        cur.execute(
            f"SELECT COUNT(*) FROM audit_log WHERE retcode!=0 AND operation!='CHANGEUSER' AND {date_filter}",
            params
        )
        total_errors = cur.fetchone()[0]
        return {
            'total_errors': total_errors,
            'error_codes': error_codes
        }

def analyze_after_hours_access(conn, date_filter, date_filter_value, users, wh_start, wh_end):
    if not users:
        return {'total': 0, 'details': []}
    user_list = ','.join(["'%s'" % u for u in users])
    with conn.cursor() as cur:
        if isinstance(date_filter_value, tuple):
            params = date_filter_value
        else:
            params = (date_filter_value,)
        cur.execute(
            f"""SELECT username, host, operation, timestamp
                FROM audit_log
                WHERE username IN ({user_list}) AND {date_filter}
            """,
            params
        )
        rows = cur.fetchall()
        after_hours = []
        for username, host, operation, ts in rows:
            try:
                dt = datetime.strptime(ts, "%Y%m%d %H:%M:%S")
            except:
                continue
            if dt.weekday() >= 5 or not (wh_start <= dt.hour < wh_end):
                after_hours.append((username, host, operation, dt.strftime('%Y-%m-%d %H:%M:%S')))
        return {'total': len(after_hours), 'details': after_hours[:50]}

def analyze_privileged_user_logins(conn, date_filter, date_filter_value, users):
    if not users:
        return {'total': 0, 'by_user': [], 'details': []}
    user_list = ','.join(["'%s'" % u for u in users])
    with conn.cursor() as cur:
        if isinstance(date_filter_value, tuple):
            params = date_filter_value
        else:
            params = (date_filter_value,)
        cur.execute(
            f"""SELECT username, COUNT(*) as cnt
                FROM audit_log
                WHERE operation='CONNECT' AND username IN ({user_list}) AND {date_filter}
                GROUP BY username
                ORDER BY cnt DESC
            """,
            params
        )
        by_user = cur.fetchall()
        cur.execute(
            f"""SELECT username, host, timestamp
                FROM audit_log
                WHERE operation='CONNECT' AND username IN ({user_list}) AND {date_filter}
                ORDER BY timestamp DESC
            """,
            params
        )
        details = cur.fetchall()
        cur.execute(
            f"""SELECT COUNT(*) FROM audit_log
                WHERE operation='CONNECT' AND username IN ({user_list}) AND {date_filter}
            """,
            params
        )
        total = cur.fetchone()[0]
        return {'total': total, 'by_user': by_user, 'details': details}

def analyze_non_whitelisted_ips(conn, date_filter, date_filter_value, allowed_ips):
    if not allowed_ips:
        return {'total': 0, 'by_ip': [], 'details': []}
    ip_list = ','.join(["'%s'" % ip for ip in allowed_ips])
    with conn.cursor() as cur:
        if isinstance(date_filter_value, tuple):
            params = date_filter_value
        else:
            params = (date_filter_value,)
# çºŒå‰é¢çš„ç¨‹å¼ç¢¼...

        cur.execute(
            f"""SELECT host, COUNT(*) as cnt
                FROM audit_log
                WHERE host NOT IN ({ip_list}) AND operation!='CHANGEUSER' AND {date_filter}
                GROUP BY host
                ORDER BY cnt DESC
            """,
            params
        )
        by_ip = cur.fetchall()
        cur.execute(
            f"""SELECT username, host, operation, timestamp
                FROM audit_log
                WHERE host NOT IN ({ip_list}) AND operation!='CHANGEUSER' AND {date_filter}
                ORDER BY timestamp DESC
            """,
            params
        )
        details = cur.fetchall()
        cur.execute(
            f"""SELECT COUNT(*) FROM audit_log
                WHERE host NOT IN ({ip_list}) AND operation!='CHANGEUSER' AND {date_filter}
            """,
            params
        )
        total = cur.fetchone()[0]
        return {'total': total, 'by_ip': by_ip, 'details': details}


# ========== å ±è¡¨ç”¢ç”Ÿï¼ˆCSVï¼‰ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰ ==========

def generate_csv_report(output_dir, report_title, summary, failed, priv_ops, priv_user_logins, op_stats, err, after_hours, non_whitelisted, period_label):
    """
    ç”¢ç”Ÿ CSV å ±è¡¨ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰
    """
    os.makedirs(output_dir, exist_ok=True)
    csv_file = os.path.join(output_dir, f'mysql_audit_analysis_{period_label}.csv')
    
    # è¨ˆç®—ç¸½å…±è¦å¯«å…¥çš„å€å¡Šæ•¸é‡
    total_sections = 9
    
    if TQDM_AVAILABLE:
        progress_bar = tqdm(
            total=total_sections,
            desc="ğŸ“Š ç”¢ç”Ÿ CSV å ±è¡¨",
            unit="å€å¡Š",
            colour='yellow'
        )
    
    w = lambda row: writer.writerow(row)
    
    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # æ¨™é¡Œ
        w([f'{report_title} - {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}'])
        w([])
        
        # åŸºæœ¬çµ±è¨ˆ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥åŸºæœ¬çµ±è¨ˆ")
        w(['=== Basic Statistics ==='])
        w(['Total Events', summary['total_events']])
        w(['Unique Users', summary['unique_users']])
        w(['Unique Hosts', summary['unique_hosts']])
        w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # å¤±æ•—ç™»å…¥åˆ†æ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥å¤±æ•—ç™»å…¥åˆ†æ")
        w(['=== Failed Login Analysis ==='])
        w(['Total Failed Logins', failed['total']])
        w([])
        if failed['by_user']:
            w(['Suspicious Users (Above Threshold)'])
            w(['Username', 'Failed Count'])
            for row in failed['by_user']: 
                w(list(row))
            w([])
        if failed['by_ip']:
            w(['Suspicious IPs (Above Threshold)'])
            w(['IP Address', 'Failed Count'])
            for row in failed['by_ip']: 
                w(list(row))
            w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # ç‰¹æ¬Šæ“ä½œåˆ†æ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥ç‰¹æ¬Šæ“ä½œåˆ†æ")
        w(['=== Privileged Operations Analysis ==='])
        w(['Total Privileged Operations', priv_ops['total']])
        w([])
        if priv_ops['by_user']:
            w(['By User Statistics'])
            w(['Username', 'Operation Count'])
            for row in priv_ops['by_user']: 
                w(list(row))
            w([])
        if priv_ops.get('details'):
            w(['Detailed Privileged Operations (SQL)'])
            w(['Username', 'SQL', 'Timestamp'])
            for row in priv_ops['details']:
                w(list(row))
            w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # ç‰¹æ¬Šå¸³è™Ÿç™»å…¥åˆ†æ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥ç‰¹æ¬Šå¸³è™Ÿç™»å…¥åˆ†æ")
        w(['=== Privileged Account Login Analysis ==='])
        w(['Total Privileged Account Logins', priv_user_logins['total']])
        if priv_user_logins['by_user']:
            w(['Username', 'Login Count'])
            for row in priv_user_logins['by_user']: 
                w(list(row))
        w([])
        if priv_user_logins['details']:
            w(['Detailed Privileged Account Login Records'])
            w(['Username', 'Host', 'Timestamp'])
            for row in priv_user_logins['details']:
                w(list(row))
            w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # æ“ä½œé¡å‹çµ±è¨ˆ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥æ“ä½œé¡å‹çµ±è¨ˆ")
        w(['=== Operation Type Statistics ==='])
        w(['Operation Type', 'Count'])
        for row in op_stats: 
            w(list(row))
        w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # éŒ¯èª¤åˆ†æ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥éŒ¯èª¤åˆ†æ")
        w(['=== Error Analysis ==='])
        w(['Total Errors', err['total_errors']])
        w([])
        if err['error_codes']:
            w(['Error Code Statistics'])
            w(['Error Code', 'Count'])
            for row in err['error_codes']: 
                w(list(row))
        w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # éä¸Šç­æ™‚é–“å­˜å–åˆ†æ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥éä¸Šç­æ™‚é–“å­˜å–åˆ†æ")
        w(['=== After-hours Access (Specify account) ==='])
        w(['Total After-hours Access', after_hours['total']])
        if after_hours['details']:
            w(['Username', 'Host', 'Operation', 'Time'])
            for row in after_hours['details']:
                w(list(row))
        w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # éç™½åå–® IP å­˜å–åˆ†æ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š å¯«å…¥éç™½åå–® IP åˆ†æ")
        w(['=== Non-whitelisted IP Access Analysis ==='])
        w(['Total Events from Non-whitelisted IPs', non_whitelisted['total']])
        if non_whitelisted['by_ip']:
            w(['Non-whitelisted IPs'])
            w(['IP Address', 'Event Count'])
            for row in non_whitelisted['by_ip']: 
                w(list(row))
            w([])
        if non_whitelisted['details']:
            w(['Details (Username, Host, Operation, Time)'])
            for row in non_whitelisted['details']:
                w(list(row))
        w([])
        if TQDM_AVAILABLE:
            progress_bar.update(1)
        
        # å®Œæˆ
        if TQDM_AVAILABLE:
            progress_bar.set_description("ğŸ“Š CSV å ±è¡¨å®Œæˆ")
            progress_bar.update(1)
    
    if TQDM_AVAILABLE:
        progress_bar.close()
    
    print(f"âœ… CSV report generated: {csv_file}")
    return csv_file

# ========== å ±è¡¨ç”¢ç”Ÿï¼ˆPDFï¼‰ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰ ==========

def generate_pdf_report(output_dir, report_title, summary, failed, priv_ops, priv_user_logins, op_stats, err, after_hours, non_whitelisted, period_label):
    """
    ç”¢ç”Ÿ PDF å ±è¡¨ï¼ˆåŠ å…¥é€²åº¦é¡¯ç¤ºï¼‰
    """
    if not REPORTLAB_AVAILABLE:
        print("âŒ ReportLab not installed, PDF report cannot be generated.")
        return None
    
    os.makedirs(output_dir, exist_ok=True)
    pdf_file = os.path.join(output_dir, f'mysql_audit_analysis_{period_label}.pdf')
    
    print("ğŸ“„ æ­£åœ¨ç”¢ç”Ÿ PDF å ±è¡¨...")
    start_time = datetime.now()
    
    doc = SimpleDocTemplate(pdf_file, pagesize=A4)
    styles = getSampleStyleSheet()
    story = []
    
    # æ¨™é¡Œ
    story.append(Paragraph(f"<b>{report_title}</b>", styles['Title']))
    story.append(Spacer(1, 12))
    story.append(Paragraph(f"Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", styles['Normal']))
    story.append(Spacer(1, 8))

    def add_table(title, data, colnames):
        story.append(Spacer(1, 8))
        story.append(Paragraph(f"<b>{title}</b>", styles['Heading3']))
        if not data:
            story.append(Paragraph("(No data)", styles['Normal']))
        else:
            table = Table([colnames] + list(data), hAlign='LEFT')
            table.setStyle(TableStyle([
                ('BACKGROUND', (0,0), (-1,0), colors.lightgrey),
                ('GRID', (0,0), (-1,-1), 0.5, colors.grey),
                ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),
            ]))
            story.append(table)

    # åŸºæœ¬çµ±è¨ˆ
    story.append(Paragraph("<b>=== Basic Statistics ===</b>", styles['Heading2']))
    story.append(Paragraph(f"Total Events: {summary['total_events']}<br />"
                        f"Unique Users: {summary['unique_users']}<br />"
                        f"Unique Hosts: {summary['unique_hosts']}", styles['Normal']))
    
    # å„ç¨®åˆ†æè¡¨æ ¼
    add_table("Suspicious Users (Failed Logins)", failed['by_user'], ['Username', 'Failed Count'])
    add_table("Suspicious IPs (Failed Logins)", failed['by_ip'], ['IP Address', 'Failed Count'])
    add_table("Privileged Operations by User", priv_ops['by_user'], ['Username', 'Operation Count'])
    add_table("Detailed Privileged Operations (SQL)", priv_ops.get('details', []), ['Username', 'SQL', 'Timestamp'])
    add_table("Privileged Account Login Statistics", priv_user_logins['by_user'], ['Username', 'Login Count'])
    add_table("Privileged Account Login Details", priv_user_logins['details'], ['Username', 'IP Address', 'Timestamp'])
    add_table("Operation Type Statistics", op_stats, ['Operation', 'Count'])
    add_table("Error Code Statistics", err['error_codes'], ['Error Code', 'Count'])
    add_table("After-hours Access (Specify account)", after_hours['details'], ['Username', 'IP Address', 'Operation', 'Timestamp'])
    add_table("Non-whitelisted IPs", non_whitelisted['by_ip'], ['IP Address', 'Event Count'])
    add_table("Non-whitelisted IP Access Details", non_whitelisted['details'], ['Username', 'IP Address', 'Operation', 'Timestamp'])

    doc.build(story)
    
    duration = (datetime.now() - start_time).total_seconds()
    print(f"âœ… PDF report generated: {pdf_file} (è€—æ™‚ {duration:.2f} ç§’)")
    return pdf_file

# ========== éƒµä»¶å¯„é€ ==========

def send_email_with_attachment(config: Config, subject, body, attachment_path):
    if not (config.smtp_server and config.mail_from and config.mail_to):
        print("âŒ SMTP æˆ–æ”¶ä»¶äººè¨­å®šä¸å®Œæ•´ï¼Œç„¡æ³•å¯„ä¿¡ã€‚")
        return
    
    print("ğŸ“§ æ­£åœ¨å¯„é€éƒµä»¶...")
    start_time = datetime.now()
    
    msg = EmailMessage()
    msg['Subject'] = subject
    msg['From'] = config.mail_from
    msg['To'] = ', '.join(config.mail_to)
    msg.set_content(body)
    
    with open(attachment_path, 'rb') as f:
        file_data = f.read()
        file_name = os.path.basename(attachment_path)
    
    maintype, subtype = ('application', 'pdf') if file_name.endswith('.pdf') else ('application', 'octet-stream')
    msg.add_attachment(file_data, maintype=maintype, subtype=subtype, filename=file_name)
    
    try:
        with smtplib.SMTP(config.smtp_server, config.smtp_port) as server:
            server.send_message(msg)
        
        duration = (datetime.now() - start_time).total_seconds()
        print(f"ğŸ“§ éƒµä»¶å·²å¯„å‡ºè‡³: {', '.join(config.mail_to)} (è€—æ™‚ {duration:.2f} ç§’)")
        
    except Exception as e:
        print(f"âŒ éƒµä»¶å¯„é€å¤±æ•—: {e}")

# ========== ä¸»ç¨‹å¼ï¼ˆåŠ å…¥å®Œæ•´çš„é€²åº¦è¿½è¹¤ï¼‰ ==========

def main():
    parser = argparse.ArgumentParser(description='MySQL Audit Log Security Analyzer (MySQL backend) - Enhanced with Progress Tracking')
    parser.add_argument('--import-date', help='Import logs for specific date (format: YYYY-MM-DD)')
    parser.add_argument('--import-month', help='Import logs for specific month (format: YYYY-MM)')
    parser.add_argument('--analyze-date', help='Analyze logs for specific date (format: YYYY-MM-DD)')
    parser.add_argument('--analyze-month', help='Analyze logs for specific month (format: YYYY-MM)')
    parser.add_argument('--output-dir', help='Output directory')
    parser.add_argument('--csv-only', action='store_true', help='Generate CSV report only')
    parser.add_argument('--show-env', action='store_true', help='Show all env/config parameters and exit')
    parser.add_argument('--disable-load-data', action='store_true', help='Disable LOAD DATA INFILE optimization')
    parser.add_argument('--disable-progress', action='store_true', help='Disable progress bars (useful for automation)')
    
    args = parser.parse_args()
    config = Config()
    
    # å¦‚æœæŒ‡å®šäº† --disable-progressï¼Œå‰‡å…¨åŸŸåœç”¨ tqdm
    if args.disable_progress:
        global TQDM_AVAILABLE
        TQDM_AVAILABLE = False
        print("âš ï¸  å·²åœç”¨é€²åº¦æ¢é¡¯ç¤º")
    
    # å¦‚æœæŒ‡å®šäº† --disable-load-dataï¼Œå‰‡é—œé–‰å„ªåŒ–
    if args.disable_load_data:
        config.use_load_data_infile = False
        print("âš ï¸  å·²åœç”¨ LOAD DATA INFILE å„ªåŒ–")

    # é¡¯ç¤ºæ‰€æœ‰ env è¨­å®š
    if args.show_env:
        print("ğŸ” ç›®å‰æŠ“åˆ°çš„ .env/ç’°å¢ƒè®Šæ•¸åƒæ•¸å¦‚ä¸‹ï¼š\n")
        for k, v in config.as_dict().items():
            print(f"{k}: {v}")
        return

    # ç¨‹å¼é–‹å§‹åŸ·è¡Œæ™‚é–“
    program_start_time = datetime.now()
    print(f"ğŸš€ ç¨‹å¼é–‹å§‹åŸ·è¡Œ: {program_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        conn = get_db_conn(config)
        print("âœ… è³‡æ–™åº«é€£ç·šæˆåŠŸ")
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«é€£ç·šå¤±æ•—: {e}")
        return

    # åŒ¯å…¥æ—¥èªŒ
    if args.import_date:
        print(f"\nğŸ“… é–‹å§‹åŒ¯å…¥å–®æ—¥æ—¥èªŒ: {args.import_date}")
        log = get_log_file_for_date(config, args.import_date)
        if log:
            import_log_file_to_db(log[0], log[1], conn, config)
        else:
            print(f"âŒ No log file found for {args.import_date}")
        
        total_duration = (datetime.now() - program_start_time).total_seconds()
        print(f"\nğŸ‰ å–®æ—¥åŒ¯å…¥å®Œæˆï¼ç¸½è€—æ™‚: {total_duration:.2f} ç§’")
        return
        
    elif args.import_month:
        print(f"\nğŸ“… é–‹å§‹åŒ¯å…¥æœˆä»½æ—¥èªŒ: {args.import_month}")
        logs = get_log_files_for_month(config, args.import_month)
        if not logs:
            print(f"âŒ No log files found for {args.import_month}")
            return
        
        total_files = len(logs)
        print(f"ğŸ“ æ‰¾åˆ° {total_files} å€‹æ—¥èªŒæª”æ¡ˆ")
        
        # æœˆä»½åŒ¯å…¥é€²åº¦æ¢
        if TQDM_AVAILABLE:
            month_progress = tqdm(
                total=total_files,
                desc="ğŸ“‚ æœˆä»½åŒ¯å…¥é€²åº¦",
                unit="æª”æ¡ˆ",
                colour='green'
            )
        
        import_stats = {
            'total_files': 0,
            'success_files': 0,
            'failed_files': 0,
            'total_records': 0
        }
        
        for i, (log_path, log_date) in enumerate(logs, 1):
            if not TQDM_AVAILABLE:
                print(f"\nğŸ“ è™•ç†æª”æ¡ˆ {i}/{total_files}: {os.path.basename(log_path)}")
            else:
                month_progress.set_description(f"ğŸ“ è™•ç†: {os.path.basename(log_path)}")
            
            try:
                import_log_file_to_db(log_path, log_date, conn, config)
                import_stats['success_files'] += 1
            except Exception as e:
                print(f"âŒ æª”æ¡ˆ {log_path} åŒ¯å…¥å¤±æ•—: {e}")
                import_stats['failed_files'] += 1
            
            import_stats['total_files'] += 1
            
            if TQDM_AVAILABLE:
                month_progress.update(1)
                month_progress.set_postfix({
                    'æˆåŠŸ': import_stats['success_files'],
                    'å¤±æ•—': import_stats['failed_files']
                })
        
        if TQDM_AVAILABLE:
            month_progress.close()
        
        total_duration = (datetime.now() - program_start_time).total_seconds()
        print(f"\nğŸ‰ æœˆä»½åŒ¯å…¥å®Œæˆï¼")
        print(f"   ğŸ“ ç¸½æª”æ¡ˆæ•¸: {import_stats['total_files']}")
        print(f"   âœ… æˆåŠŸæª”æ¡ˆ: {import_stats['success_files']}")
        print(f"   âŒ å¤±æ•—æª”æ¡ˆ: {import_stats['failed_files']}")
        print(f"   â±ï¸  ç¸½è€—æ™‚: {total_duration:.2f} ç§’")
        print(f"   ğŸ“Š å¹³å‡æ¯æª”: {total_duration/import_stats['total_files']:.2f} ç§’")
        return

    # åˆ†æéšæ®µ
    print(f"\nğŸ” é–‹å§‹é€²è¡Œå®‰å…¨åˆ†æ...")
    analysis_start_time = datetime.now()
    
    if args.analyze_month:
        # ä¿®æ­£ï¼šæ­£ç¢ºæ”¯æ´ DATE å‹æ…‹çš„ log_date
        year, month = map(int, args.analyze_month.split('-'))
        days_in_month = calendar.monthrange(year, month)[1]
        date_start = f"{year:04d}-{month:02d}-01"
        date_end = f"{year:04d}-{month:02d}-{days_in_month:02d}"
        period_label = args.analyze_month.replace('-', '')
        date_filter = "log_date BETWEEN %s AND %s"
        date_filter_value = (date_start, date_end)
        print(f"ğŸ“Š åˆ†ææœŸé–“: {args.analyze_month} ({date_start} åˆ° {date_end})")
    else:
        date_str = args.analyze_date if args.analyze_date else datetime.now().strftime('%Y-%m-%d')
        period_label = date_str.replace('-', '')
        date_filter = "log_date = %s"
        date_filter_value = date_str
        print(f"ğŸ“Š åˆ†ææ—¥æœŸ: {date_str}")

    # å®šç¾©æ‰€æœ‰åˆ†æåŠŸèƒ½
    analysis_functions = [
        ("åŸºæœ¬çµ±è¨ˆ", analyze_summary, None),
        ("å¤±æ•—ç™»å…¥åˆ†æ", analyze_failed_logins, (config.failed_login_threshold,)),
        ("ç‰¹æ¬Šæ“ä½œåˆ†æ", analyze_privileged_operations, (config.privileged_keywords,)),
        ("æ“ä½œé¡å‹çµ±è¨ˆ", analyze_operation_stats, None),
        ("éŒ¯èª¤ä»£ç¢¼åˆ†æ", analyze_error_codes, None),
        ("éä¸Šç­æ™‚é–“å­˜å–", analyze_after_hours_access, (config.after_hours_users, config.work_hour_start, config.work_hour_end)),
        ("ç‰¹æ¬Šå¸³è™Ÿç™»å…¥", analyze_privileged_user_logins, (config.privileged_users,)),
        ("éç™½åå–®IPåˆ†æ", analyze_non_whitelisted_ips, (config.allowed_ips,))
    ]
    
    # åŸ·è¡Œåˆ†æ
    results = run_analysis_with_progress(analysis_functions, conn, date_filter, date_filter_value, config)
    
    # è§£æ§‹çµæœ
    summary = results.get("åŸºæœ¬çµ±è¨ˆ", {})
    failed = results.get("å¤±æ•—ç™»å…¥åˆ†æ", {})
    priv_ops = results.get("ç‰¹æ¬Šæ“ä½œåˆ†æ", {})
    op_stats = results.get("æ“ä½œé¡å‹çµ±è¨ˆ", [])
    err = results.get("éŒ¯èª¤ä»£ç¢¼åˆ†æ", {})
    after_hours = results.get("éä¸Šç­æ™‚é–“å­˜å–", {})
    priv_user_logins = results.get("ç‰¹æ¬Šå¸³è™Ÿç™»å…¥", {})
    non_whitelisted = results.get("éç™½åå–®IPåˆ†æ", {})
    
    analysis_duration = (datetime.now() - analysis_start_time).total_seconds()
    print(f"âœ… å®‰å…¨åˆ†æå®Œæˆï¼Œè€—æ™‚ {analysis_duration:.2f} ç§’")

    # å ±è¡¨ç”¢ç”Ÿéšæ®µ
    print(f"\nğŸ“Š é–‹å§‹ç”¢ç”Ÿå ±è¡¨...")
    report_start_time = datetime.now()
    
    output_dir = args.output_dir or config.output_dir
    csv_file = None
    pdf_file = None
    
    if config.generate_csv:
        csv_file = generate_csv_report(output_dir, config.report_title, summary, failed, priv_ops, priv_user_logins, op_stats, err, after_hours, non_whitelisted, period_label)
    
    if config.generate_pdf and not args.csv_only:
        pdf_file = generate_pdf_report(output_dir, config.report_title, summary, failed, priv_ops, priv_user_logins, op_stats, err, after_hours, non_whitelisted, period_label)
        
        if pdf_file and config.send_email:
            # å–å¾—åˆ†ææœŸé–“çš„æ—¥æœŸè³‡è¨Š
            if args.analyze_month:
                year, month = map(int, args.analyze_month.split('-'))
                period_text = f"{year}å¹´{month:02d}æœˆ"
            else:
                date_str = args.analyze_date if args.analyze_date else datetime.now().strftime('%Y-%m-%d')
                year, month, day = map(int, date_str.split('-'))
                period_text = f"{year}å¹´{month:02d}æœˆ{day:02d}æ—¥"
            
            send_email_with_attachment(
                config,
                subject=f"HamiPass MySQL ç¨½æ ¸æ—¥èªŒå®‰å…¨åˆ†æå ±å‘Š ({period_label})",
                body=f"æª¢é™„ HamiPass MySQL ç¨½æ ¸æ—¥èªŒå®‰å…¨åˆ†æå ±å‘Šï¼Œåˆ†ææœŸé–“ç‚º {period_text}ã€‚",
                attachment_path=pdf_file
            )
        elif pdf_file:
            print("âœ‰ï¸  SEND_EMAIL=falseï¼Œæœªé€²è¡Œéƒµä»¶å¯„é€ã€‚")
    elif args.csv_only:
        print("âœ‰ï¸  å·²æŒ‡å®š --csv-onlyï¼Œä¸ç”¢ç”ŸPDFäº¦ä¸å¯„ä¿¡ã€‚")
    else:
        print("âœ‰ï¸  æœªç”¢ç”ŸPDFï¼Œä¸é€²è¡Œå¯„ä¿¡ã€‚")

    report_duration = (datetime.now() - report_start_time).total_seconds()
    print(f"âœ… å ±è¡¨ç”¢ç”Ÿå®Œæˆï¼Œè€—æ™‚ {report_duration:.2f} ç§’")

    # ç¸½çµ
    total_duration = (datetime.now() - program_start_time).total_seconds()
    
    print("\n" + "="*60)
    print("ğŸ“Š Analysis Result Summary")
    print("="*60)
    print(f"ğŸ“… åˆ†ææœŸé–“: {period_label}")
    print(f"ğŸ“ˆ ç¸½äº‹ä»¶æ•¸: {summary.get('total_events', 0):,}")
    print(f"ğŸ‘¥ ç¨ç‰¹ä½¿ç”¨è€…: {summary.get('unique_users', 0):,}")
    print(f"ğŸ–¥ï¸  ç¨ç‰¹ä¸»æ©Ÿ: {summary.get('unique_hosts', 0):,}")
    print(f"âŒ å¤±æ•—ç™»å…¥: {failed.get('total', 0):,}")
    print(f"ğŸ” ç‰¹æ¬Šæ“ä½œ: {priv_ops.get('total', 0):,}")
    print(f"ğŸ‘‘ ç‰¹æ¬Šå¸³è™Ÿç™»å…¥: {priv_user_logins.get('total', 0):,}")
    print(f"âš ï¸  éŒ¯èª¤äº‹ä»¶: {err.get('total_errors', 0):,}")
    print(f"ğŸš« éç™½åå–®IPäº‹ä»¶: {non_whitelisted.get('total', 0):,}")
    
    if failed.get('by_user'):
        print(f"âš ï¸  å¯ç–‘ä½¿ç”¨è€…: {len(failed['by_user'])}")
    if failed.get('by_ip'):
        print(f"âš ï¸  å¯ç–‘IP: {len(failed['by_ip'])}")
    if non_whitelisted.get('by_ip'):
        print(f"âš ï¸  éç™½åå–®IP: {len(non_whitelisted['by_ip'])}")
    if after_hours.get('total'):
        print(f"âš ï¸  éä¸Šç­æ™‚é–“å­˜å–: {after_hours['total']}")
    
    print("\n" + "="*60)
    print("â±ï¸  åŸ·è¡Œæ™‚é–“çµ±è¨ˆ")
    print("="*60)
    print(f"ğŸ” åˆ†æè€—æ™‚: {analysis_duration:.2f} ç§’")
    print(f"ğŸ“Š å ±è¡¨è€—æ™‚: {report_duration:.2f} ç§’")
    print(f"ğŸ•’ ç¸½åŸ·è¡Œæ™‚é–“: {total_duration:.2f} ç§’")
    print(f"ğŸ ç¨‹å¼çµæŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main()
